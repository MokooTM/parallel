# 组通信
<!-- toc -->

前面提到的通信都是点到点通信，这里介绍组通信。MPI 组通信和点到点通信的一个重要区别就在于它需要一个**特定组内的所有进程**同时参加通信，而不是像点对点通信那样只涉及到发送方和接收方两个进程。组通信在**各个进程中的调用方式完全相同**，而不是像点对点通信那样在形式上有发送和接收的区别。

## 功能
组通信一般实现三个功能：
* 通信：主要完成组内数据的传输
* 同步：实现组内所有进程在特定点的执行速度保持一致
* 计算：对给定的数据完成一定的操作

### 消息通信
对于组通信来说，按照通信方向的不同，可以分为以下三种：一对多通信，多对一通信和多对多通信，下面是这三类通信的示意图：

![](/images/组通信一对多.png)
<br />

![](/images/组通信多对一.png)
<br />

![](/images/组通信多对多.png)

### 同步
组通信提供了专门的调用以完成各个进程之间的同步，从而协调各个进程的进度和步伐。下面是 MPI 同步调用的示意图

![](/images/组通信同步调用.png)

### 计算
MPI 组通信提供了计算功能的调用，通过这些调用可以对接收到的数据进行处理。当消息传递完毕后，组通信会用给定的计算操作对接收到的数据进行处理，处理完毕后将结果放入指定的接收缓冲区。

## 广播
`MPI_Bcast` 是一对多通信的典型例子，它可以将 root 进程中的一条信息广播到组内的其它进程，同时包括它自身。在执行调用时，组内所有进程（不管是 root 进程还是其它的进程）都使用同一个通信域 comm 和根标识 root，其执行结果是将根进程消息缓冲区的消息拷贝到其他的进程中去。下面是 `MPI_Bcast` 的函数原型：
```c
int MPI_Bcast(
    void * buffer,          // 通信消息缓冲区的起始位置
    int count,              // 广播 / 接收数据的个数
    MPI_Datatype datatype,  // 广播 / 接收数据的数据类型
    int root,               // 广播数据的根进程号
    MPI_Comm comm           // 通信域
);
```
对于广播调用，不论是广播消息的根进程，还是从根接收消息的其他进程，在调用形式上完全一致，即指明相同的根，相同的元素个数以及相同的数据类型。下面是广播前后各进程缓冲区中数据的变化

![](/images/广播.png)

`MPI_Bcast` 的实现类似于下面的代码，不过 MPI 的实现进行了优化，使广播更加高效。
```c
void my_bcast(void* data, int count, MPI_Datatype datatype, int root,
              MPI_Comm communicator) {
  int world_rank;
  MPI_Comm_rank(communicator, &world_rank);
  int world_size;
  MPI_Comm_size(communicator, &world_size);

  if (world_rank == root) {
    // If we are the root process, send our data to everyone
    int i;
    for (i = 0; i < world_size; i++) {
      if (i != world_rank) {
        MPI_Send(data, count, datatype, i, 0, communicator);
      }
    }
  } else {
    // If we are a receiver process, receive the data from the root
    MPI_Recv(data, count, datatype, root, 0, communicator,
             MPI_STATUS_IGNORE);
  }
}
```
下面是使用 MPI 广播的一个例子，进程 0 初始化数据，同时广播到其他进程
```c
#include <stdio.h>
#include <stdlib.h>
#include "mpi.h"

int main() {
    int rank;
    int value;
    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    if(rank == 0) {
        value = 10;
    }

    // 将进程 0 的数据广播到其他进程中
    MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);
    printf("Process %d value is %d\n", rank, value);
    MPI_Finalize();
}
```
## 收集
通过 `MPI_Gather` 可以将其他进程中的数据收集到根进程。根进程接收这些消息，并把它们按照进程号 rank 的顺序进行存储。对于所有非根进程，接收缓冲区会被忽略，但是各个进程仍需提供这一参数。在 gather 调用中，发送数据的个数 sendcount 和发送数据的类型 sendtype 接收数据的个数 recvcount 和接受数据的类型 recvtype 要完全相同。下面是 `MPI_Gather` 的函数原型
```c
int MPI_Gather(
    void * sendbuf,         // 发送缓冲区的起始地址
    int sendcount,          // 发送数据的个数
    MPI_Datatype sendtype,  // 发送数据类型
    void * recvbuf,         // 接收缓冲区的起始地址
    int recvcount,          // 接收数据的个数
    MPI_Datatype recvtype,  // 接收数据的类型
    int root,               // 根进程的编号
    MPI_Comm comm           // 通信域
);
```
下面是 gather 的示意图：

![](/images/收集.png)

`MPI_Gatherv` 和 `MPI_Gather` 类似，也可以完成数据收集的功能，但是它可以从不同的进程接受不同数量的数据。进程接收元素的个数 recvcounts 是一个数组，用来指定从不同进程接受的数据元素的个数 。跟从每一个进程接收的数据元素个数可以不同，但是需要注意的是**发送和接受的个数需要保持一致**。另外 `MPI_Gatherv` 还提供一个位置偏移数组 displs，用户指定接收的数据在消息缓冲区中的索引，下面是 `MPI_Gatherv` 的函数原型：
```c
int MPI_Gatherv(
    void * sendbuf,         // 发送缓冲区的起始地址
    int sendcount,          // 发送数据的个数
    MPI_Datatype sendtype,  // 发送数据类型
    void * recvbuf,         // 接收缓冲区的起始地址
    int * recvcounts,       // 从每个进程接收的数据个数
    int * displs,           // 接收数据在消息缓冲区中的索引
    MPI_Datatype recvtype,  // 接收数据的类型
    int root,               // 根进程的编号
    MPI_Comm comm           // 通信域
);
```
下面是使用 `MPI_Gather` 的一个示例：
```c
void gather() {
    int size;
    int rank;
    int n = 10;
    int send_array[n];
    int * recv_array;
    int i;

    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    // 初始化其它进程的数据
    for(i = 0; i < n; i++) {
        send_array[i] = i + rank * n;
    }
    if(rank == 0) {
        recv_array = (int *)malloc(sizeof(int) * n * size);
    }
    MPI_Gather(send_array, n, MPI_INT, recv_array, n, MPI_INT, 0, MPI_COMM_WORLD);
    if(rank == 0) {
        for(i = 0; i < n * size; i++) {
            printf("recv_array[%d] id %d\n", i, recv_array[i]);
        }
        free(recv_array);
    }
    MPI_Finalize();
}
```
下面是使用 `MPI_Gatherv` 的一个示例
```c
void gatherv() {
    int size;
    int rank;
    int n = 10;
    int send_array[n];
    int * recv_array;
    int i;

    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    int recv_count[size];
    int displs[size];
    for(i = 0; i < size; i++) {
        recv_count[i] = i + 1;
        displs[i] = 10 * i;
    }
    // 初始化其它进程的数据
    for(i = 0; i < n; i++) {
        send_array[i] = i + rank * n;
    }
    if(rank == 0) {
        recv_array = (int *)malloc(sizeof(int) * n * size);
    }
    MPI_Gatherv(send_array, recv_count[rank], MPI_INT, recv_array, recv_count, displs, MPI_INT, 0, MPI_COMM_WORLD);
    if(rank == 0) {
        for(i = 0; i < n * size; i++) {
            printf("recv_array[%d] id %d\n", i, recv_array[i]);
        }
        free(recv_array);
    }
    MPI_Finalize();
}
```
## 散发
`MPI_Scatter` 是一对多的组通信调用，和广播不同的是，root 进程向各个进程发送的数据可以是不同的。`MPI_Scatter` 和 `MPI_Gather` 的效果正好相反，两者互为逆操作。下面是 `MPI_Scatter` 的函数原型

```c
int MPI_scatter(
    void * sendbuf,         // 发送缓冲区的起始地址
    int sendcount,          // 发送数据的个数
    MPI_Datatype sendtype,  // 发送数据类型
    void * recvbuf,         // 接收缓冲区的起始地址
    int recvcount,          // 接收数据的个数
    MPI_Datatype recvtype,  // 接收数据的类型
    int root,               // 根进程的编号
    MPI_Comm comm           // 通信域
);
```
下面是 scatter 的示意图：

![](/images/散发.png)

`MPI_Scatterv` 和 `MPI_Gatherv` 也是一对互逆操作，下面是 `MPI_Scatterv` 的函数原型
```c
int MPI_scatter(
    void * sendbuf,         // 发送缓冲区的起始地址
    int* sendcounts,        // 向每个进程发送的数据个数
    int* displs,            // 发送数据的偏移
    MPI_Datatype sendtype,  // 发送数据类型
    void * recvbuf,         // 接收缓冲区的起始地址
    int recvcount,          // 接收数据的个数
    MPI_Datatype recvtype,  // 接收数据的类型
    int root,               // 根进程的编号
    MPI_Comm comm           // 通信域
);
```
下面是使用 `MPI_Scatter` 的一个示例：
```c
void scatter() {
    int size;
    int rank;
    int n = 10;
    int * send_array;
    int recv_array[n];

    int i, j;

    MPI_Init(NULL, NULL);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    if(rank == 0) {
        send_array = (int *)malloc(sizeof(int) * n * size);
        for(i = 0; i < n * size; i++) {
            send_array[i] = i;
        }
    }
    MPI_Scatter(send_array, n, MPI_INT, recv_array, n, MPI_INT, 0, MPI_COMM_WORLD);
    for(i = 0; i < size; i++) {
        MPI_Barrier(MPI_COMM_WORLD);
        if(rank == i) {
            for(j = 0;j < n; j++) {
                printf("Process %d recv[%d] is %d\n", rank, j, recv_array[j]);
            }            
        }
    }
    MPI_Finalize();
}
```
