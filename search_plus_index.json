{"./":{"url":"./","title":"前言","keywords":"","body":"并行计算 记录并行计算的相关知识，目前主要包括下面的内容 MPI MPI-HelloWorld 基础函数 通信类别 通信模式 编程模式 非阻塞通信 重复非阻塞通信 组通信 数据类型 OpenMP OpenMP 基本使用 编译制导指令 运行环境交互 更多指令和子句 Pthreads 语法 流水线模型 MIC HelloWorld offload in / out / inout 详细用法 offload 其他函数 共享虚拟内存模式 异步计算和传输 向量化 "},"mpi.html":{"url":"mpi.html","title":"MPI","keywords":"","body":"MPI 学习记录 高性能计算之并行编程技术 —— MPI并行程序设计 MPI Tutorial Message Passing Interface DeinoMPI 大部分内容摘抄自 MPI并行程序设计，同时增加了部分内容以及自己的一些理解。另外代码全部使用 c 语言，大部分在原书代码的基础上进行了修改，使代码更易于理解。 "},"mpi-helloworld.html":{"url":"mpi-helloworld.html","title":"MPI-HelloWorld","keywords":"","body":"HelloWord 安装 MPI 运行环境 HelloWorld 编译 运行 安装 MPI 运行环境 Installing MPICH2 on a Single Machine 去 MPICH2 官网下载源码包，然后安装 tar -xzf mpich-3.2.tar.gz cd mpich-3.2 ./configure --disable-fortran CC=gcc CXX=g++ make sudo mark install 安装了 Intel 编译器的可以使用 mpiicc 和 mpiicpc HelloWorld #include #include int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, &world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, &name_len); // Print off a hello world message printf(\"Hello world from processor %s, rank %d\" \" out of %d processors\\n\", processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); } 编译 mpicc -o helloworld helloworld.c 运行 mpiexec ./helloworld // 或者 mpirun ./helloworld mpirun 等同于 mpiexec mpirun 命令 mpirun -n -ppn -f ./myprog -n - sets the number of MPI processes to launch; if the option is not specified, the process manager pulls the host list from a job scheduler, or uses the number of cores on the machine. -ppn - sets the number of processes to launch on each node; if the option is not specified, processes are assigned to the physical cores on the first node; if the number of cores is exceeded, the next node is used. -f - specifies the path to the host file listing the cluster nodes; alternatively, you can use the -hosts option to specify a comma-separated list of nodes; if hosts are not specified, the local node is used. 如果需要在多个节点上运行运行，可以参考 无需超级用户mpi多机执行 "},"基础函数.html":{"url":"基础函数.html","title":"基础函数","keywords":"","body":"基本函数 6个基本函数 信息传递 预定义数据类型 任意源和任意标识 其他常用函数 获得当前时间 获得机器名字 获得 MPI 版本 判断 MPI_Init 是否执行，唯一一个可以在 MPI_Init 之前调用的函数。 使通信域 Comm 中的所有进程退出 示例 线程依次传参 使用 MPI_BYTE 获得接受信息的长度 使用任意源和任意标识 6个基本函数 MPI 有6个基本函数，从理论上讲 MPI 的所有通信功能都可以使用它的6个基本函数来实现。这 6 个函数为 // 初始化 MPI_Init(int *argc, char ***argv)` // MPI 结束调用 MPI_Finalize(void) /** * 获得进程的标识号 * 进程标保存到 rank 里 */ MPI_Comm_rank(MPI_Comm comm, int *rank) /** * 获得当前通信域中进程的个数 * 进程数量保存到 size 里 */ MPI_Comm_size(MPI_Comm comm, int *size)` // 发送消息 MPI_Send(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm) // 接受消息 MPI_Recv(void * buf, int count, MPI_Datatype data, int source, int tag, MPI_Comm comm, MPI_Status *status) 信息传递 MPI 通过 MPI_Send 和 MPI_Receive 发送和接受消息。其中这两种属于阻塞通信，对于发送方来说，消息发送不出去就会一直等待，而对于接收方来说，接收不到消息就会一直等待。MPI_Send 各参数的含义： buf - 发送缓冲区的起始位置 count - 发送的数据个数 datatype - 发送数据的数据类型 dest - 目标进程标号 tag - 消息标志 comm - 通信域 MPI_Recv 各参数含义： buf - 接受缓冲区地址 count - 最多可接受的数据个数 datatype - 接受的数据类型 source - 发送数据的进程号 tag - 消息标志 comm - 通信域 status - 返回状态 发送和接受的时候是以指定的 datatype 为基本单位的，count 是 datatype 类型数据的数目。在接受数据时，接受缓冲区的长度可以大于发送数据的长度。但是 MPI 中没有数据截断，如果发送数据长度大于接受缓冲区的长度就会报错。 在 C 实现中，状态变量 MPI_Status 必须要包含 3 个信息：MPI_SOURCE, MPI_TAG 和 MPI_ERROR，除此之外还可以包含其它的附加域。 通过 MPI_Status，我们可以获得下面的三种主要信息 发送进程的标号，存放在 MPI_SOURCE 属性中 消息的标记号，存放在 MPI_TAG 属性中 消息的长度，通过借助于 MPI_Get_count 函数，将 status 变量和数据类型传入，消息长度存放在 count 中 MPI_Get_count( MPI_Status* status, MPI_Datatype datatype, int* count) 预定义数据类型 MPI 预定义了下面几种数据类型 MPI预定义数据类型 对应的C数据类型 MPI_CHAR signed char MPI_SHORT signed short int MPI_INT signed int MPI_LONG signed long int MPI_LONG_LONG_INT long long int (optional) MPI_UNSIGNED_CHAR unsigned char MPI_UNSIGNED_SHORT unsigned short int MPI_UNSIGNED unsigned int MPI_UNSIGNED_LONG unsigned long int MPI_FLOAT float MPI_DOUBLE double MPI_LONG_DOUBLR long double MPI_BYTE 无 MPI_PACKED 无 在传递信息的时候，要保证两个方面的类型匹配（除了 MPI_BYTE 和 MPI_PACKED ）： 传输的数据类型和通信中声明的 MPI 类型要对应，即数据类型为 int， 那么通信时声明的数据类型就要为 MPI_INT。 发送方和接受方的类型要匹配 MPI_BYTE 和 MPI_PACKED 可以和任意以字节为单位的存储相匹配。 MPI_BYTE 可以用于不加修改的传送内存中的二进制值。 任意源和任意标识 在消息传递时，发送操作必须明确指定发送对象的进程标号和消息标识，但是接收消息时，可以通过使用 MPI_ANY_SOURCE 和 MPI_ANY_TAG 来接受任意进程发送给本进程的消息，类似于通配符。MPI_ANY_SOURCE 和 MPI_ANY_TAG 可以同时使用或者分别单独使用。 其他常用函数 这里记录一下 MPI 中其他的常用函数 获得当前时间 double MPI_Wtime(void) 获得机器名字 int MPI_Get_processor_name(char *name, int *result_len) name - 当前进程所在机器的名字 result_len - 返回名字的长度 获得 MPI 版本 int MPI_Get_version(int *version, int *subversion) version - MPI 主版本号 subversion - MPI 次版本号 判断 MPI_Init 是否执行，唯一一个可以在 MPI_Init 之前调用的函数。 int MPI_Initialized(int *flag) - flag - 是否已调用 使通信域 Comm 中的所有进程退出 int MPI_Abort(MPI_Comm comm, int errorcode) comm - 退出进程所在的通信域 errorcode - 错误码 示例 线程依次传参 #include #include #include \"mpi.h\" /** * 数据传递 * * 线程 i 向线程 i+1 传数据 */ int main() { int param; int process_num; int process_id; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &process_id); MPI_Comm_size(MPI_COMM_WORLD, &process_num); if(process_id == 0) { param = 3; printf(\"rank 0 send rank 1: %d\\n\", param); param++; MPI_Send(&param, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); } else { MPI_Recv(&param, 1, MPI_INT, process_id - 1, 99, MPI_COMM_WORLD, &status); printf(\"rank %d receive from %d: %d\\n\",process_id, process_id-1, param); if(process_id 运行结果 rank 0 send rank 1: 3 rank 1 receive from 0: 4 rank 1 send rank 2: 5 rank 2 receive from 1: 5 rank 2 send rank 3: 6 rank 3 receive from 2: 6 使用 MPI_BYTE 下面是一个使用 MPI_BYTE 传递自定义结构体的例子。 #include #include #include \"mpi.h\" typedef struct _custom_t { int a; double b; } custom_t; int main() { int rank; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); custom_t msg; if(rank == 0) { msg.a = 2; msg.b = 10.0; MPI_Send(&msg, sizeof(custom_t), MPI_BYTE, 1, 99, MPI_COMM_WORLD); } if(rank == 1) { MPI_Recv(&msg, sizeof(custom_t), MPI_BYTE, 0, 99, MPI_COMM_WORLD, &status); printf(\"msg: a is %d and b is %.2f\\n\", msg.a, msg.b); } MPI_Finalize(); } 获得接受信息的长度 在接收完消息后，获得消息的长度。要求接受缓冲区的长度要大于消息的长度 #include #include #include \"mpi.h\" int main() { int n = 10; int half_n = 5; int buffer[n]; int i; int rank; MPI_Status status; for(i = 0; i 上面的方法不太灵活，因为在接受消息的时候我们仍然不知道消息的长度，在 Recv函数中的 count 不太好指定。通过使用 MPI_Probe 方法，我们可以事先获取要接受的消息的相关信息，可以灵活的接受消息。 下面是 MPI_Probe 函数的原型： MPI_Probe( int source, int tag, MPI_Comm comm, MPI_Status* status) 通过传入发送进程的进程号，消息标记以及通信域，就可以提前获得消息的 status，下面是一个示例： #include #include #include \"mpi.h\" int main() { int n = 10; int half_n = 5; int buffer[n]; int i; int rank; MPI_Status status; for(i = 0; i 使用任意源和任意标识 线程 1 到 n-1 向线程 0 发送消息 #include #include #include \"mpi.h\" int main() { int rank, size; int value = 0; int i; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { for(i = 0; i 示例输出 receive from process 2 and values is 4 receive from process 3 and values is 6 receive from process 1 and values is 2 "},"通信类别.html":{"url":"通信类别.html","title":"通信类别","keywords":"","body":"通信类别 对于 MPI 中的通信中，从通信对象方面看可以分为两类： 点对点通信 - 只涉及发送方和接收方两个进程，并且发送和接收时要指明发送和接收的对象。在发送进程和接收进程里，两者的调用的函数也不相同。 组通信 - 一个特定组内的所有进程同时参与通信，并且组通信在每个进程中的调用方式完全一样。 对于点对点通信来讲，从发送行为不同又可以分为下面三类： 阻塞通信 - 在发送和接受操作完成之前，程序一直处于等待状态 非阻塞通信 - 无需等待发送和操作行为完成就可以返回，然后再调用特定的方法判断通信操作是否完成 重复非阻塞通信 - 针对一个通信被多次调用的情况（例如循环结构内的通信调用），重复利用通信对象，而不是每次都开启一个新的通信。单次的通信和非阻塞通信的行为相同。 对于组通信来说，会为组内的进程隐式添加一个同步点，等到所有进程到达后再往下执行。在后面的文章我们会详细介绍上面提到的通信类别。 "},"通信模式.html":{"url":"通信模式.html","title":"通信模式","keywords":"","body":"通信模式 模式类别 标准通信模式 缓存通信模式 同步通信模式 就绪通信模式 模式类别 对于点对点通信来说，有四种通信模式，分别是标准通信、缓存通信、同步通信和就绪通信。在前面我们还提到点对点通信可以分为阻塞通信、非阻塞通信和重复非阻塞通信三种类别，每种通信类别都有这四种通信模式。这里我们以非阻塞通信为例介绍这四种通信模式。下面是非阻塞通信对应的四种通信模式： 通信模式 发送 接受 标准通信模式（standard mode） MPI_Send MPI_Recv 缓存通信模式（buffered mode） MPI_Bsend   同步通信模式（synchronous mode） MPI_Ssend   就绪通信模式（ready mode） MPI_Rsend   对于非标准的通信模式来说，只有发送操作，没有相应的接收操作。这四种模式的不同点主要表现在两个方面： 数据缓冲区（ buffering ）- 在消息被目标进程接收之前，数据存储的地方 同步（ synchronization ） - 怎样才算完成了发送操作 标准通信模式 使用 MPI_Send 进行消息发送的被成为标准通信模式，在这种模式下，是否使用数据缓冲区以及对数据缓冲区的管理都是由 MPI 自身决定的，用户无法控制。根据 MPI 是否选择缓存发送数据，可以将发送操作完成的标准可以分为下面两种情况： MPI 缓存数据 - 在这种情况下，发送操作不管接受操作是否执行，都可以进行，并且发送操作不需要接收操作收到数据就可以成功返回。 MPI 不缓存数据 - 缓存数据是需要付出代价的，它会延长通信的时间，并且缓冲区并不是总能得到的，所以 MPI 可以选择不缓存数据。在这种情况下，只有当接收操作被调用，并且发送的数据完全到达接收缓冲区后，发送操作才算完成。需要注意的一点，对于非阻塞通信，发送操作虽然没有完成，但是发送调用可以正确返回，程序可以执行其他操作。 下面是标准通信模式的示意图 缓存通信模式 如果希望可以直接对通信缓冲区进行控制，我们可以使用缓存通信模式，下面是缓存发送的函数原型： int MPI_Bsend( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程 int tag, // 消息标识 MPI_Comm comm // 通信域 ) MPI_Bsend 和 MPI_Send 的各参数含义相同，只是在使用 MPI_Bsend 之前需要用户手动指定缓冲区，假设我们不指定缓冲区就直接调用 MPI_Bsend，程序就会报下面的错误： Fatal error in MPI_Bsend: Invalid buffer pointer, error stack: MPI_Bsend(214).......: MPI_Bsend(buf=0x7ffdff7c2d84, count=1, MPI_INT, dest=1, tag=99, MPI_COMM_WORLD) failed MPIR_Bsend_isend(311): Insufficient space in Bsend buffer; requested 4; total buffer size is 0 下面是缓存通信模式的示意图 在手动指定缓冲区时，有3件事需要我们考虑： 如何指定缓冲区 应该指定多大的缓冲区 怎么释放缓冲区 通过 MPI_Buffer_attach 我们可以指定缓冲区，下面是函数原型 int MPI_Buffer_attach( void * buffer, // 缓冲区地址 int size // 缓冲区大小（以字节为单位） ) 通过 MPI_Buffer_detach 我们可以回收缓冲区，下面是函数原型。 int MPI_Buffer_detach( void ** buffer, // 缓冲区地址 int * size // 缓冲区大小 ) 回收操作是阻塞调用，它会一直等到使用该缓存的消息发送完成之后才返回。只有调用返回之后，用户才可以重新使用该缓冲区或者将缓冲区释放。 缓冲区的大小的计算稍微繁琐一些。首先的一点，申请的总缓冲区的大小应该是所有未完成的 MPI_Bsend 所需缓冲区大小的总和。每个 MPI_Bsend 所需的缓冲区大小除了它所传输的数据大小还需要加上一个 MPI_BSEND_OVERHEAD。MPI_BSEND_OVERHEAD 指在 MPI_Bsend 调用时，该调用自身可能占用的最大空间。另外，我们需要使用 MPI_Pack_size 函数获取所传输数据大小，下面是函数原型： int MPI_Pack_size( int incount, // 数据的个数 MPI_Datatype datatype, // 数据的类型 MPI_Comm comm, // 通信域 int *size // 数据所需要的空间，以字节为单位 ) 假设只有一个 MPI_Bsend 调用， 缓冲区大小计算如下所示： MPI_Bsend( ..., count=c, datatype=type, ... ); MPI_Pack_size(c, type, comm, &s1); size = s1 + MPI_BSEND_OVERHEAD; 假设有两个 MPI_Bsend 调用，缓冲区的大小计算如下所示： MPI_Bsend( ..., count=c1, datatype=type1, ... ); MPI_Bsend( ..., count=c2, datatype=type2, ... ); MPI_Pack_size(c1, type1, comm, &s1); MPI_Pack_size(c2, type2, comm, &s2); size = s1 + s2 + 2 * MPI_BSEND_OVERHEAD; 下面是使用 MPI_Bsend 的一个例子 void bsend() { int rank; int a; int * tmp_buffer; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { a = 3; int size; // 计算缓冲区大小 MPI_Pack_size(1, MPI_INT, MPI_COMM_WORLD, &size); size += MPI_BSEND_OVERHEAD; tmp_buffer = (int *)malloc(size); // 指定缓冲区 MPI_Buffer_attach(tmp_buffer, size); MPI_Bsend(&a, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); // 回收缓冲区 MPI_Buffer_detach(&tmp_buffer, &size); free(tmp_buffer); } if(rank == 1) { MPI_Recv(&a, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &status); printf(\"a is %d\\n\", a); } MPI_Finalize(); } 同步通信模式 在同步通信模式中，发送进程必须要等到相应的接受进程开始后才可以正确返回。也就是说如果发送的数据一直没有被接受，发送进程就会一直处于等待状态。当同步发送操作返回之后，说明发送缓冲区中的数据已经全被系统缓冲区缓存，并且已经开始发送，这样发送缓冲区就可以被释放或者重新使用。下面是同步发送的函数原型： MPI_Ssend( void *buf, // 发送缓冲区起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程号 int tag, // 消息标识 MPI_Comm comm // 通信域 ) 下面是同步通信模式的示意图： 下面是同步通信的示例： void ssend() { int rank; int a; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { a = 3; MPI_Ssend(&a, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); printf(\"rank 0 has finished\\n\"); } if(rank == 1) { MPI_Recv(&a, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &status); printf(\"a is %d\\n\", a); } MPI_Finalize(); } 就绪通信模式 在就绪通信模式中，只有当接收进程的接收操作已经启动时，才可以在发送进程启动发送操作。就绪通信模式的特殊之处在于它要求接受操作先于发送操作而被启动，因此在一个正确的程序中，一个就绪发送可以被一个标准发送替代，它对程序的语义没有影响，而对程序的性能有影响。下面是就绪发送的函数原型： MPI_Rsend( void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm ) 下面是就绪通信的示意图： "},"编程模式.html":{"url":"编程模式.html","title":"编程模式","keywords":"","body":"编程模式 模式类别 对等模式 单独发送接收 同时发送接受 使用虚拟进程 主从模式 矩阵向量乘 模式类别 使用 MPI 时有两种最基本的并行程序设计模式，即对等模式和主从模式（master-salve）。在对等模式中，每个进程的功能和代码基本一致，只是处理的数据和对象有所不同。而主从模式中，会有一个 master 线程，用来管理其他的线程（称为 workers 或者 slaves）。 对等模式 单独发送接收 这里我们使用 MPI 并行程序设计 书中的例子 -- Jacobi 迭代来展示对等模式。下面是根据书上的代码修改后的原始代码。从下面的代码可以看出，Jacobi 迭代得到的新值其实就是原来旧值点相邻数值点的平均数。这里为了简单，我们忽略了第一行、第一列、最后一行和最后一列值的计算，而只是使用它们的初始值。还有一点，在每次 k 迭代时，a 更新后的值会首先保存到数组 b 中，等到 a 的值全部计算出来之后，再将 a 的值更新，这样在进行 i 迭代和 j 迭代时，迭代之间就没有数据依赖关系，可以并行。 如果将 b[i][j] = 0.25 * ... 修改为 a[i][j] = 0.25 * ...，这样在进行 i 迭代和 j 迭代时，每次迭代都会依赖前一次的计算结果，是很难并行的。 // 迭代10次，计算时忽略了 0，n-1 行 和 0，n-1 列 for (k = 0; k 对于上面的代码，并行策略十分简单，假设有 n 个进程，m 行数据，每个进程计算 m / n 行数据即可。 假设有 4 个进程，8行数据（去掉第一行和最后一行），那么进程 0 计算 1 和 2 行数据，进程 1 计算 3 和 4 行数据，以此类推。进程在每次 k 迭代时，除了之后自己计算行的数据，还需要知道相邻行的数据。以进程 1 为例，在 i 迭代开始之前，除了需要知道第 3 行和第 4 行的数据，还需要知道第 2 行和第 5 行的数据，这两行数据分别由进程 0 和进程 2 计算，需要在 i 迭代开始之前，由进程 0 和 进程 2 传递过来，同时进程 1 的第 3 行数据需要传给进程 0， 第 4 行数据数据需要传给进程 2 。下面是一张示意图，原文中是按列计算，和按行计算原理一样。 下面是代码实现 void mpi_jacobi() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i 0) { MPI_Send(&a[start][0], n, MPI_INT, rank - 1, 100, MPI_COMM_WORLD); } // 向右侧邻居发送数据 if(rank 0 ) { MPI_Recv(&a[start - 1][0], n, MPI_INT, rank - 1, 99, MPI_COMM_WORLD, &status); } for(i = start; i 同时发送接受 通过使用 MPI_Sendrecv 函数，我们可以实现同时向其他进程发送数据以及从其他进程接受数据，下面是函数原型： MPI_Sendrecv( void *sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据的数据类型 int dest, // 目标进程 int sendtag, // 发送消息标识 void *recvbuf， // 接收缓冲区的初始地址 int recvcount， // 最大接受数据个数 MPI_Datatype recvtype, // 接受数据类型 int source, // 源进程标识 int recvtag, // 接受消息标识 MPI_Comm comm, // 通信域 MPI_Status *status // 返回状态 ) MPI_Sendrecv 可以接收 MPI_Send 的消息， MPI_Recv 也可以接收 MPI_Sendrecv 的消息。 除了 MPI_Sendrecv，我们还可以使用 MPI_Sendrecv_replace 来同时发送和接受。下面是函数原型： MPI_Sendrecv_replace( void * buf, // 发送和接收缓冲区的起始地址 int count, // 发送和接收缓冲区中的数据个数 MPI_Datatype datatype, // 缓冲区中的数据类型 int dest, // 目标进程标识 int sendtag, // 发送信息标识 int source, // 源进程标识 int recvtag, // 接收消息标识 MPI_Comm comm, // 通信域 MPI_Status status // 返回状态 ) MPI_Sendrecv_replace 函数会首先将缓冲区的数据发送出去，然后再将接受的数据放到缓冲区里。 下面是代码示例： void mpi_jacobi2() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i 0) { MPI_Sendrecv(&a[start][0], n, MPI_INT, rank - 1, 100, &a[start - 1][0], n, MPI_INT, rank - 1, 100, MPI_COMM_WORLD, &status); } for(i = start; i 使用虚拟进程 虚拟进程（MPI_PROC_NULL）是不存在的假想进程。在 MPI 中的主要作用是充当真是进程通信的目标或者源。使用虚拟进程可以大大简化处理边界的代码，使程序更加清晰。一个真实进程向虚拟进程 MPI_PROC_NULL 发送消息会立即成功返回，一个真实进程从虚拟进程 MPI_PROC_NULL 接收消息也会立即返回。下面是示例 void mpi_jacobi3() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { dist_pro = MPI_PROC_NULL; } // 向右侧邻居发送数据并且从右侧邻居获得数据 MPI_Sendrecv(&a[end][0], n, MPI_INT, dist_pro, 100, &a[end+1][0], n, MPI_INT, dist_pro, 100, MPI_COMM_WORLD, &status); dist_pro = rank - 1; if(dist_pro 主从模式 矩阵向量乘 在矩阵向量乘中，首先主进程将向量 B 广播给所有的从进程，然后将矩阵 A 的各行依次发送给从进程，从进程计算一行和 B 相乘的结果，然后将结果返回给主线程。为了简单我们使用下面的矩阵，程序中一共 4 个进程，除去一个主进程，其余进程正好处理一行 A 矩阵。 [012123234]×[222]=[61218] \\begin{bmatrix} 0 & 1 & 2 \\\\ 1 & 2 & 3 \\\\ 2 & 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 12 \\\\ 18 \\end{bmatrix} ​⎣​⎡​​​0​1​2​​​1​2​3​​​2​3​4​​​⎦​⎤​​×​⎣​⎡​​​2​2​2​​​⎦​⎤​​=​⎣​⎡​​​6​12​18​​​⎦​⎤​​ 下面是矩阵乘的原始代码 void matrix() { int n = 3; int a[n][n]; int b[n][1]; int c[n][1]; int i, j; for(i = 0; i 在修改之前，我们首先介绍一个函数 MPI_Bcast，MPI_Bcast 可以将数据广播给其他进程，下面是函数原型 MPI_Bcast( void* data, // 缓冲区的起始地址 int count, // 数据的个数 MPI_Datatype datatype, // 数据类型 int root, // 广播数据的根进程标识 MPI_Comm communicator // 通信域 ) MPI_Bcast 的实现类似于下面的代码，不过 MPI 的实现进行了优化，使广播更加高效。 void my_bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) { int world_rank; MPI_Comm_rank(communicator, &world_rank); int world_size; MPI_Comm_size(communicator, &world_size); if (world_rank == root) { // If we are the root process, send our data to everyone int i; for (i = 0; i 在使用 MPI_Bcast 的时候，我们要注意不需要使用 MPI_Recv 接受，而是在每个进程里都调用 MPI_Bcast，下面是使用 MPI 实现向量乘的一个简单示例： void mpi_matrix() { int n = 3; int a[n][n]; int b[n][1]; int c[n][1]; int i, j; int rank, size; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { // 在主进程进行初始化 for(i = 0; i "},"非阻塞通信.html":{"url":"非阻塞通信.html","title":"非阻塞通信","keywords":"","body":"非阻塞通信 简介 非阻塞通信模式 非阻塞通信函数 非阻塞通信完成 基本示例 非阻塞通信取消 释放非阻塞通信对象 消息到达后检查 使用非阻塞通信实现 Jacobi 迭代 简介 前面所讲的 MPI_Send 的通信模式为阻塞通信模式，在这种模式下，当一个阻塞通信正确返回后，可以得到下面的信息： 通信操作已正确完成，即消息已成功发出或者接收 通信占用的缓冲区可以使用，若是发送操作，则该缓冲区可以被其他操作更新，若是接收操作，那么该缓冲区中的数据已经完整，可以被正确使用。 下面是阻塞消息发送和接收的示意图： 在阻塞通信中，对于接收进程，在接受消息时，要保证按照消息发送的顺序接受消息.例如进程 0 向进程 1 连续发送了 2 条消息，记为消息0 和消息1，消息0先发送，这时即便消息1 先到达了进程1，进程1 也无法接受消息1，必须要等到消息0 被接收之后，消息1 才可以被接收。 与阻塞通信不同，非阻塞通信不必等到通信操作完成完成便可以返回，相对应的通信操作会交给特定的通信硬件去完成，在该通信硬件进行通信操作的同时，处理器可以同时进行计算。通过通信与计算的重叠，可以大大提高程序执行的效率。下面是非阻塞消息发送和接收的示意图： 非阻塞通信模式 在前面阻塞通信中，我们知道有 4 种基本的通信模式： 标准通信模式 缓存通信模式 同步通信模式 就绪通信模式 非阻塞通信和这四种通信模式相结合，也有四种不同的模式。同时针对某些通信是在一个循环中重复执行的情况， MPI 又引入了重复非阻塞通信方式，以进一步提高效率。对于重复非阻塞通信，和四种通信模式相结合，又有四种不同的具体形式。下面是具体的通信模式： 通信模式 发送 接受 标准通信模式 MPI_Isend MPI_IRecv 缓存通信模式 MPI_Ibsend 同步通信模式 MPI_Issend 就绪通信模式 MPI_Irsend 重复非阻塞通信 标准通信模式 MPI_Send_init MPI_Recv_init 缓存通信模式 MPI_Bsend_init 同步通信模式 MPI_Ssend_init 就绪通信模式 MPI_Rsend_init 同时需要注意的是只要消息信封相吻合，并且符合有序接受的语义，任何形式的发送和任何形式的接受都可以匹配。 非阻塞通信函数 标准非阻塞发送操作 MPI_Isend( void * buf, // 发送缓冲区起始地址 int count, // 发送数据个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程号 int tag, // 消息标志 MPI_Comm comm, // 通信域 MPI_Request * request // 返回的非阻塞通信对象 ) 标准非阻塞接收 MPI_Irecv( void * buf, // 接受缓冲区的起始地址 int count, // 接受数据的最大个数 MPI_Datatype datatype, // 数据类型 int source, // 源进程标识 int tag, // 消息标志 MPI_Comm comm, // 通信域 MPI_Request * request // 非阻塞通信对象 ) 其余的三种通信模式和阻塞通信的函数形式类似，只是函数名称修改了一下，这里不做详细介绍。 // 同步通信模式 MPI_Issend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) // 缓存通信模式 MPI_Ibsend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) // 就绪通信模式 MPI_Irsend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) 非阻塞通信完成 由于非阻塞通信返回并不意味着该通信已经完成，因此 MPI 提供了一个非阻塞通信对象 -- MPI_Request 来查询通信的状态。通过结合 MPI_Request 和下面的一些函数，我们等待或者检测阻塞通信。 对于单个非阻塞通信来说，可以使用下面两个函数来等待或者检测非阻塞通信。其中 MPI_Wait 会阻塞当前进程，一直等到相应的非阻塞通信完成之后再返回。MPI_Test 只是用来检测通信是否完成，它会立即返回，不会阻塞当前进程。如果通信完成，将 flag 置为 true，如果通信还没完成，则将 flag 置为 false。 MPI_Wait( MPI_Request * request, // 非阻塞通信对象 MPI_Status * status // 返回的状态 ); MPI_Test( MPI_Request * request, // 非阻塞通信对象 int * flag, // 操作是否完成，完成 - true，未完成 - false MPI_Status * status // 返回的状态 ); 对于多个非阻塞通信，MPI 也提供了相应的等待或者检测函数。MPI_Waitany 用来等待多个非阻塞对象中的任何一个非阻塞对象，MPI_Waitall 会等待所有的非阻塞通信完成，MPI_Waitsome 介于 MPI_Waitany 和 MPI_Waitall之间，只要有一个或者多个非阻塞通信完成，该调用就返回。 MPI_Waitany( int count, // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 非阻塞通信对象数组 int * index, // 通信完成的对象在数组中的索引 MPI_Status * status // 返回的状态 ); MPI_Waitall( int count， // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 费组摄通信对象数组 MPI_Status * array_of_status // 状态数组 ); MPI_Waitsome( int incount, // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 非阻塞通信对象数组 int * outcount, // 已完成对象的数目 int * array_of_indices // 已完成对象的索引数组 MPI_Status * array_of_statuses // 已完成对象的状态数组 ); MPI_Testany 用来检测非阻塞通信中是否有任何一个对象已经完成（若有多个非阻塞通信对象完成则从中任取一个），这里只是检测，不会阻塞进程。MPI_Testall 用来检测是否所有的非阻塞通信都已经完成，MPI_Testsome用来检测有非阻塞通信已经完成。 MPI_Testany( int count, // 非阻塞通信对象的个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * index, // 非阻塞通信对象的索引 int * flag, // 是否有对象完成 MPI_Status * status // 返回的状态 ); MPI_Testall( int count, // 非阻塞通信对象个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * flag, // 所有非阻塞通信对象是否都完成 MPI_Status * array_of_statuses // 状态数组 ); MPI_Testsome( int incount, // 非阻塞通信对象的个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * outcount, // 已完成对象的数目 int * array_of_indices, // 已完成对象的索引数组 MPI_Status * array_of_statuses // 已完成对象的状态数组 ) 基本示例 下面是非阻塞通通信的一个基本示例 #include #include #include \"mpi.h\" void isend() { int rank; int n = 1000; int a[n]; int i; int has_finished; MPI_Request request; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { for(i = 0; i 非阻塞通信取消 可以使用 MPI_Cancel 来取消已经调用的非阻塞通信，该调用立即返回。取消调用并不意味着相应的通信一定会被取消，如果非阻塞通信已经开始，那么它会正常完成。如果取消操作时非阻塞通信还没有开始，那么可以取消该阻塞通信，释放通信占用的资源。对于非阻塞通信，即使执行了取消操作，也必须调用 MPI_Wait 或者 MPI_Test 来释放对象。下面是 MPI_Cancel 的函数原型： int MPI_Cancel(MPI_Request *request); 如果一个非阻塞通信已经被执行了取消操作，那么该通信中的 MPI_Wait 和 MPI_Test 将释放非阻塞通信对象，并且在返回结果 status 中指明该通信已经被取消。 一个通信是否被取消，可以通过 MPI_Test_cancelled 来检查，如果返回结果 flag=1 则表明通信已被成功取消，负责说明通信还没有被取消。 int MPI_Test_cancelled( MPI_Status * status, // 状态 int * flag // 是否取消标志 ); 下面是使用 MPI_Cancel 的一个示例 #include #include #include \"mpi.h\" int main() { int rank; MPI_Request request; MPI_Status status; int value; int has_canceled; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; MPI_Send(&value,1, MPI_INT, 1, 99, MPI_COMM_WORLD); } else if(rank == 1) { MPI_Irecv(&value, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &request); MPI_Cancel(&request); MPI_Wait(&request, &status); MPI_Test_cancelled(&status, &has_canceled); printf(\"has_canceled is %d\\n\", has_canceled); printf(\"value is %d\\n\", value); if(has_canceled) { MPI_Irecv(&value, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &request); } MPI_Wait(&request, &status); printf(\"value is %d\\n\", value); } MPI_Finalize(); } 释放非阻塞通信对象 当能够确认一个非阻塞通信操作已经完成时，我们可以直接调用 MPI_Request_free 直接释放掉该对象所占的资源（非阻塞通信对象一定要被释放，通过MPI_Wait 和 MPI_Test 可以释放非阻塞通信对象，但是对于 MPI_Test 来说，只要通信完成之后调用才可以释放对象）。当调用 MPI_Request_free 操作之后，通信对象不会立即释放，而是要等到阻塞通信结束之后才会释放。下面是函数原型 int MPI_Request_free(MPI_Request * request); 消息到达后检查 通过调用 MPI_Probe 和 MPI_Iprobe，我们可以在不实际接收消息的情况下检查消息是否到达，其中 MPI_Probe 用于阻塞通信，MPI_Iprobe 用于非阻塞通信。如果消息已经到达，那么通过调用 MPI_Probe 和 MPI_Iprobe 返回的 status 和通过 MPI_Recv 返回的 status 是一样的。通过返回的 status，我们可以获得消息的相关信息，例如消息的长度等。下面是函数的原型： int MPI_Probe( int source, // 源进程标识 或者 `MPI_ANY_SOURCE` int tag, // 特定的消息标识值 或者 `MPI_ANY_TAG` MPI_Comm comm, // 通信域 MPI_Status *status // 返回的状态 ) int MPI_Iprobe( int source, // 源进程标识 或者 `MPI_ANY_SOURCE` int tag, // 特定的消息标识值 或者 `MPI_ANY_TAG` MPI_Comm comm, // 通信域 int * falg, // 消息是否到达 MPI_Status * status // 返回的状态 ); 下面是使用 MPI_Iprobe 的一个示例，在接收时调用了5次 MPI_Iprobe 函数，主要是用来等待消息到达。 #include #include #include \"mpi.h\" int main() { int rank; MPI_Request request; MPI_Status status; int value; int hava_message = 0; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; MPI_Isend(&value,1, MPI_INT, 1, 99, MPI_COMM_WORLD, &request); MPI_Request_free(&request); } else if(rank == 1) { for(int i = 0; i 使用非阻塞通信实现 Jacobi 迭代 通过使用非阻塞通信实现 Jacobi 迭代，可以实现通信和计算的重叠。为了实现计算与通信的最大重叠，一个通用的原则就是“尽早开始通信，进晚结束通信”，在开始通信和完成通信之间进行计算，这样就有更多的计算任务和通信重叠。这里，我们首先计算边界处的数据，计算完成之后执行非阻塞通信，在通信的同时计算剩余的数据。下面是具体实现方式： void mpi_jacobi_new() { int m = 18; int n = 18; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { right = MPI_PROC_NULL; } MPI_Isend(&b[start][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[0]); MPI_Isend(&b[end][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[1]); MPI_Irecv(&a[start - 1][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[2]); MPI_Irecv(&a[end+1][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[3]); // 计算剩余的部分 for(i = start+1; i "},"重复非阻塞通信.html":{"url":"重复非阻塞通信.html","title":"重复非阻塞通信","keywords":"","body":"重复非阻塞通信 介绍 通信模式 示例 实现 Jacobi 迭代 介绍 如果一个通信会被重复执行，比如循环结构内的通信调用，MPI 提供了重复非阻塞通信进行优化，以降低不必要的通信开销，下面是非阻塞通信的流程： 在重复通信时，通信的初始化操作并没有启动消息通信，消息的真正通信是由 MPI_START 触发的，消息的完成操作并不释放相应的非阻塞通信对象，只是将其状态置为非活动状态，若下面进行重复通信，再由 MPI_START 将对象置为活动状态，并启动通信。当不需要再进行通信时，必须通过显式的语句MPI_Request_free将非阻塞通信对象释放掉。 通信模式 根据通信模式的不同，重复非阻塞通信也有四种不同的形式，即标准模式、缓存模式、同步模式和就绪模式，分别对应的函数为 MPI_Send_init，MPI_Bsend_init，MPI_Ssend_init和MPI_Rsend_init，下面是函数原型： // 标准模式 int MPI_Send_init( void * buf, // 发送缓冲区起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request * request // 非阻塞通信对象 ); // 缓存模式 int MPI_Bsend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); // 同步模式 int MPI_Ssend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); // 就绪模式 int MPI_Ssend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); 通过 MPI_Recv_init 函数来完成接收操作，下面是函数原型： int MPI_Recv_init( void * buf, // 接受缓冲区的起始地址 int count, // 接受数据的个数 MPI_Datatype datatype, // 接受数据的数据类型 int dest, // 源进程标识 或者 MPI_ANY_SOURCE int tag, // 消息标识 或者 MPI_ANY_TAG MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); 在前面提到，一个非阻塞通信在创建后会处于非活动状态，需要使用 MPI_Start 函数来激活通信，下面是函数原型 int MPI_Start( MPI_Request * request // 费祖通信对象 ); 对于多个非阻塞通信，我们还可以使用 MPI_Startall 来同时激活多个非阻塞通信，下面是函数原型 int MPI_Startall( int count, // 开始非阻塞通信对象的个数 MPI_Request * requests // 非阻塞通信对象数组 ); 示例 下面是使用重复非阻塞通信的一个示例： #include #include #include \"mpi.h\" int main() { int rank; int value; MPI_Request request; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { MPI_Send_init(&value, 1, MPI_INT, 1, 99, MPI_COMM_WORLD, &request); for(int i = 0; i 实现 Jacobi 迭代 下面是用重复非阻塞通信实现的Jacobi 迭代 void mpi_jacobi_new2() { int m = 18; int n = 18; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { right = MPI_PROC_NULL; } MPI_Send_init(&b[start][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[0]); MPI_Send_init(&b[end][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[1]); MPI_Recv_init(&a[start - 1][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[2]); MPI_Recv_init(&a[end+1][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[3]); // 迭代10次，计算时忽略了 0，n-1 行 和 0，n-1 列 for(k = 0; k "},"组通信.html":{"url":"组通信.html","title":"组通信","keywords":"","body":"组通信 功能 消息通信 同步 计算功能 通信 广播 收集 散发 组收集 全互换 进程同步 计算 规约 计算 PI 组规约 规约并散发 扫描 规约操作对比 最大值与最小值 自定义规约操作 前面提到的通信都是点到点通信，这里介绍组通信。MPI 组通信和点到点通信的一个重要区别就在于它需要一个特定组内的所有进程同时参加通信，而不是像点对点通信那样只涉及到发送方和接收方两个进程。组通信在各个进程中的调用方式完全相同，而不是像点对点通信那样在形式上有发送和接收的区别。 功能 组通信一般实现三个功能： 通信：主要完成组内数据的传输 同步：实现组内所有进程在特定点的执行速度保持一致 计算：对给定的数据完成一定的操作 消息通信 对于组通信来说，按照通信方向的不同，可以分为以下三种：一对多通信，多对一通信和多对多通信，下面是这三类通信的示意图： 同步 组通信提供了专门的调用以完成各个进程之间的同步，从而协调各个进程的进度和步伐。下面是 MPI 同步调用的示意图 计算功能 MPI 组通信提供了计算功能的调用，通过这些调用可以对接收到的数据进行处理。当消息传递完毕后，组通信会用给定的计算操作对接收到的数据进行处理，处理完毕后将结果放入指定的接收缓冲区。 通信 广播 MPI_Bcast 是一对多通信的典型例子，它可以将 root 进程中的一条信息广播到组内的其它进程，同时包括它自身。在执行调用时，组内所有进程（不管是 root 进程还是其它的进程）都使用同一个通信域 comm 和根标识 root，其执行结果是将根进程消息缓冲区的消息拷贝到其他的进程中去。下面是 MPI_Bcast 的函数原型： int MPI_Bcast( void * buffer, // 通信消息缓冲区的起始位置 int count, // 广播 / 接收数据的个数 MPI_Datatype datatype, // 广播 / 接收数据的数据类型 int root, // 广播数据的根进程号 MPI_Comm comm // 通信域 ); 对于广播调用，不论是广播消息的根进程，还是从根接收消息的其他进程，在调用形式上完全一致，即指明相同的根，相同的元素个数以及相同的数据类型。下面是广播前后各进程缓冲区中数据的变化 MPI_Bcast 的实现类似于下面的代码，不过 MPI 的实现进行了优化，使广播更加高效。 void my_bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) { int world_rank; MPI_Comm_rank(communicator, &world_rank); int world_size; MPI_Comm_size(communicator, &world_size); if (world_rank == root) { // If we are the root process, send our data to everyone int i; for (i = 0; i 下面是使用 MPI 广播的一个例子，进程 0 初始化数据，同时广播到其他进程 #include #include #include \"mpi.h\" int main() { int rank; int value; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; } // 将进程 0 的数据广播到其他进程中 MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD); printf(\"Process %d value is %d\\n\", rank, value); MPI_Finalize(); } 收集 通过 MPI_Gather 可以将其他进程中的数据收集到根进程。根进程接收这些消息，并把它们按照进程号 rank 的顺序进行存储。对于所有非根进程，接收缓冲区会被忽略，但是各个进程仍需提供这一参数。在 gather 调用中，发送数据的个数 sendcount 和发送数据的类型 sendtype 接收数据的个数 recvcount 和接受数据的类型 recvtype 要完全相同。下面是 MPI_Gather 的函数原型 int MPI_Gather( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是 gather 的示意图： MPI_Gatherv 和 MPI_Gather 类似，也可以完成数据收集的功能，但是它可以从不同的进程接受不同数量的数据。进程接收元素的个数 recvcounts 是一个数组，用来指定从不同进程接受的数据元素的个数 。跟从每一个进程接收的数据元素个数可以不同，但是需要注意的是发送和接受的个数需要保持一致。另外 MPI_Gatherv 还提供一个位置偏移数组 displs，用户指定接收的数据在消息缓冲区中的索引，下面是 MPI_Gatherv 的函数原型： int MPI_Gatherv( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int * recvcounts, // 从每个进程接收的数据个数 int * displs, // 接收数据在消息缓冲区中的索引 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是使用 MPI_Gather 的一个示例： void gather() { int size; int rank; int n = 10; int send_array[n]; int * recv_array; int i; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); // 初始化其它进程的数据 for(i = 0; i 下面是使用 MPI_Gatherv 的一个示例 void gatherv() { int size; int rank; int n = 10; int send_array[n]; int * recv_array; int i; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); int recv_count[size]; int displs[size]; for(i = 0; i 散发 MPI_Scatter 是一对多的组通信调用，和广播不同的是，root 进程向各个进程发送的数据可以是不同的。MPI_Scatter 和 MPI_Gather 的效果正好相反，两者互为逆操作。下面是 MPI_Scatter 的函数原型 int MPI_scatter( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是 scatter 的示意图： MPI_Scatterv 和 MPI_Gatherv 也是一对互逆操作，下面是 MPI_Scatterv 的函数原型 int MPI_scatter( void * sendbuf, // 发送缓冲区的起始地址 int* sendcounts, // 向每个进程发送的数据个数 int* displs, // 发送数据的偏移 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是使用 MPI_Scatter 的一个示例： void scatter() { int size; int rank; int n = 10; int * send_array; int recv_array[n]; int i, j; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { send_array = (int *)malloc(sizeof(int) * n * size); for(i = 0; i 组收集 MPI_Gather 是将数据收集到 root 进程，而 MPI_Allgather 相当于每个进程都作为 root 进程执行了一次 MPI_Gather 调用，即一个进程都收集到了其它所有进程的数据。下面是 MPI_Allgather 的函数原型： int MPI_Allgather( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 向每个进程发送的数据个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 MPI_Comm comm // 通信域 ); 下面是 MPI_Allgather 的示意图 MPI_Allgatherv 和 MPI_Allgather 功能类似，只不过可以为每个进程指定发送和接受的数据个数以及接受缓冲区的起始地址，下面是 MPI_Allgatherv 的函数原型： int MPI_Allgather( void * sendbuf, int sendcount, MPI_Datatype sendtype, void * recvbuf, int* recvcounts, int * displs, MPI_Datatype recvtype, MPI_Comm comm ); 全互换 MPI_Alltoall 是组内进程完全交换，每个进程都向其它所有的进程发送消息，同时每一个进程都从其他所有的进程接收消息。它与 MPI_Allgather 不同的是:MPI_Allgather 接收完消息后每个进程接收缓冲区的数据是完全相同的，但是 MPI_Alltoall 接受完消息后接收缓冲区的数据一般是不同的，下面是 MPI_Alltoall 的示意图，如果将进程和对应的数据看做是一个矩阵的话，MPI_Alltoall 就相当于把矩阵的行列置换了一下： 下面是 MPI_Alltoall 和 MPI_Alltoallv 的函数原型： int MPI_Alltoall( void * sendbuf, int sendcount, MPI_Datatype sendtype, void * recvbuf, int recvcount, MPI_Datatype recvtype, MPI_Comm comm ); int MPI_Alltoallv( void * sendbuf, int sendcount, MPI_Datatype sendtype, void * recvbuf, int* recvcounts, int * displs, MPI_Datatype recvtype, MPI_Comm comm ); 下面是使用 MPI_Alltoall 的一个示例： void all_to_all() { int size; int rank; int n = 2; int i; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); int send_array[n * size]; int recv_array[n * size]; for(i = 0; i 进程同步 MPI_Barrier 会阻塞进程，直到组中的所有成员都调用了它，组中的进程才会往下执行，在上面的代码中我们使用 MPI_Barrier 来顺序输出每个进程的数据， for(i = 0; i 这里解释一下为什么下面的代码可以做到顺序输出。我们直到 MPI_Barrier 的作用是阻塞进程，直到所有进程都到达这个点。当第一次循环时，各个进程在 MPI_Barrier 的地方首先同步一下，然后继续往下执行，进程 1, 2, 3 直接跳过开始第二次循环，而进程 0 开始输出自己的数据。而进程 1, 2, 3 又会在 MPI_Barrier 处等待，直到进程 0 输出完数据，也到达第二次循环的同步点。此时所有的进程又开始往下执行。不过和上次不同的是，这次是进程 0, 2, 3 进入第三次循环，而进程 1 开始输出数据。进程 0, 2, 3 又会在 MPI_Barrier 处等待进程 1 。重复上面的过程，我们就可以顺序输出每个进程的数据。 下面是 MPI_Barrier 的函数原型： int MPI_Barrier( MPI_Comm comm ); 计算 规约 MPI_Reduce 用来将组内每个进程输入缓冲区中的数据按给定的操作 op 进行预案算，然后将结果返回到序号为 root 的接收缓冲区中。操作 op 始终被认为是可以结合的，并且所有 MPI 定义的操作被认为是可交换的。用户自定义的操作被认为是可结合的，但是可以不是可交换的（先抄下来，不太懂）。下面是 MPI_Reduce 的示意图： 下面是 MPI_Reduce 的函数原型 int MPI_Reduce( void * sendbuf, // 发送缓冲区的起始地址 void * recvbuf, // 接收缓冲区的起始地址 int count, // 发送/接收 消息的个数 MPI_Datatype datatype, // 发送消息的数据类型 MPI_Op op, // 规约操作符 int root, // 根进程序列号 MPI_Comm comm // 通信域 ); MPI 预定义了一些规约操作，如下表所示： 操作 含义 MPI_MAX 最大值 MPI_MIN 最小值 MPI_SUM 求和 MPI_PROD 求积 MPI_LAND 逻辑与 MPI_BAND 按位与 MPI_LOR 逻辑或 MPI_BOR 按位或 MPI_LXOR 逻辑异或 MPI_BXOR 按位异或 MPI_MAXLOC 最大值且相应位置 MPI_MINLOC 最小值且相应位置 下面是使用 MPI_Reduce 的一个示例： void reduce() { int size; int rank; int n = 2; int send_array[n]; int recv_array[n]; int i, j; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); for(i = 0; i 计算 PI PI 的计算公式可以通过下面的公式计算出来： f(x)=41+x2 f(x) = \\frac{4}{1+x^{2}} f(x)=​1+x​2​​​​4​​ π≈1N×∑i=1Nf(i−0.5N) \\pi \\approx \\frac{1}{N} \\times \\sum_{i=1}^{N} f(\\frac{i-0.5}{N}) π≈​N​​1​​×​i=1​∑​N​​f(​N​​i−0.5​​) 使用 MPI 并行的思路是每个进程计算一部分 N 值，计算完成之后通过 MPI_Reduce 将结果收集起来，下面是实现代码： void cal_pi_mpi() { int N = 10; int size = 0; int rank = 0; int start; int end; int unit_space = 0; int i; double pi = 0.0; double result; double x; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); unit_space = N / size; if(rank == size - 1) { start = unit_space * rank; end = N; } else { start = unit_space * rank; end = unit_space * (rank + 1); } for(i = start + 1; i 组规约 组规约 MPI_Allreduce 相当于组中每个进程作为 ROOT 进行了一次规约操作，即每个进程都有规约的结果。下面是 MPI_Allreduce 的函数原型 int MPI_Reduce( void * sendbuf, // 发送缓冲区的起始地址 void * recvbuf, // 接收缓冲区的起始地址 int count, // 发送/接收 消息的个数 MPI_Datatype datatype, // 发送消息的数据类型 MPI_Op op, // 规约操作符 MPI_Comm comm // 通信域 ); 规约并散发 MPI_Reduce_scatter 会将规约结果分散到组内的所有进程中去。在 MPI_Reduce_scatter 中，发送数据的长度要大于接收数据的长度，这样才可以把规约的一部分结果散射到各个进程中。该函数的参数中有个 recvcounts 数组，用来记录每个进程结束数据的数量，这个数组元素的和就是发送数据的长度。下面是示意图 下面是函数原型： int MPI_Reduce( void * sendbuf, // 发送缓冲区的起始地址 void * recvbuf, // 接收缓冲区的起始地址 int* recvcounts, // 接受数据的个数（数组） MPI_Datatype datatype, // 发送消息的数据类型 MPI_Op op, // 规约操作符 MPI_Comm comm // 通信域 ); 下面是一个使用示例： void reduce_gather() { int size; int rank; int n = 2; int i, j; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); int send_array[n]; int recv_array[n]; int num[size]; for(i = 0; i 扫描 可以将扫面看做是一种特殊的规约，即每个进程都对排在它前面的进程进行规约操作。 MPI_Scan 的调用结果是，对于每一个进程i，它对进程 0,...,1 的发送缓冲区的数据进行指定的规约操作，结果存入进程 i 的接收缓冲区。下面是 MPI_Scan 的函数原型： int MPI_Reduce( void * sendbuf, // 发送缓冲区的起始地址 void * recvbuf, // 接收缓冲区的起始地址 int count, // 输入缓冲区中元素的个数 MPI_Datatype datatype, // 发送消息的数据类型 MPI_Op op, // 规约操作符 MPI_Comm comm // 通信域 ); 规约操作对比 下面是不同的规约操作的数据变化： 规约操作： 组规约操作： 规约并发散操作 扫描操作 最大值与最小值 MPI_MINLOC 用来计算全局最小值和最小值所在进程的索引，MPI_MAXLOC 用来计算全局最大值和最大值的索引。这里我们可以看到得到的结果是值和索引，所以需要定义一个 struct 来存储这两个值，下面是一个示例： struct { int value; int rank; } in[n], out[n]; rank的类型一定是整形，但是 value 的值可以不是整形，因此在 MPI 里定义几种类型用来指定 value 是什么类型的值，如下所示： 名称 描述 MPI_FLOAT_INT 浮点型和整形 MPI_DOUBLE_INT 双精度和整形 MPI_LONG_INT 长整形和整形 MPI_2INT 整型值对 MPI_SHORT_INT 短整形和整形 MPI_LONG_DOUBLE_INT 长双精度浮点型和整型 下面是一个使用示例： #include #include #include #include \"mpi.h\" void max() { int n = 10; int max_value = 100; struct { int value; int rank; } in[n], out[n]; int rank; int size; int i, j; MPI_Init(NULL, NULL); MPI_Comm_size(MPI_COMM_WORLD, &size); MPI_Comm_rank(MPI_COMM_WORLD, &rank); srand(time(NULL) + rank); for(i = 0; i 自定义规约操作 我们可以通过调用 MPI_Op_create 来自定义规约操作，下面是函数原型： int MPI_Op_create( MPI_User_function * function, // 用户自定义函数 int commute, // 是否可交换，是true，否false MPI_Op *op // 操作句柄 ) 这里需要注意的是用户自定义的操作必须是可以结合的，即 a+b+c = a+(b+c)，像减法都不满足结合率，a-b-c ≠ a-(b-c)，如果commute=ture，那么操作同时也是可交换的。function 是用户自定义的函数，函数必须具备四个参数，原型如下所示： void user_function_name( void * invec, // 被规约元素 1 所在缓冲区的首地址 void * inoutvec, // 被规约元素 2 所在缓冲区的首地址，返回结果要保存到这个数组里 int * len, // 数组长度 MPI_Datatype * datatype // 数据类型 ) 使用 MPI_Op_free 可以释放掉规约操作，下面是函数原型： int MPI_Op_free(MPI_Op *op) 下面是一个减法的示例，上面我们说了减法不满足结合率，所以结果和预期的是不一样的，不过我们可以从下面的示例看到如何自定义操作 #include #include #include \"mpi.h\" void my_prod(void * in, void * inout, int *len, MPI_Datatype *datatype) { int * in_tmp = (int *)in; int * inout_tmp = (int *)inout; int i; for(i = 0; i "},"数据处理.html":{"url":"数据处理.html","title":"数据类型","keywords":"","body":"MPI 数据类型 MPI 除了可以发送或接受连续的数据之外，还可以处理不连续的数据，其基本方法有两种，一是允许用户自定义新的数据类型（又称为派生数据类型），二是数据的打包与捷解包，即在发送方将不连续的数据打包到连续的区域，然后发送出去，在接收方将打包的连续数据解包到不连续的存储空间。 派生数据类型 连续复制类型 向量数据 索引数据类型 结构体数据类型 新类型递交和释放 地址函数 相关函数 打包和解包 派生数据类型 连续复制类型 通过 MPI_Type_contiguous 函数，我们可以把多个相同的数据类型合成一个数据类型，下面是函数原型： int MPI_Type_contiguous( int count, // 旧类型的个数 MPI_Datatype oldtype, // 旧数据类型 MPI_Datatype * newtype // 新数据类型 ) 下面是一个使用示例 #include #include #include \"mpi.h\" typedef struct _contiguous_type{ int a; int b; } contiguous_type; void cont_type() { int rank; contiguous_type data; MPI_Datatype newtype; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Type_contiguous(2, MPI_INT, &newtype); MPI_Type_commit(&newtype); if(rank == 0) { data.a = 1; data.b = 2; MPI_Send(&data, 1, newtype, 1, 99, MPI_COMM_WORLD); } if(rank == 1) { MPI_Recv(&data, 1, newtype, 0, 99, MPI_COMM_WORLD, &status); printf(\"data.a is %d and data.b is %d\\n\", data.a, data.b); } MPI_Finalize(); } int main() { cont_type(); } 向量数据 MPI_Type_vector 允许复制的数据之间有空隙，下面是函数原型： int MPI_Type_vector( int count, // 块的数量 int blocklength, // 每个块中所含元素的个数 int stride, // 各块第一个元素之间相隔的元素数 MPI_Datatype oldtype, // 旧数据类型 MPI_Datatype *newtype // 新数据类型 ) 为了更加直观的理解，我们给出 count=2, blocklength=2, stride=3 时的示例图。上面的是原始数据，下面的新数据类型所包含的数据。 MPI_Type_hvector 和 MPI_Type_vector 功能类似，只不过 MPI_Type_hvector 针对的是字节，下面是函数原型： int MPI_Type_hvector( int count, // 块的数量 int blocklength, // 每个块中所含元素的个数 int stride, // 各块第一个元素之间相隔的字节数 MPI_Datatype oldtype, // 旧数据类型 MPI_Datatype *newtype // 新数据类型 ) 下面是使用 MPI_Type_vector 的一个使用示例： void vector_type() { int rank; int n = 10; int buffer[10]; int i; MPI_Datatype newtype; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Type_vector(2, 2, 3, MPI_INT, &newtype); MPI_Type_commit(&newtype); if(rank == 0) { for(i = 0; i 索引数据类型 索引数据类型更加灵活，可以分别指定每个块中的数据量以及每个块的起始位置，下面是函数原型 int MPI_Type_indexed( int count, // 块的数量 int * array_of_blocklengths, // 每个块中所含元素的个数 int * array_of_displacements, // 各块偏移字节 MPI_Datatype oldtype, // 旧数据类型 MPI_Datatype *newtype // 新数据类型 ) 下面是 count=2, array_of_blocklengths=[1,2], array_of_displacements=[0,3] 的示意图： 下面是函数用法示例： void index_type() { int rank; int n = 10; int buffer[10]; int blocklength[2]; int index[2]; int i; MPI_Datatype newtype; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); blocklength[0] = 1; blocklength[1] = 2; index[0] = 0; index[1] = 3; MPI_Type_indexed(2, blocklength, index, MPI_INT, &newtype); MPI_Type_commit(&newtype); if(rank == 0) { for(i = 0; i 结构体数据类型 使用MPI_Type_struct 可以生成结构体类型，下面是函数原型 int MPI_Type_struct( int count, // 块的数量 int * array_of_blocklengths, // 每个块中所含元素的个数 MPI_Aint * array_of_displacements, // 各块偏移字节（注意是字节） MPI_Datatype oldtype, // 旧数据类型 MPI_Datatype *newtype // 新数据类型 ) 下面是使用示例 typedef struct _my_struct { double d; double d2; int i; char c; } my_struct; void struct_type() { int rank; int blocklength[3]; MPI_Aint index[3]; MPI_Datatype oldtype[3]; MPI_Datatype newtype; MPI_Status status; my_struct data; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); blocklength[0] = 2; blocklength[1] = 1; blocklength[2] = 1; index[0] = 0; index[1] = sizeof(MPI_DOUBLE) * 2; index[2] = sizeof(MPI_DOUBLE) * 2 + sizeof(MPI_INT); oldtype[0] = MPI_DOUBLE; oldtype[1] = MPI_INT; oldtype[2] = MPI_CHAR; MPI_Type_struct(3, blocklength, index, oldtype, &newtype); MPI_Type_commit(&newtype); if(rank == 0) { data.d = 2.0; data.d2 = 3.5; data.i = 4; data.c = 'c'; MPI_Send(&data, 1, newtype, 1, 99, MPI_COMM_WORLD); } if(rank == 1) { MPI_Recv(&data, 1, newtype, 0, 99, MPI_COMM_WORLD, &status); printf(\"data.d is %.2f, data.d2 is %.2f, i is %d, data.c is %c\\n\", data.d, data.d2, data.i, data.c); } MPI_Finalize(); } 新类型递交和释放 新定义的数据在使用之前，必须先使用MPI_Type_commit 递交给 MPI 系统，下面是函数原型： int MPI_Type_commit(MPI_Datatype * datatype) 如果需要释放已经递交的数据类型，可以使用MPI_Type_free，下面是函数原型： int MPI_Type_free(MPI_Datatype * datatype) 地址函数 MPI_ADdress 可以返回一个变量在内存中相对于预定义的地址 MPI_BOTTOM 偏移地址，下面是函数原型: int MPI_ADdress( void * location, // 内存地址 MPI_Aint *address // 相对于位置 MPI_BOTTOM 的偏移 ) 下面是使用方法 int buf[10]; MPI_Aint a1, a2; MPI_Get_address( &buf[0], &a1 ); MPI_Get_address( &buf[1], &a2 ); 相关函数 MPI_Type_extent 以字节为单位返回一个数据类型的跨度 extent，下面是函数原型 int MPI_Type_extent( MPI_Datatype datatype, // 数据类型 MPI_Aint * extent // 数据类型跨度 ) MPI_Type_size 以字节为单位，返回给定数据有用部分所占空间的大小，即跨度减去类型中的空隙后的空间大小，和 MPI_Type_extent 相比，MPI_Type_size 不包括由于对齐等原因导致的数据类型中的空隙所占的空间。下面是 MPI_Type_size 的函数原型： int MPI_Type_size( MPI_Datatype datatype, // 数据类型 int * size // 数据类型跨度 ) 通过 MPI_Get_count 和 MPI_Get_elements 可以返回接收的数据的个数，下面是两者的函数原型： int MPI_Get_elements( MPI_Status * status, // 接收操作返回的状态 MPI_Datatype datatype, // 接收操作使用的数据类型 int *count // 接收到的基本类型元素的个数 ) int MPI_Get_count( MPI_Status * status, // 接收操作返回的状态 MPI_Datatype datatype, // 接收操作使用的数据类型 int *count // 接收到的指定数据类型的个数 ) MPI_Get_elements 和 MPI_Get_count 不同的是，前者是以基本类型元素为单位的，后者是以指定的数据类型为单位的，假设接收一个结构体，结构体的定义如下 typedef struct _my_struct { double d; double d2; int i; char c; } my_struct; 那么使用 MPI_Get_elements 返回的结果就是 4 (两个 double， 一个int，一个char)，而使用 MPI_Get_count 返回的结果则是 1 。 打包和解包 打包和解包是另外一种灵活发送数据的方式，在发送之前显式的将数据包装到一个连续的缓冲区中，在接受之后从连续缓冲区中解包。 MPI_Pack 完成打包操作，它首先将数据放到一个缓冲区中，发送时会发送该缓冲区。下面是函数原型： int MPI_Pack( void * inbuf, // 输入缓冲区的起始地址 int incount, // 输入数据的个数 MPI_Datatype datatype, // 每个输入数据的类型 void * packbuf, // 打包缓冲区的起始地址 int outcount, // 打包缓冲区的大小 int * position, // 缓冲区当前的位置 MPI_Comm comm // 通信域 ) MPI_Unpack 完成解包操作，它把数据首先接收到解包缓冲区里，然后在把解包缓冲区的数据解包成对应的值。下面是函数原型： int MPI_Unpack( void * packbuf, // 解包缓冲区的起始地址 int insize, // 解包缓冲区的大小 int *position, // 缓冲区当前位置 void * outbuf, // 解包数据的起始地址 int outcount, // 解包数据的个数 MPI_Datatype datatype, // 解包数据的类型 MPI_Comm comm // 通信域 ) 通过对不同位置的数据调用打包操作，就可以将不连续的消息放到一个连续的空间里。同理通过多次调用解包操作，就可以将缓冲区的数据解包到不同的位置。需要注意的一点是在发送和接收的时候数据类型要使用MPI_PACKED MPI_Pack_size 可以计算打包 incount 个 datatype 数据类型需要空间的大小。该调用返回的是一个上界，而不是一个精确界，这是因为包装一个消息所需的精确空间可能依赖于上下文。下面是函数原型： int MPI_Pack_size( int incount, // 指定的数据类型的个数 MPI_Datatype datatype, // 数据类型 MPI_Comm comm, // 通信域 int * size // 以字节为单位，incount 个 datatype 数据类型数据需要的空间 ) 下面是使用 MPI_Pack 和 MPI_Unpack的一个示例 #include #include #include \"mpi.h\" void pack_and_unpack() { int rank; int i; double d; char c; char buffer[20]; int position = 0; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { i = 1; d = 4.5; c = 'a'; MPI_Pack(&i, 1, MPI_INT, buffer, 20, &position, MPI_COMM_WORLD); MPI_Pack(&d, 1, MPI_DOUBLE, buffer, 20, &position, MPI_COMM_WORLD); MPI_Pack(&c, 1, MPI_CHAR, buffer, 20, &position, MPI_COMM_WORLD); printf(\"postion is %d\\n\", position); MPI_Send(buffer, position, MPI_PACKED, 1, 99, MPI_COMM_WORLD); } if(rank == 1) { MPI_Recv(buffer, 20, MPI_PACKED, 0, 99, MPI_COMM_WORLD, &status); MPI_Unpack(buffer, 20, &position, &i, 1, MPI_INT, MPI_COMM_WORLD); MPI_Unpack(buffer, 20, &position, &d, 1, MPI_DOUBLE, MPI_COMM_WORLD); MPI_Unpack(buffer, 20, &position, &c, 1, MPI_CHAR, MPI_COMM_WORLD); printf(\"i is %d, d id %.2f, c is %c\\n\", i, d, c); } MPI_Finalize(); } int main() { pack_and_unpack(); } "},"openmp.html":{"url":"openmp.html","title":"OpenMP","keywords":"","body":"OpenMP 学习笔记 Using OpenMP: Portable Shared Memory Parallel Programming OpenMP 编译原理及实现技术 这里主要参考 Using OpenMP 这本书。 "},"openmp-基本使用.html":{"url":"openmp-基本使用.html","title":"OpenMP 基本使用","keywords":"","body":"基本使用 前言 执行模式 HelloWorld 前言 OpenMP 是基于共享内存模式的一种并行编程模型, 使用十分方便, 只需要串行程序中加入OpenMP预处理指令, 就可以实现串行程序的并行化. 这里主要进行一些学习记录, 使用的书籍为: Using OpenMP: Portable Shared Memory Parallel Programming 和OpenMP编译原理及实现技术 执行模式 OpenMP编程模型是以线程为基础的, OpenMP 执行模式采用fork-join的方式, 其中fork创建新线程或者唤醒已有的线程, join将多个线程合并. 在程序执行的时候, 只有主线程在运行, 当遇到需要并行计算的区域, 会派生出线程来并行执行, 在并行执行的时候, 主线程和派生线程共同工作, 在并行代码结束后, 派生线程退出或者挂起, 不再工作, 控制流程回到单独的线程中下. 下图说明了fork-join模型的执行流程 HelloWorld #include #include #include int main() { #pragma omp parallel { printf(\"The parallel region is executed by thread %d\\n\", omp_get_thread_num()); if ( omp_get_thread_num() == 2 ) { printf(\" Thread %d does things differently\\n\", omp_get_thread_num()); } } return 0; } 然后使用gcc编译程序, 为了使用OpenMP需要加上-fopenmp选项 gcc -fopenmp helloworld.c -o helloworld 下面是执行结果 The parallel region is executed by thread 2 Thread 2 does things differently The parallel region is executed by thread 1 The parallel region is executed by thread 3 The parallel region is executed by thread 0 在上面的代码中, 程序开了四个线程, 其编号分别为0-3, 线程之间的执行是没有顺序的, 当下次再执行上述代码输出的结果可能就会不一样. 在上面的代码中, 我们并没有显式的指定线程的数量, OpenMP会根据下面的规则确定线程数量: num_threads的设置 omp_set_num_threads()库函数的设置 OMP_NUM_THREADS环境变量的设置 编译器默认实现（一般而言，默认实现的是总线程数等于处理器的核心数） 上面规则的优先级是依次递减的. 如果1 2 3 都没有指定, 那么就会使用规则4 参考文章OpenMP Tutorial学习笔记(4)OpenMP指令之同步构造（Parallel）OpenMP学习笔记：基本概念 "},"编译制导指令.html":{"url":"编译制导指令.html","title":"编译制导指令","keywords":"","body":"编译制导指令 前言 Parallel Construct(并行域结构) Work-sharing Construct(任务分担结构) for sections single Combined Parallel Work-Sharing Constructs Clauses to Control Parallel and Work-Sharing Constructs shared private lastprivate firstprivate default nowait schedule Synchronization Constructs(同步) barrier ordered critical atomic locks master 前言 OpenMP通过在串行程序中插入编译制导指令, 来实现并行化, 支持OpenMP的编译器可以识别, 处理这些指令并实现对应的功能. 所有的编译制导指令都是以#pragma omp开始, 后面跟具体的功能指令(directive)或者命令. 一般格式如下所示: #pragma omp directive [clause [[,] clause]...] structured block Parallel Construct(并行域结构) 为了使程序可以并行执行, 我们首先要构造一个并行域(parallel region), 在这里我们使用parallel指令来实现并行域的构造, 其语法形式如下 #pragma omp parallel [clause [[,] clause]...] structured block 我们看到其实就是在omp后面加了一个parallel关键字, 该指令主要作用就是用来构造并行域, 创建线程组并且并发执行任务. 需要注意的是该指令只保证代码以并行的方式执行, 但是并不负责线程之间的任务分发. 在并行域执行结束之后, 会有一个隐式的屏障(barrier), 来同步所有的该区域内的所有线程. 下面是一个使用示例: void parallel_construct() { #pragma omp parallel { printf(\"Hello from thread %d\\n\", omp_get_thread_num()); } } 其中omp_get_thread_num()用来获取当前线程的编号, 该函数是定义在中的. 输出结果如下: Hello from thread 1 Hello from thread 3 Hello from thread 0 Hello from thread 2 parallel指令后面可以跟一些子句(clause), 如下所示 if(scalar-expression) num_threads(integer-expression) private(list) firstprivate(list) shared(list) default(none | shared) copyin(list) reduction(operator:list) 在后面会介绍这些从句的用法 Work-sharing Construct(任务分担结构) 任务分担指令主要用于为线程分配不同的任务, 一个任务分担域(work-sharing region)必须要和一个活跃(active)的并行域(parellel region)关联, 如果任务分担指令处于一个不活跃的并行域或者处于一个串行域中, 那么该指令就会被忽略. 在C/C++有3个任务分担指令: for、sections、single, 严格意义上讲只有for和sections是任务分担指令, 而single只是协助任务分担的指令. for 用于for循环中, 将不同的循环分配给不同的线程, 语法如下所示: #pragma omp for [clause[[,] clause]...] for-loop 下面是一个使用示例: void parallel_for() { int n = 9; int i = 0; #pragma omp parallel shared(n) private(i) { #pragma omp for for(i = 0; i 下面是程序执行结果 Thread 2 executes loop iteration 5 Thread 2 executes loop iteration 6 Thread 3 executes loop iteration 7 Thread 3 executes loop iteration 8 Thread 0 executes loop iteration 0 Thread 0 executes loop iteration 1 Thread 0 executes loop iteration 2 Thread 1 executes loop iteration 3 Thread 1 executes loop iteration 4 在上面的程序中共有4个线程执行9次循环, 线程0分到了3次, 剩余的线程分到了2次, 这是一种常用的调度方式, 即假设有n次循环迭代, t个线程, 那么每个线程分配到n/t 或者 n/t + 1 次连续的迭代计算, 但是某些情况下使用这种方式并不是最好的选择, 我们可以使用schedule 来指定调度方式, 在后面会具体介绍. 下面是for 指令后面可以跟的一些子句: private(list) fistprivate(list) lastprivate(list) reduction(operator:list) ordered schedule(kind[,chunk_size]) nowait sections sections指令可以为不同的线程分配不同的任务, 语法如下所示: #pragma omp sections [clause[[,] clause]...] { [#pragma omp section] structured block [#pragma omp section] structured block ... } 从上面的代码中我们可以看到, sections将代码分为多个section, 每个线程处理一个section, 下面是一个使用示例: /** * 使用#pragma omp sections 和 #pragma omp section, 来使不同的线程执行不同的任务 * 如果线程数量大于section数量, 那么多余的线程会处于空闲状态(idle) * 如果线程数量少于section数量, 那么一个线程会执行多个section代码 */ void funcA() { printf(\"In funcA: this section is executed by thread %d\\n\", omp_get_thread_num()); } void funcB() { printf(\"In funcB: this section is executed by thread %d\\n\", omp_get_thread_num()); } void parallel_section() { #pragma omp parallel { #pragma omp sections { #pragma omp section { (void)funcA(); } #pragma omp section { (void)funcB(); } } } } 下面是执行结果: In funcA: this section is executed by thread 3 In funcB: this section is executed by thread 0 下面是sections后面可以跟的一些子句 private(list) firstprivate(list) lastprivate(list) reduction(operator:list) nowait single single 指令用来指定某段代码块只能被一个线程来执行, 如果没有nowait字句, 所有线程在 single 指令结束处隐市同步点同步, 如果single指令有nowait从句, 则别的线程直接往下执行. 不过single指令并不指定哪个线程来执行. 语法如下所示: #pragma omp single [clause[[,] clause]...] structured block 下面是一个使用示例 void parallel_single() { int a = 0, n = 10, i; int b[n]; #pragma omp parallel shared(a, b) private(i) { // 只有一个线程会执行这段代码, 其他线程会等待该线程执行完毕 #pragma omp single { a = 10; printf(\"Single construct executed by thread %d\\n\", omp_get_thread_num()); } // A barrier is automatically inserted here #pragma omp for for(i = 0; i 下面是执行结果: Single construct executed by thread 2 After the parallel region: b[0] = 10 b[1] = 10 b[2] = 10 b[3] = 10 b[4] = 10 b[5] = 10 b[6] = 10 b[7] = 10 b[8] = 10 b[9] = 10 下面是single指令后面可以跟随的子句: private(list) firstprivate(list) copyprivate(list) nowait Combined Parallel Work-Sharing Constructs 将parallel指令和work-sharing指令结合起来, 使代码更加简洁. 如下面的代码 #pragma omp parallel { #pragma omp for for(.....) } 可以写为 #pragma omp parallel for for(.....) 具体的参见下图: 使用这些组合结构体(combined constructs)不仅增加程序的可读性, 而且对程序的性能有一定的帮助. 当使用这些组合结构体的时候, 编译器可以知道下一步要做什么, 从而可能会生成更高效的代码. Clauses to Control Parallel and Work-Sharing Constructs OpenMP指令后面可以跟一些子句, 用来控制构造器的行为. 下面介绍一些常用的子句. shared shared子句用来指定哪些数据是在线程之间共享的, 语法形式为shared(list), 下面是其使用方法: #pragma omp parallel for shared(a) for(i = 0; i 在并行域中使用共享变量时, 如果存在写操作, 需要对共享变量加以保存, 因为可能存在多个线程同时修改共享变量或者在一个线程读取共享变量时另外一个变量在更新共享变量的情况, 而这些情况都可能会引起程序错误. private private子句用来指定哪些数据是线程私有的, 即每个线程具有变量的私有副本, 线程之间互不影响. 其语法形式为private(list), 使用方法如下: void test_private() { int n = 8; int i=2, a = 3; // i,a 定义为private之后不改变原先的值 #pragma omp parallel for private(i, a) for ( i = 0; i 下面是程序运行结果: In for: thread 2 has a value of a = 5 for i = 4 In for: thread 2 has a value of a = 6 for i = 5 In for: thread 3 has a value of a = 7 for i = 6 In for: thread 3 has a value of a = 8 for i = 7 In for: thread 0 has a value of a = 1 for i = 0 In for: thread 0 has a value of a = 2 for i = 1 In for: thread 1 has a value of a = 3 for i = 2 In for: thread 1 has a value of a = 4 for i = 3 Out for: thread 0 has a value of a = 3 for i = 2 对于private子句中的变量, 需要注意一下两点: 不论该变量之前有没有初始值, 在进入并行域之后都是未初始化的. 并行域中对变量的修改只在该域中起作用, 当离开并行域后, 变量值仍然是未进入并行域之前的值 lastprivate lastprivate会在退出并行域时, 将其修饰变量的最后取值(last value)保存下来, 可以作用于 for 和 sections, 语法格式为lastprivate(list). 关于last value的定义: 如果是作用于for指令, 那么last value就是指串行执行的最后一次循环的值；如果是作用于sections指令, 那么last value就是执行完最后一个包含该变量的section之后的值. 使用方法如下: void test_last_private() { int n = 8; int i=2, a = 3; // lastprivate 将for中最后一次循环(i == n-1) a 的值赋给a #pragma omp parallel for private(i) lastprivate(a) for ( i = 0; i 程序执行结果为: In for: thread 3 has a value of a = 7 for i = 6 In for: thread 3 has a value of a = 8 for i = 7 In for: thread 2 has a value of a = 5 for i = 4 In for: thread 2 has a value of a = 6 for i = 5 In for: thread 1 has a value of a = 3 for i = 2 In for: thread 0 has a value of a = 1 for i = 0 In for: thread 0 has a value of a = 2 for i = 1 In for: thread 1 has a value of a = 4 for i = 3 Out for: thread 0 has a value of a = 8 for i = 2 firstprivate firstprivate 子句用于为private变量提供初始值. 使用firstprivate修饰的变量会使用在前面定义的同名变量的值作为其初始值. 语法形式为firstprivate(list), 使用方法如下: void test_first_private() { int n = 8; int i=0, a[n]; for(i = 0; i 执行结果如下: thread 0: a[0] is 1 thread 0: a[1] is 2 thread 2: a[4] is 5 thread 2: a[5] is 6 thread 3: a[6] is 7 thread 3: a[7] is 8 thread 1: a[2] is 3 thread 1: a[3] is 4 default default子句用于设置变量默认的data-sharing属性, 在C/C++中只支持default(none | shared), 其中default(shared)设置所有的变量默认为共享的, default(none)取消变量的默认属性, 需要显示指定变量是共享的还是私有的. nowait 用于取消任务分担结构(work-sharing constructs)中的隐式屏障(implicit barrier), 下面是一个使用示例: void test_nowait() { int i, n =6; #pragma omp parallel { #pragma omp for nowait for(i = 0; i 如果第一个 for 后面没有加 nowait , 那么输出如下所示: thread 3: ++++ thread 0: ++++ thread 0: ++++ thread 2: ++++ thread 1: ++++ thread 1: ++++ thread 0: ---- thread 0: ---- thread 3: ---- thread 1: ---- thread 1: ---- thread 2: ---- 因为for指令有一个隐式的屏障, 会同步所有的线程直到第一个for循环执行完, 再继续往下执行. 加上 nowait 之后就消除了这个屏障, 使线程执行完第一个for循环之后无需再等待其他线程就可以去执行第二个for循环的内容, 下面是加上nowait之后的输出: thread 2: ++++ thread 2: ---- thread 1: ++++ thread 1: ++++ thread 1: ---- thread 1: ---- thread 3: ++++ thread 3: ---- thread 0: ++++ thread 0: ++++ thread 0: ---- thread 0: ---- 使用nowait时需要注意前后for之间有没有依赖关系, 如果第二个for循环需要用到第一个for循环的结果, 那么使用nowait就可能会造成程序错误. schedule schedule子句只作用于循环结构(loop construct), 它用来设置循环任务的调度方式. 语法形式为schedule(kind[,chunk_size]), 其中kind的取值有 static, dynamic, guided, auto, runtime, chunk_size是可选项,可以指定也可以不指定. 下面是使用方法: void test_schedule() { int i, n = 10; #pragma omp parallel for default(none) schedule(static, 2) \\ private(i) shared(n) for(i = 0; i 下面介绍一下各个取值的含义, 假设有n次循环, t个线程 static静态调度, 如果不指定chunk_size , 那么会为每个线程分配 n/t 或者 n/t+1(不能除尽)次连续的迭代计算, 如果指定了 chunk_size, 那么每次为线程分配chunk_size次迭代计算, 如果第一轮没有分配完, 则循环进行下一轮分配, 假设n=8, t=4, 下表给出了chunk_size未指定、等于1、等于3时的分配情况. 线程编号\\chunk_size 未指定 chunk_size = 1 chunk_size = 3 0 0 1 0 4 0 1 2 1 2 3 1 5 3 4 5 2 4 5 2 6 6 7 3 6 7 3 7 　 dynamic 动态调度, 动态为线程分配迭代计算, 只要线程空闲就为其分配任务, 计算快的线程分配到更多的迭代. 如果不指定chunk_size参数, 则每次为一个线程分配一次迭代循环(相当于chunk_size=1), 若指定chunk_size, 则每次为一个线程分配chunk_size次迭代循环. 在动态调度下, 分配结果是不固定的, 重复执行同一个程序, 每次的分配结果一般来说是不同的, 下面给出n=12, t=4时, chunk_size未指定、等于2时的分配情况(运行两次) 线程编号\\chunk_size 未指定(第一次) 未指定(第二次) chunk_size=2(第一次) chunk_size = 2(第二次) 0 2 0 4 5 8 9 10 11 0 1 1 0 4 5 6 7 8 9 10 11 3 0 1 4 5 2 3 1 4 5 6 7 8 9 10 11 2 3 6 7 3 1 2 6 7 2 3 8 9 10 11 使用动态动态可以一定程度减少负载不均衡的问题, 但是需要注意任务动态申请时也会有一定的开销. guided guided调度是一种指定性的启发式自调度方法. 开始时每个线程会分配到较大的迭代块, 之后分配到的迭代块的大小会逐渐递减. 如果指定chunk_size, 则迭代块会按指数级下降到指定的chunk_size大小, 如果没有指定size参数, 那么迭代块大小最小会降到1(相当于chunk_size=1). 和动态调度一样, 执行块的线程会分到更多的任务, 不同的是这里迭代块的大小是变化的. 同样使用guided调度的分配结果也不是固定的, 重复执行会得到不同的分配结果. 下面给出n=20, t=4, chunk_size未指定、chunk_size=3时的分配情况(执行两次) 线程编号\\chunk_size 未指定(第一次) 未指定(第二次) chunk_size=3(第一次) chunk_size = 3(第二次) 0 12 13 0 1 2 3 4 0 1 2 3 4 5 6 7 8 　18 19 1 5 6 7 8 　16 17　 18 19 5 6 7 8 9 10 11 9 10 11 2 0 1 2 3 4　 14 15 9 10 11　 14 15 16　 17 18 19 5 6 7 8 　15 16 17 　18 19 0 1 2 3 4 　 15 16 17 3 9 10 11 12 13 12 13 14 12 13 14 当设置chunk_size=3时, 因为最后只剩下18、19两次循环, 所以最后执行的那个线程只分配到2次循环. 下面的图展示了当循环次数为200次, 线程数量为4时, static 、 (dynamic,7) 、(guided, 7) 3种调度方式的分配情况 runtime 运行时调度, 并不是一种真正的调度方式, 在运行时同时环境变量OMP_SCHEDULE来确定调度类型, 最终的调度类型仍为上面的3种调度方式之一. 在bash下可以使用下面的方式设置: export OMP_SCHEDULE=\"static\" auto 将选择的权利赋予编译器, 让编译器自己选择合适的调度决策. 负载不均衡 在for循环中, 如果每次循环之间花费的时间是不同的, 那么就可能出现负载不均衡问题, 下面代码模拟一下这种情况, void test_schedule() { int i,j, n = 10; double start, end; GET_TIME(start); #pragma omp parallel for default(none) schedule(static) \\ private(i, j) shared(n) for(i = 0; i GET_TIME的定义如下: #ifndef _TIMER_H_ #define _TIMER_H_ #include #include #include #define GET_TIME(now) { \\ struct timeval t; \\ gettimeofday(&t, NULL); \\ now = t.tv_sec + t.tv_usec/1000000.0; \\ } #endif 在上面的代码中, 对于第一个for循环, i越大, 循环消耗的时间越多, 下面是n=10时的输出 static : use time 1.74s static,2 : use time 1.84s dynamic : use time 1.53s dynamic,2: use time 1.84s guided : use time 1.63s guided,2 : use time 1.53s 下面是n=20的输出 static : use time 8.67s static,2 : use time 6.42s dynamic : use time 5.62s dynamic,2: use time 6.43s guided : use time 5.92s guided,2 : use time 6.43s 对于static调度, 如果不指定chunk_size的值, 则会将最后几次循环分给最后一个线程, 而最后几次循环是最耗时的, 其他线程执行完各自的工作需要等待这个线程执行完毕, 浪费了系统资源, 这就造成了负载不均衡. dynamic和guided可以在一定程度上减轻负载不均衡, 但是也不是绝对的, 最终选用哪种方式还是要根据具体的问题. Synchronization Constructs(同步) 同步指令主要用来控制多个线程之间对于共享变量的访问. 它可以保证线程以一定的顺序更新共享变量, 或者保证两个或多个线程不同时修改共享变量. barrier 同步路障(barrier), 当线程遇到路障时必须要停下等待, 直到并行区域中的所有线程都到达路障点, 线程才继续往下执行. 在每一个并行域和任务分担域的结束处都会有一个隐式的同步路障, 即在parallel、for、sections、single构造的区域之后会有一个隐式的路障, 因此在很多时候我们无需显示的插入路障. 下面是语法形式: #pragma omp barrier 下面是一个使用示例: void print_time(int tid, char* s ) { int len = 10; char buf[len]; NOW_TIME(buf, len); printf(\"Thread %d %s at %s\\n\", tid, s, buf); } void test_barrier() { int tid; #pragma omp parallel private(tid) { tid = omp_get_thread_num(); if(tid 其中NOW_TIME的定义如下 #ifndef _TIMER_H_ #define _TIMER_H_ #include #include #include #define NOW_TIME(buf, len) { \\ time_t nowtime; \\ nowtime = time(NULL); \\ struct tm *local; \\ local = localtime(&nowtime); \\ strftime(buf, len, \"%H:%M:%S\", local); \\ } #endif 在上面的代码中有一半的线程(tid #pragma omp barrier Thread 3 before barrier at 16:55:44 Thread 2 before barrier at 16:55:44 Thread 3 after barrier at 16:55:44 Thread 2 after barrier at 16:55:44 Thread 1 before barrier at 16:55:47 Thread 0 before barrier at 16:55:47 Thread 0 after barrier at 16:55:47 Thread 1 after barrier at 16:55:47 下面上加上路障的输出结果: Thread 3 before barrier at 17:05:29 Thread 2 before barrier at 17:05:29 Thread 0 before barrier at 17:05:32 Thread 1 before barrier at 17:05:32 Thread 0 after barrier at 17:05:32 Thread 1 after barrier at 17:05:32 Thread 2 after barrier at 17:05:32 Thread 3 after barrier at 17:05:32 通过对比我们可以看出, 加上路障之后, 各线程要在路障点同步一次, 然后再继续往下执行. ordered ordered结构允许在并行域中以串行的顺序执行一段代码, 如果我们在并行域中想按照顺序打印被不同的线程计算的数据, 就可以使用这个子句, 下面是语法形式 #pragma omp ordered structured block 在使用时需要注意一下两点 ordered 只作用于循环结构(loop construct) 使用ordered时需要在构造并行域的时候加上ordered子句, 如下面所示#pragma omp parallel for ordered 下面是一个使用示例 void test_order() { int i, tid, n = 5; int a[n]; for(i = 0; i 下面是程序运行结果: Thread 0 updates a[0] Thread 2 updates a[2] Thread 1 updates a[3] Thread 0 printf value of a[0] = 0 Thread 0 updates a[4] Thread 3 updates a[1] Thread 3 printf value of a[1] = 1 Thread 2 printf value of a[2] = 2 Thread 1 printf value of a[3] = 3 Thread 0 printf value of a[4] = 4 从输出结果我们可以看到, 在update时是以乱序的顺序更新, 但是在打印时是以串行顺序的形式打印. critical 临界区(critical), 临界区保证在任意一个时间段内只有一个线程执行该区域中的代码, 一个线程要进入临界区必须要等待临界区处于空闲状态, 下面是语法形式 #pragma omp critical [(name)] structured block 其中name是为临界区指定的一个名字. 下面是一个求和的使用示例, 注意这里只是用来说明临界区的作用, 对于求和操作我们可以使用reduction指令 void test_critical() { int n = 100, sum = 0, sumLocal, i, tid; int a[n]; for(i = 0; i 在该代码中, sum是全局的, localSum是每个线程执行完各自的求和任务后的和值, 将每个线程的sumLocal加给sum, 就是最后的和值. 在执行sum+=sunLocal操作时, 需要保证一次只有一个线程执行该操作, 因此这里使用了临界区, 下面是运行结果: Thread 2: sumLocal = 1550 sum =1550 Thread 3: sumLocal = 2175 sum =3725 Thread 1: sumLocal = 925 sum =4650 Thread 0: sumLocal = 300 sum =4950 Value of sum after parallel region: 4950 下面是将临界区去掉的运行结果(运行结果不是固定的, 这里只是其中一种情况): Thread 2: sumLocal = 1550 sum =1550 Thread 3: sumLocal = 2175 sum =2475 Thread 1: sumLocal = 925 sum =925 Thread 0: sumLocal = 300 sum =300 Value of sum after parallel region: 2475 通过对比我们可以看到临界区保证了程序的正确性. atomic 原子操作, 可以锁定一个特殊的存储单元(可以是一个单独的变量,也可以是数组元素), 使得该存储单元只能原子的更新, 而不允许让多个线程同时去写. atomic只能作用于单条赋值语句, 而不能作用于代码块. 语法形式为: #pragma omp atomic statement 在C/C++中, statement必须是下列形式之一 x++, x--, ++x, --x x binop= expr 其中binop是二元操作符: +, -, *, /, &, ^, |, >之一 atomic的可以有效的利用的硬件的原子操作机制来控制多个线程对共享变量的写操作, 效率较高, 下面是一个使用示例 void test_atomic() { int counter=0, n = 1000000, i; #pragma omp parallel for shared(counter, n) for(i = 0; i 对于下面的情况 #pragma omp atomic ic += func(); atomic只保证ic的更新是原子的, 即不会被多个线程同时更新, 但是不会保证func函数的执行是原子的, 即多个线程可以同时执行func函数, 如果要使func的执行也是原子的, 可以使用临界区. locks 互斥锁, 提供了一个更底层的机制来处理同步的问题, 比使用critical和atomic有更多的灵活性, 但也相对更加复杂一些. openmp提供了两种类型的锁--简单锁(simple locks) 和 嵌套锁(nested locks), 对于简单锁来说, 如果其处于锁住的状态, 那么它就可能无法再次被上锁. 而对于嵌套锁来说, 可以被同一个线程上锁多次. 下面是简单锁的几个函数 void omp_init_lock(omp_lock_t *lck) // 初始化互斥锁 void omp_destroy_lock(omp_lock_t *lck) // 销毁互斥锁 void omp_set_lock(omp_lock_t *lck) // 获得互斥锁 void omp_unset_lock(omp_lock_t *lck) // 释放互斥锁 bool omp_test_lock(omp_lock_t *lck) // 尝试获得互斥锁, 如果获得成功返回true, 否则返回false 嵌套锁的函数和简单锁略有不同, 如下所示 void omp_init_nest_lock(omp_nest_lock_t *lck) void omp_destroy_nest_lock(omp_nest_lock_t *lck) void omp_set_nest_lock(omp_nest_lock_t *lck) void omp_unset_nest_lock(omp_nest_lock_t *lck) void omp_test_nest_lock(omp_nest_lock_t *lck) 下面是一个使用示例 void test_lock() { omp_lock_t lock; int i,n = 4; omp_init_lock(&lock); #pragma omp parallel for for(i = 0; i 其中system(\"sleep 0.1\") 是为了两次的输出有个间隔, 以便和不加锁时的情况进行对比. 下面是程序的输出: Thread 1: + Thread 1: - Thread 2: + Thread 2: - Thread 3: + Thread 3: - Thread 0: + Thread 0: - 下面是去掉锁的输出 Thread 3: + Thread 2: + Thread 0: + Thread 1: + Thread 2: - Thread 3: - Thread 0: - Thread 1: - master 用于指定一段代码只由主线程执行. master指令和single指令的区别如下: master指令包含的代码段只有主线程执行, 而single指令包含的代码可以由任意一个线程执行. master指令在结束处没有隐式同步, 也不可以使用nowait从句 下面是一个使用示例: void test_master() { int a, i, n = 5; int b[n]; #pragma omp parallel shared(a, b) private(i) { #pragma omp master { a = 10; printf(\"Master construct is executed by thread %d\\n\", omp_get_thread_num()); } #pragma omp barrier #pragma omp for for(i = 0; i 下面是输出结果 Master construct is executed by thread 0 After the parallel region: b[0] = 10 b[1] = 10 b[2] = 10 b[3] = 10 b[4] = 10 "},"运行环境交互.html":{"url":"运行环境交互.html","title":"运行环境交互","keywords":"","body":"与运行环境交互 Internal Control Variables nthread-var dyn-var nest-var def-sched-var 其它函数 Internal Control Variables OpenMP标准定义了内部控制变量(internal control variables), 这些变量可以影响程序运行时的行为, 但是它们不能被直接访问或者修改, 我们需要通过OpenMP函数或者环境变量来访问或者修改它们, 下面是被定义的内部变量 nthread-var : 存储并行域的线程数量 dyn-var : 控制在并行域执行时是否可以动态调整线程的数量 nest-var : 控制在并行域执行时是否允许嵌套并行 run-sched-var : 存储在循环域(loop regions)使用 runtime 调度子句时的调度类型 def-sched-var : 存储对于循环域默认的调度类型 nthread-var 我们可以通过以下几种方式来设置线程数量 OMP_NUM_THREADS 我们可以在命令行(command line)下设置OMP_NUM_THREADS环境变量的值, 而该变量的值用于初始化 nthread-var 变量. omp_set_num_threads 在程序中我们可以使用omp_set_num_threads函数来设置线程数量, 语法形式为omp_set_num_threads(integer) num_threads 最后我们可以在构造并行域的时候使用num_threads子句来控制线程的数量 上面的三种方式优先级依次递增, 另外在程序执行时, 我们可以使用下面几个函数获得线程的数量信息 omp_get_max_threads : 获得可以使用的最大线程数量, 数量是可以确定的, 与在串行域还是并行域调用无关. omp_get_num_threads: 获得当前运行线程的数量, 如果不在并行域内调用则返回1 omp_get_thread_num: 获得线程的编号, 从0开始 下面是一个使用示例 void test_numthread() { printf(\"max thread nums is %d\\n\", omp_get_max_threads()); printf(\"omp_get_num_threads: out parallel region is %d\\n\", omp_get_num_threads()); omp_set_num_threads(2); printf(\"after omp_set_num_threads: max thread nums is %d\\n\", omp_get_max_threads()); #pragma omp parallel { #pragma omp master { printf(\"omp_get_num_threads: in parallel region is %d\\n\\n\", omp_get_num_threads()); } printf(\"1: thread %d is running\\n\", omp_get_thread_num()); } printf(\"\\n\"); #pragma omp parallel num_threads(3) { printf(\"2: thread %d is running\\n\", omp_get_thread_num()); } } 下面是程序运行结果: max thread nums is 4 omp_get_num_threads: out parallel region is 1 after omp_set_num_threads: max thread nums is 2 omp_get_num_threads: in parallel region is 2 1: thread 0 is running 1: thread 1 is running 2: thread 0 is running 2: thread 1 is running 2: thread 2 is running dyn-var dyn-var控制程序是否在运行中是都可以动态的调整线程的数量, 可以通过下面的两种方式来设置 OMP_DYNAMIC 通过OMP_DYNAMIC环境变量来控制, 如果设为true, 则代表允许动态调整, 设为false则不可以 omp_set_dynamic 通过omp_set_dynamic函数, omp_set_dynamic(1)表示允许, omp_set_dynamic(0)表示不可以, 注意omp_set_dynamic可以传入其他非负整数, 但是作用和输入1是相同的, 都是表示true. 可以通过omp_get_dynamic来获得dynamic的状态, 返回值为0和1, 下面是一个使用示例: void test_dynamic() { printf(\"dynamic state is %d\\n\", omp_get_dynamic()); omp_set_num_threads(6); #pragma omp parallel { printf(\"thread %d is running\\n\", omp_get_thread_num()); } omp_set_dynamic(1); printf(\"\\n\"); printf(\"dynamic state is %d\\n\", omp_get_dynamic()); #pragma omp parallel { printf(\"thread %d is running\\n\", omp_get_thread_num()); } } 下面是输出结果: dynamic state is 0 thread 3 is running thread 4 is running thread 0 is running thread 5 is running thread 1 is running thread 2 is running dynamic state is 1 thread 3 is running thread 1 is running thread 2 is running thread 0 is running 当允许动态调整之后, 第二个for循环只打印了四次,即只有四个线程在执行. 一般来说动态调整会根据系统资源来确定线程数量, 大多数情况下会生成和CPU数目相同的线程. 还有一点, 动态调整时生成的线程不会超过当前运行环境所允许的最大线程数量, 在上面的代码中, 如果将omp_set_num_threads(6)改为omp_set_num_threads(2), 那么动态调整时最多只会生成两个线程. nest-var nest-var用来控制是否可以嵌套并行, 可以通过下面两种方式来设置 OMP_NESTED 通过设置OMP_NESTED环境变量, true表示允许, false表示不允许 omp_set_nested 通过omp_set_nested函数, omp_set_nested(1或其他非负整数)表示允许, omp_set_nested(0)表示不允许. 可以通过omp_get_nested来获得是否可以嵌套并行, 返回值是0或1, 下面是一个使用示例: void test_nested() { int tid; printf(\"nested state is %d\\n\", omp_get_nested()); #pragma omp parallel num_threads(2) private(tid) { tid = omp_get_thread_num(); printf(\"In outer parallel region: thread %d is running\\n\", tid); #pragma omp parallel num_threads(2) firstprivate(tid) { printf(\"In nested parallel region: thread %d is running and outer thread is %d\\n\", omp_get_thread_num(), tid); } } omp_set_nested(1); printf(\"\\n\"); printf(\"nested state is %d\\n\", omp_get_nested()); #pragma omp parallel num_threads(2) private(tid) { tid = omp_get_thread_num(); printf(\"In outer parallel region: thread %d is running\\n\", tid); #pragma omp parallel num_threads(2) { printf(\"In nested parallel region: thread %d is running and outer thread is %d\\n\", omp_get_thread_num(), tid); } } } 下面是程序运行结果: nested state is 0 In outer parallel region: thread 0 is running In nested parallel region: thread 0 is running and outer thread is 0 In outer parallel region: thread 1 is running In nested parallel region: thread 0 is running and outer thread is 1 nested state is 1 In outer parallel region: thread 1 is running In outer parallel region: thread 0 is running In nested parallel region: thread 0 is running and outer thread is 0 In nested parallel region: thread 0 is running and outer thread is 1 In nested parallel region: thread 1 is running and outer thread is 1 In nested parallel region: thread 1 is running and outer thread is 0 当不允许嵌套并行时, 在并行域内创建的新并行域会以单线程执行, 而允许嵌套并行之后, 会在并行域内创建新的并行域, 为其分配新的线程执行. def-sched-var 通过OMP_SCHEDULE环境变量, 可以设置循环调度为runtime时的调度类型, 具体参见这里 其它函数 omp_get_num_procs 获得程序中可以使用的处理器数量, 是一个全局的值 omp_in_parallel 判断是否在一个活跃的并行域(active parallel region)内, 返回0或1. "},"更多指令和子句.html":{"url":"更多指令和子句.html","title":"更多指令和子句","keywords":"","body":"更多指令和子句 指令 flush threadprivate 子句 if reduction copyin copyprivate 指令 flush flush指令主要用于处理内存一致性问题. 每个处理器(processor)都有自己的本地(local)存储单元:寄存器和缓存, 当一个线程更新了共享变量之后, 新的值会首先存储到寄存器中, 然后更新到本地缓存中. 这些更新并非立刻就可以被其他线程得知, 因此在其它处理器中运行的线程不能访问这些存储单元. 如果一个线程不知道这些更新而使用共享变量的旧值就行运算, 就可能会得到错误的结果. 通过使用flush指令, 可以保证线程读取到的共享变量的最新值. 下面是语法形式: #pragma omp flush[(list)] list指定需要flush的共享变量, 如果不指定list, 将flush作用于所有的共享变量. 在下面的几个位置已经隐式的添加了不指定list的flush指令. 所有隐式和显式的路障(barrier) Entry to and exit from critical regions Entry to and exit from lock routines threadprivate threadprivate作用于全局变量, 用来指定该全局变量被各个线程各自复制一份私有的拷贝, 即各个线程具有各自私有、线程范围内的全局对象, 语法形式如下: #pragma omp threadprivate(list) 其与private不同的时, threadprivate变量是存储在heap或者Thread local storage当中, 可以跨并行域访问, 而private绝大多数情况是存储在stack中, 只在当前并行域中访问, 下面是一个使用示例: int counter; #pragma omp threadprivate(counter) void test_threadprivate() { #pragma omp parallel { counter = omp_get_thread_num(); printf(\"1: thread %d : counter is %d\\n\", omp_get_thread_num(), counter); } printf(\"\\n\"); #pragma omp parallel { printf(\"2: thread %d : counter is %d\\n\", omp_get_thread_num(), counter); } } 下面是输出结果 1: thread 3 : counter is 3 1: thread 0 : counter is 0 1: thread 2 : counter is 2 1: thread 1 : counter is 1 2: thread 2 : counter is 2 2: thread 0 : counter is 0 2: thread 3 : counter is 3 2: thread 1 : counter is 1 从输出结果我们可以看到, 在第二个并行域中, counter保存了在第一个并行域中的值. 如果要使两个并行域之间可以共享threadprivate变量的值, 需要满足以下几个条件: 任意一个并行域都不能嵌套在其他并行域中(Neither parallel region is nested inside another explicit parallel region.) 执行两个并行域的线程数量要相同(The number of threads used to execute both parallel regions is the same.) 执行两个并行域时的线程亲和度策略要相同( The thread affinity policies used to execute both parallel regions are the same.) 在进入并行域之前dyn-var变量的值必须为false(0). (The value of the dyn-var internal control variable in the enclosing task region is false at entry to both parallel regions.) 子句 if 用来控制并行域是串行执行还是并行执行, 只能作用于paralle指令, 下面是其语法形式: #pragma omp parallel if(scalar-logical-expression) 如果if的判断条件为true, 则并行执行, 否则串行执行, 下面是一个使用示例 void test_if() { int n = 1, tid; printf(\"n = 1\\n\"); #pragma omp parallel if(n>5) default(none) \\ private(tid) shared(n) { tid = omp_get_thread_num(); printf(\"thread %d is running\\n\", tid); } printf(\"\\n\"); n = 10; printf(\"n = 10\\n\"); #pragma omp parallel if(n>5) default(none) \\ private(tid) shared(n) { tid = omp_get_thread_num(); printf(\"thread %d is running\\n\", tid); } } 输出结果如下 n = 1 thread 0 is running n = 10 thread 0 is running thread 2 is running thread 3 is running thread 1 is running reduction 如果利用循环, 将某项计算的所有结果进行求和(或者减、乘等其他操作)得出一个数值, 这在并行计算中十分常见, 通常将其称为规约. OpenMP提供了reduction子句由于规约操作, 其语法形式为 reduction(operator:list) 下面是一个使用实例: void test_reduction() { int sum, i; int n = 100; int a[n]; for(i = 0; i 使用规约子句之后, 无需再对sum进行保护, 下面是reduction支持的操作符以及变量的初值 在使用乘法时发现其初始值同样为0, 可能和具体的实现有关. copyin 将主线程中threadprivate变量的值复制到执行并行域的各个线程的threadprivate变量中, 作为各线程中threadprivate变量的初始值. 作用于parallel指令, 下面是一个使用示例: int counter = 10; #pragma omp threadprivate(counter) void test_copyin() { printf(\"counter is %d\\n\", counter); #pragma omp parallel copyin(counter) { counter = omp_get_thread_num() + counter + 1; printf(\" thread %d : counter is %d\\n\", omp_get_thread_num(), counter); } printf(\"counter is %d\\n\", counter); } 下面是输出结果: counter is 10 thread 0 : counter is 11 thread 2 : counter is 13 thread 3 : counter is 14 thread 1 : counter is 12 counter is 11 copyprivate 将一个线程私有变量的值广播到执行同一并行域的其他线程. 只能作用于single指令, 下面是一个使用示例: int counter = 10; #pragma omp threadprivate(counter) void test_copyprivate() { int i; #pragma omp parallel private(i) { #pragma omp single copyprivate(i, counter) { i = 50; counter = 100; printf(\"thread %d execute single\\n\", omp_get_thread_num()); } printf(\"thread %d: i is %d and counter is %d\\n\",omp_get_thread_num(), i, counter); } } 下面是程序运行结果: thread 3 execute single thread 2: i is 50 and counter is 100 thread 3: i is 50 and counter is 100 thread 0: i is 50 and counter is 100 thread 1: i is 50 and counter is 100 下面是将copyprivate(i, counter)去掉的运行结果 thread 0 execute single thread 2: i is 0 and counter is 10 thread 0: i is 50 and counter is 100 thread 3: i is 0 and counter is 10 thread 1: i is 32750 and counter is 10 "},"pthread.html":{"url":"pthread.html","title":"Pthreads","keywords":"","body":"Pthreads 学习笔记 与OpenMP相比，Pthreads的使用相对要复杂一些，需要我们显式的创建、管理、销毁线程，但也正因为如此，我们对于线程有更强的控制，可以更加灵活的使用线程。这里主要记录一下Pthreads的基本使用方法，如果不是十分复杂的使用环境，这些知识应该可以了。本文大部分内容都是参考自 pthread Tutorial，有兴趣的可以看一下原文。 "},"Pthreads-使用.html":{"url":"Pthreads-使用.html","title":"语法","keywords":"","body":"基本使用 HelloWorld 创建线程 Join 和 Detach Join(合并) Detach(分离) Mutex(互斥锁) 属性 Condition Variables(条件变量) Semaphores(信号量) Reader/Writer Locks 读写锁 参考文章 HelloWorld #include #include #include void * hello(void * args) { long rank = (long) args; printf(\"Hello form sub thread %ld\\n\", rank); return NULL; } int main() { int thread_num = 4; long thread_index; pthread_t * thread_handles; thread_handles =(pthread_t *) malloc(sizeof(pthread_t ) * thread_num); for(thread_index = 0; thread_index 编译程序, 需要加上 '-lpthread' gcc -o helloworld helloworld.c -lpthread 一种可能的输出结果 hello from main thread Hello form sub thread 2 Hello form sub thread 3 Hello form sub thread 1 Hello form sub thread 0 创建线程 Pthreads使用 pthread_create 函数来创建线程, 函数原型如下: int pthread_create( pthread_t * thread, const pthread_attr_t * attr, void * (*start_routine) (void *), void * arg ); 参数说明:thread　　指向执行线程标识符的指针, 通过该变量来控制线程attr　　设置线程属性, 如果为NULL, 则使用默认的属性start_routine　　线程运行函数的起始地址arg　　运行函数的参数, 这里使用 void*来作为参数类型, 以便可以向运行函数中传递任意类型的参数, 当然需要在运行函数中将参数转换为其原来的类型.返回值　　如果创建线程成功会返回0, 否则返回错误码. 下面是一个使用示例: void * thread_function(void *arg) { int * incoming = (int *)arg; printf(\"this is in pthread and arg is %d\\n\", *incoming); return NULL; } void hello_world() { pthread_t thread_id ; int value = 63; pthread_create(&thread_id, NULL, thread_function, &value); // 等待线程执行完 pthread_join(thread_id, NULL); } 在上面的代码中, 在程序最后加上了 pthread_join 函数, 用来完成线程间的同步, 即主线程等待指定的线程(在上面的代码中是 thread_id 对应的线程)执行完再往下执行. 在下面会详细介绍该函数. Join 和 Detach Join(合并) pthread_join 可以用于线程之间的同步, 当一个线程对另一个线程调用了join操作之后, 该线程会处于阻塞状态, 直到另外一个线程执行完毕. 下面是一个示意图: 下面是 pthread_join的函数原型: int pthread_join( pthread_t thread, void ** retval ); 参数说明:thread　　线程标识符, 用来指定等待哪个线程retaval　　用来存储等待线程的返回值 下面是通过获取函数返回值的一个示例: void * p_result(void * arg) { char * m = malloc(sizeof(char) * 3); m[0] = 'A'; m[1] = 'B'; m[2] = 'C'; return m; } void test_get_result() { pthread_t thread_id; void * exit_status ; pthread_create(&thread_id, NULL, p_result, NULL); pthread_join(thread_id, & exit_status); char * m = (char* ) exit_status; printf(\"m is %s\\n\", m); free(m); } 在 p_result 函数中为了使线程执行完, 我们还可以访问到变量 m 中的数据, m 的内存采用动态分配的方式, 如果静态分配, 即如 char m[3] 的形式, 那么在函数执行完就会清空 m 的值, 我们就无法获得想要的结果. 对于一个线程来说, 其终止方式有两种: 执行完线程函数或者自身调用 pthread_exit(void *), 如果线程通过执行完线程函数而终止的, 那么其他线程通过pthread_join获得的线程返回值就是线程函数的返回值(如上面的例子), 如果线程是通过 pthread_exit(void *) 方式结束的线程, 其线程返回值就是 pthread_exit 传入的参数, 下面是一个示例: void * p_exit_result(void * arg) { printf(\"print before pthread_exit\\n\"); pthread_exit((void *)10L); printf(\"print after pthread_exit\\n\"); return NULL; } void test_exit_result() { pthread_t thread_id; void * exit_status ; pthread_create(&thread_id, NULL, p_exit_result, NULL); pthread_join(thread_id, & exit_status); long m = (long ) exit_status; printf(\"m is %ld\\n\", m); } 下面是输出结果 print before pthread_exit m is 10 一般来说, 使用 Pthreads 创建的线程默认应该是可 join 的, 但是并不是所有实现都会这样, 所以必要情况下, 我们可以在创建线程时, 显式的指定线程是可 join 的 pthread_t thread_id; pthread_attr_t attr; pthread_attr_init(&attr); pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE); pthread_create(&thread_id, &attr, work, (void *)arg); pthread_attr_destroy(&attr); pthread_join(thread_id, NULL); Detach(分离) 对于可 join 的线程, 只有当其他线程对其调用了 pthread_join 之后, 该线程才会释放所占用的资源(例如线程所对应的标识符pthread_t, 线程的返回值信息), 如果想要系统回收线程的资源, 而不是通过调用pthread_join回收资源(会阻塞线程), 我们可以将线程设置为 DETACHED (分离的), 有三种方式将线程设为 detached的 创建线程时指定线程的 detach 属性: pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED); 通过在子线程中调用 pthread_detach(pthread_self()); 在主线程中调用 pthread_detach(thread_id);(非阻塞, 执行完会立即会返回), 通过上面的方式将线程设为 detached, 线程运行结束后会自动释放所有资源. Mutex(互斥锁) 互斥锁用来保护共享变量, 它可以保证某个时间内只有一个线程访问共享变量, 下面是使用互斥锁的具体步骤 声明 pthread_mutex_t (互斥锁类型) 类型的变量 调用 pthread_mutex_init() 来初始化变量 在访问共享变量之前, 调用 pthread_mutex_lock() 获得互斥锁, 如果互斥锁被其他线程占用, 该线程会处于等待状态 访问完共享变量之后, 调用 pthread_mutex_unlock() 释放互斥锁, 以便其他线程使用 程序执行完后调用 pthread_mutex_destroy()释放资源. 创建互斥锁有两种方式: 静态方式和动态方式. 静态方式是使用宏 PTHREAD_MUTEX_INITIALIZER 来初始化锁, 如下所示: pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER; 动态方式是调用 pthread_mutex_init 函数动态初始锁, 下面是该函数原型 int pthread_mutex_init(pthread_mutex_t *mutex, const pthread_mutexattr_t * attr) 下面是使用互斥锁的一个示例(使用动态方式): pthread_mutex_t lock; int share_data; void * p_lock(void * arg) { int i; for(i = 0; i 下是使用互斥量的几个注意点: 使用 lock 和 unlock 一个互斥锁时, 一定要先初始化该互斥锁 释放互斥锁的线程必须是获得互斥锁的那个线程 当 destroy 互斥锁的时候, 不该有线程还在使用这个互斥锁 属性 在动态创建互斥锁时, 我们可以传入一个锁属性变量 pthread_mutexattr_t 来初始化锁的属性, 通过下面两个函数来初始化和销毁该属性对象 int pthread_mutexattr_init(pthread_mutexattr_t *attr); int pthread_mutexattr_destroy(pthread_mutexattr_t *attr); 然后可以调用下面的方法对属性进行设置 范围 可以指定互斥锁是进程之间的同步还是进程内的同步, 下面是对应的两个锁的范围(scope) PTHREAD_PROCESS_SHARE: 进程间同步 PTHREAD_PROCESS_PRIVATE: 进程内同步, 默认值 通过调用下面的函数可以设置和获取锁的范围 int pthread_mutexattr_getpshared(const pthread_mutexattr_t * restrict attr, int *restrict pshared); int pthread_mutexattr_setpshared(pthread_mutexattr_t *attr, int pshared); 类型 互斥锁的类型有以下几种取值方式(为了兼容性, 一个类型可能有多个名称): PTHREAD_MUTEX_TIMED_NP / PTHREAD_MUTEX_NORMAL / PTHREAD_MUTEX_DEFAULT : 缺省值, 也就是普通锁. 当一个线程获得锁之后, 其余请求锁的线程将形成一个等待队列, 并在加锁线程解锁后按照优先级获得锁. 这种策略保证了资源分配的公正性. PTHREAD_MUTEX_RECURSIVE_NP / PTHREAD_MUTEX_RECURSIVE : 嵌套锁, 允许一个线程对同一个锁成功获得多次, 并通过多次 unlock 来解锁. 如果是不同线程请求, 则在加锁线程解锁后重新竞争. PTHREAD_MUTEX_ERRORCHECK_NP / PTHREAD_MUTEX_ERRORCHECK: 如果同一个线程请求同一个锁，则返回EDEADLK，否则与PTHREAD_MUTEX_TIMED_NP类型动作相同。这样就保证当不允许多次加锁时不会出现最简单情况下的死锁 PTHREAD_MUTEX_ADAPTIVE_NP: 适应锁，动作最简单的锁类型，仅等待解锁后重新竞争 可以使用下面的函数获取和设置锁的类型 int pthread_mutexattr_gettype(const pthread_mutexattr_t *attr, int *type); int pthread_mutexattr_settype(pthread_mutexattr_t *attr, int type); Condition Variables(条件变量) 条件变量对应的数据类型为 pthread_cond_t, 通过使用条件变量, 可以使线程在某个 特定条件 或者 事件 发生之前处于挂起状态. 当事件或者条件发生之后, 另一个线程可以通过信号来唤起挂起的线程. 条件变量主要使用下面几个函数 初始化(init) 和互斥锁一样, 条件变量也有两种初始化方式: 静态方式和动态方式 // 静态 pthread_cond_t cond = PTHREAD_COND_INITIALIZER; // 动态, 成功返回0 int pthread_cond_init(pthread_cond_t *restrict cond, const pthread_condattr_t *restrict attr); 销毁(destroy) int pthread_cond_destroy(pthread_cond_t *cond); 等待函数(wait) int pthread_cond_wait( pthread_cond_t * restrict cond, pthread_mutex_t * restrict mutex ); int pthread_cond_timedwait( pthread_cond_t * restrict cond, pthread_mutex_t * restrict mutex, const struct timespec * restrict abstime ); 通过调用 wait 函数, 线程会处于挂起状态. 其中 pthread_cond_timedwait 的含义为: 如果在 abstime 时间内(系统时间小于abstime), 线程没有被唤醒, 那么线程就会结束等待, 同时返回 ETIMEDOUT 错误. 唤醒函数(signal) int pthread_cond_broadcast(pthread_cond_t *cond); int pthread_cond_signal(pthread_cond_t *cond); singal 函数一次只能唤醒一个线程, 而 broadcast 会唤醒所有在当前条件变量下等待的线程. 下面是条件变量的具体使用, 首先一个线程会根据条件来确实是否需要处于挂起状态, 即如下面的形式 if(flag == 0){ pthread_cond_wait(...); } 如果flag不为0, 那么线程就不进入等待状态, 否则就挂起线程, 等待flag不为0(满足条件了, 可以往下执行)时被唤醒. 唤醒该线程的代码如下所示: flag = 1; pthread_cond_signal(...); 下面考虑一下这种情况, 首先 flag = 0, 当线程1执行到 if(flag == 0) 时, 发现不满足继续往下执行的条件, 即将进入挂起状态, 就在其刚要挂起的时候(还没挂起), 线程2执行了唤醒线程1的代码(修改flag的值, 唤醒线程1), 假设线程2执行完上述操作之后, 线程1仍然还没有挂起, 所以 pthread_cond_signal 并没有起到作用. 此后线程1终于进入了挂起状态, 等待线程2的唤醒, 而线程2则认为它已经唤醒了线程1, 让其往下执行了. 此时问题就来了, 如果线程2不再执行唤醒线程1的操作, 那么线程1就会永远处于挂起状态. 为了解决这种情况, 需要满足从判断 flag==0 到 pthread_cond_wait() 执行, flag 的值不能发生变化,并且不能提前执行唤醒操作. 为了实现这种需求, 我们需要加一个锁操作, 等待代码: pthread_mutex_lock(&mutex); if(flag == 0){ pthread_cond_wait(...); } pthread_mutex_unlock(&mutex); 唤醒代码 pthread_mutex_lock(&mutex); flag = 1; pthread_mutex_unlock(&mutex); pthread_cond_signal(&condition); 我们看到 pthread_cond_wait 的函数原型中第一个参数为条件变量, 第二个参数为互斥锁, 之所以需要传入互斥锁, 是因为如果不传入互斥锁, 当线程进入挂起状态时, 就无法释放掉该互斥锁, 而其他线程就无法获得该互斥锁,就没办法更新flag的值, 也无法唤醒线程1. 线程1就会永远处于挂起状态, 线程2就会永远处于请求互斥锁的状态. 所以当线程1进入挂起状态时需要释放掉互斥锁, 被唤醒之后再重新获得互斥锁, 即 pthread_cond_wait 可以看成下面的操作: pthread_mutex_unlock(&mutex); wait_on_signal(&condition); pthread_mutex_lock(&mutex); 所有一个条件变量总是和一个互斥锁关联. 下面再来看一下等待代码, 在某些特定情况下, 即使没有线程调用 pthread_cond_signal函数, 'pthread_cond_wait' 函数也有可能返回(具体解释可以看看 spurious wakeup), 但是此时条件并不满足, 如果程序往下执行, 那么就可能会出错. 所以为了避免这种情况, 即使线程被唤醒了, 也应该再检查一下条件是否满足, 即使用 while 循环代替 if 判断 pthread_mutex_lock(&mutex); while(flag == 0){ pthread_cond_wait(...); } pthread_mutex_unlock(&mutex); 下面是一个使用示例: pthread_cond_t is_zero; pthread_mutex_t mutex; int con_share_data = 32767; void * p_condition(void * arg) { while(con_share_data > 0) { pthread_mutex_lock(&mutex); con_share_data--; pthread_mutex_unlock(&mutex); } pthread_cond_signal(&is_zero); } void test_condition() { pthread_t thread_id; void *exit_status; int i; pthread_cond_init(&is_zero, NULL); pthread_mutex_init(&mutex, NULL); pthread_create(&thread_id, NULL, p_condition, NULL); pthread_mutex_lock(&mutex); while(con_share_data != 0) { pthread_cond_wait(& is_zero, &mutex); } pthread_mutex_unlock(&mutex); pthread_join(thread_id, &exit_status); pthread_mutex_destroy(&mutex); pthread_cond_destroy(&is_zero); } Semaphores(信号量) 信号量本质上可以看做是一个计数器, 它主要有两种操作, 第一类操作为 down 或者 wait -- sem_wait(...), 目的是为了减小计数器(将信号俩减1), 另一类为 up 或者 signal -- sem_post(...) , 目的是为了增大计数器(将信号量加1). 当线程调用 sem_wait() 时, 如果信号量的值大于0, 那么只会把信号量减1, 线程会继续往下执行. 如果信号量的值为0, 那么线程就会进入阻塞状态, 直到另外一个线程执行了 sem_post() 操作, 对信号量进行了增操作, 该线程才会继续往下执行. 信号量主要用于对一些稀缺资源的同步, 什么叫做稀缺资源, 就是说这个资源只有有限的几个, 但是又多于一个, 在某一个时刻, 可以供有限的几个线程使用, 但又不是全部线程使用. 如果将信号量初始化为1, 那么该信号量就等同于互斥锁了, 因此一次只能有一个线程获得信号量的资源, 如果其他线程想要获得, 必须等该线程对信号量进行增操作. 举个例子说: 有10个人去银行办理业务, 但是银行只有4个窗口(信号量初始化为4), 所以前4个人到了银行就可以办理业务, 但是第5个人之后就必须要等待, 等前面的某个人办理完业务(增加信号量), 空出窗口来. 而当第5个人去办理业务时, 空出的窗口又被占用了(减小信号量), 剩下的人还是要等待. 信号量在执行过程中和上述例子不同的一点是, 当有空余的资源出现时, 线程并不一定按照 FIFO(先进先出) 的顺序来获取资源, 而有可能是随机一个线程获得资源. 下面是信号量相关的函数 类型 信号量的类型是 sem_t, 需要引入头文件 #include 初始化和销毁 int sem_init(sem_t *sem, int pshared, unsigned int value); int sem_destroy(sem_t *sem); init 函数的第二个参数用来标识信号量的范围: 0 表示一个进程中线程间共享, 非0 表示进程间共享. 第三个参数就是信号量的可用数量. wait和signal int sem_wait(sem_t *sem); int sem_post(sem_t *sem); 下面是一个使用示例 int sem_share_data = 0; // use like a mutex sem_t binary_sem; void * p_sem(void * arg) { sem_wait(&binary_sem); // 减少信号量 // 在这里使用共享数据; sem_post(&binary_sem); // 增加信号量 } void test_sem() { sem_init(&binary_sem, 0, 1); // 信号量初始化为1, 当初互斥锁使用 // 在这里创建线程 sem_wait(&binary_sem); // 在这里使用共享变量 sem_post(&binary_sem); // 在这里join线程 sem_destroy(&binary_sem); } Reader/Writer Locks 读写锁 对于读写锁来说, 多个线程可以同时获得读锁, 但某一个时间内, 只有一个线程可以获得写锁. 如果已经有线程获得了读锁, 则任何请求写锁的线程将被阻塞在写锁函数的调用上, 同时如果线程已经获得了写锁, 那么任何请求读锁或者写锁 的线程都会被阻塞. 下面是读写锁的基本函数: 锁类型 pthread_rwlock_t 初始化/销毁 int pthread_rwlock_init(pthread_rwlock_t *restrict rwlock, const pthread_rwlockattr_t *restrict attr); int pthread_rwlock_destroy(pthread_rwlock_t *rwlock); 读锁 int pthread_rwlock_rdlock(pthread_rwlock_t *rwlock); 写锁 int pthread_rwlock_wrlock(pthread_rwlock_t *rwlock); 释放锁 int pthread_rwlock_unlock(pthread_rwlock_t *rwlock); 下面是一个使用示例: pthread_rwlock_t rw_lock; void * p_rwlock(void * arg) { pthread_rwlock_rdlock(&rw_lock); // 读取共享变量 pthread_rwlock_unlock(&rw_lock); } void test_rwlock() { pthread_rwlock_init(&rw_lock, NULL); // 创建线程 pthread_rwlock_wrlock(&rw_lock); // 修改共享变量 pthread_rwlock_unlock(&rw_lock); // join线程 pthread_rwlock_destroy(&rw_lock); } 参考文章 pthread Tutoriaed TutorialPOSIX Threads ProgrammingLinux线程-互斥锁pthread_mutex_tPthread：POSIX 多线程程序设计 下面列出一些学习资料，如果想深入学习Pthreads可以看下这些资料 ( 摘自POSIX 多线程程序设计)：Pthreads多线程编程指南Programing with POSIX threadPthread Primer "},"Pipline-模型.html":{"url":"Pipline-模型.html","title":"流水线模型","keywords":"","body":"流水线模型 前言 模型说明 实现 执行流程 线程等待和唤醒 数据缓冲区 完整代码 参考 前言 Pthreads 有几种工作模型，例如 Boss/Workder Model、Pileline Model(Assembly Line)、Background Task Model、Interface/Implementation Model，详细介绍可以参考 pthread Tutorial，这里给出一个流水线模型(Pipeline Model)的简单示例。在该示例中，主线程开启了两个子线程，一个子线程用来读取文件，一个子线程用于将结果写入文件，而主线程自身用来计算。 模型说明 很多时候，一个程序可以分为几个阶段，比如说读取数据、计算、将结果写入文件，当然我们可以使用每个线程依次执行这些操作，但是一个更好的选择是一个线程处理一个阶段，因为对于文件操作来说，硬盘的读写速率是一定的(IO很多时候会成为性能的瓶颈)，即使多个线程读取文件，其读写速率也不会变快(IO操作无法使用线程并行)。所以我们可以用一个线程来处理IO，另外的线程全部用于计算上，如果计算量较大，IO的耗时是可以掩盖过去的。比如读取一个 2G 的文件，然后进行计算。使用流水线模型，我们可以这样做，用一个线程专门读取文件，我们将其成为IO线程。IO线程一次读取 50M 数据，之后交给计算线程来处理这些数据，在计算线程处理数据的同时，IO线程再去读文件，假设处理 50M 数据的时间大于读取50M数据的时间， 当计算线程处理完上一份数据之后，要处理的下一份数据读取完毕，那么计算线程又可以紧接着处理这部分数据，这样循环操作，除了第一次读取数据的时候计算线程处于空闲状态，其余读取的时候计算线程都在进行计算，这样就掩盖掉了IO的时间 实现 执行流程 主线程在程序开始时创建两个子线程，一个用于读，一个用于写，读线程每次只读取一部分文件内容，写线程将这部分数据处理完之后的结果写入文件。创建完线程之后，主线程和写线程就处于等待状态，而读线程就开始读取文件，当读线程读取完第一部分数据之后，读线程进入阻塞状态，主线程开始计算，主线程计算完毕后，写线程开始写入计算结果，同时读线程开始下一部分数据的读取。按照这个流程循环取算存，直到程序结束。 线程等待和唤醒 在执行中，3个线程都会进行等待操作，并且处理完自己的任务之后，还要再次进入等待状态。这里使用条件变量来控制线程的挂起和唤醒，使用while循环控制线程的状态的多次切换。下面是示例代码 while(1) { pthread_mutex_lock(&read_lock); while(read_count == 0 ) { pthread_cond_wait(&read_cond, &read_lock); } read_count--; pthread_mutex_unlock(&read_lock); } 上面的代码中，while循环会一直执行，所以我们还要加一个是否可以跳出 while 循环的判断，以便在任务结束后可以终止线程, 如下面的代码： while(1) { pthread_mutex_lock(&read_lock); while(read_count == 0 && !read_shutdown ) { pthread_cond_wait(&read_cond, &read_lock); } if(read_shutdown) { break; } read_flag = 1 - read_flag; pthread_mutex_unlock(&read_lock); } 我们看到在判断线程是否挂起的 while 循环中也加入了!read_shutdown的判断，即如果马上就要跳出while循环，标明线程已经执行完了它的任务，则无需再进行挂起操作。唤醒该线程的代码如下所示： pthread_mutex_lock(&read_lock); if(loop_index == loop_nums - 1) { read_shutdown = 1; } read_count = 1; pthread_cond_signal(&read_cond); pthread_mutex_unlock(&read_lock); 下面分析一下条件变量，首先读线程和写线程都要对应一个条件变量，暂称为 read_cond 和 write_cond, 主线程用read_cond来告诉读线程自己已经开始计算，读线程可以继续读取下一部分数据了，用write_cond告诉写线程，计算已经完毕，可以将结果写入文件了 。而主线程需要两个条件变量，暂称为 cal_cond 和 cal_cond2 , 读线程使用 cal_cond 告诉主线程自己已经读完这部分数据了，主线程可以开始计算了。而写线程用 cal_cond2 告诉主线程自己已经写完了上次计算结果，可以再次分配写入的任务了。如果读线程没有读完或者写线程没有写完，主线程都要进入等待状态。 我们知道每个条件变量都会对应一个条件以及一个互斥锁，下面分析一下各个条件的初始值，程序开始时读线程开始工作，主线程要等待读线程读完才能进行计算，所以 read_cond 对应的条件为 true， cal_cond 对应的条件的为 false，写线程必须要等待主线程计算完才可以写，并且在第一次的时候写线程肯定是空闲的， 所以 write_cond 对应的条件为 false，cal_cond2 对应的的条件为 ture。 数据缓冲区 当读线程读完数据，将数据存到一个缓冲区中(比如一个数组)，主线程开始计算，此时读线程又去进行读取操作。如果读线程还是将数据读到上一次读取的缓冲区中（这个缓冲区此时正在被主线程使用），那么就会出现数据竞争。为了解决这个情况，我们可以使用两个缓冲区，读线程填满一个之后再去填另外一个，使用一个变量判断当前该使用哪个缓冲区，即如下面的形式： int read_buffer_a[BUFFER_SIZE], read_buffer_b[BUFFER_SIZE]; int read_flag; if(read_flag) { for(i = 0; i fp, \"%d\", read_buffer_a+i); } } else { for(i = 0; i fp, \"%d\", read_buffer_b + i); } } read_flag = 1 -read_flag; 完整代码 下面是完整的代码, 这里是github地址，可以下载下来运行一下。 #include #include #include #include #include #include #define BUFFER_SIZE 10 uint32_t microseconds = 100; // 线程信息 typedef struct _thread_info{ pthread_t thread_id; pthread_mutex_t lock; pthread_cond_t cond; int run_flag; int buffer_flag; int shutdown; } thread_info; // 线程函数参数 typedef struct _thread_arg { FILE *fp; } thread_arg; thread_info input_info, output_info, cal_input_info, cal_output_info; int read_buffer_a[BUFFER_SIZE], read_buffer_b[BUFFER_SIZE]; int write_buffer_a[BUFFER_SIZE], write_buffer_b[BUFFER_SIZE]; void init_resources(int n, ...) { va_list arg_ptr ; int i; va_start(arg_ptr, n); thread_info * tmp_info = NULL; for(i = 0; i lock), NULL); pthread_cond_init(&(tmp_info->cond), NULL); } va_end(arg_ptr); } void free_resources(int n, ...) { va_list arg_ptr; int i; va_start(arg_ptr, n); thread_info * tmp_info = NULL; for(i = 0; i lock)); pthread_cond_destroy(&(tmp_info->cond)); } va_end(arg_ptr); } void * input_task(void * args){ thread_arg * input_arg = (thread_arg *) args; int i; while(1) { pthread_mutex_lock(&(input_info.lock)); while(input_info.run_flag == 0 && !input_info.shutdown) { pthread_cond_wait(&(input_info.cond), &(input_info.lock)); } if(input_info.shutdown) { break; } input_info.run_flag = 0; input_info.buffer_flag = 1 - input_info.buffer_flag; pthread_mutex_unlock(&(input_info.lock)); if(input_info.buffer_flag) { for(i = 0; i fp, \"%d\", read_buffer_a + i); } } else { for(i = 0; i fp, \"%d\", read_buffer_b + i); } } pthread_mutex_lock(&(cal_input_info.lock)); cal_input_info.run_flag = 1; pthread_cond_signal(&(cal_input_info.cond)); pthread_mutex_unlock(&(cal_input_info.lock)); } return NULL; } void * output_task(void * args){ thread_arg * output_arg = (thread_arg *) args; int i; while(1) { pthread_mutex_lock(&(output_info.lock)); while(output_info.run_flag == 0 && !output_info.shutdown) { pthread_cond_wait(&(output_info.cond), &(output_info.lock)); } if(output_info.shutdown) { break; } output_info.run_flag = 0; output_info.buffer_flag = 1 - output_info.buffer_flag; pthread_mutex_unlock(&(output_info.lock)); if(output_info.buffer_flag) { for(i = 0; i fp, \"%d\\n\", write_buffer_a[i]); usleep(microseconds); } } else { for(i = 0; i fp, \"%d\\n\", write_buffer_b[i]); usleep(microseconds); } } pthread_mutex_lock(&(cal_output_info.lock)); cal_output_info.run_flag = 1; pthread_cond_signal(&(cal_output_info.cond)); pthread_mutex_unlock(&(cal_output_info.lock)); } return NULL; } int main(){ FILE *fp_input, *fp_output; char *input_name = \"input.txt\"; char *output_name = \"output.txt\"; int total_nums = 100; int loop_nums = total_nums / BUFFER_SIZE; int loop_index = 0; int i; thread_arg input_arg, output_arg; if((fp_input = fopen(input_name, \"r\")) == NULL) { printf(\"can't load input file\\n\"); exit(1); } if((fp_output = fopen(output_name, \"w+\")) == NULL) { printf(\"can't load output file\\n\"); exit(1); } input_arg.fp = fp_input; output_arg.fp = fp_output; init_resources(4, &input_info, &output_info, &cal_input_info, &cal_output_info); input_info.buffer_flag = output_info.buffer_flag = cal_input_info.buffer_flag = 0; input_info.run_flag = cal_output_info.run_flag = 1; output_info.run_flag = cal_input_info.run_flag = 0; input_info.shutdown = output_info.shutdown = 0; pthread_create(&(input_info.thread_id), NULL, input_task, &input_arg); pthread_create(&(output_info.thread_id), NULL, output_task, &output_arg); while(1) { pthread_mutex_lock(&(cal_input_info.lock)); while(cal_input_info.run_flag == 0) { pthread_cond_wait(&(cal_input_info.cond), &(cal_input_info.lock)); } cal_input_info.buffer_flag = 1 - cal_input_info.buffer_flag; cal_input_info.run_flag = 0; pthread_mutex_unlock(&(cal_input_info.lock)); pthread_mutex_lock(&(input_info.lock)); if(loop_index == loop_nums - 1) { input_info.shutdown = 1; } input_info.run_flag = 1; pthread_cond_signal(&(input_info.cond)); pthread_mutex_unlock(&(input_info.lock)); // 这里可以使用OpenMp if(cal_input_info.buffer_flag) { for(i = 0; i 参考 本文主要参考了这个Pthreads线程池 "},"mic.html":{"url":"mic.html","title":"MIC","keywords":"","body":"MIC 学习笔记 MIC 是英特尔至强融核系列的架构名称。现在已经出了两代产品 -- KNC 和 KNL，KNL 比 KNC 性能更好，支持的指令更多。目前关于MIC的中文书籍大概有两本: MIC 高性能计算编程指南 Intel Xeon Phi协处理器高性能编程指南 还有两本英文书，详细的介绍了基于多核和众核架构的并行编程以及在各个领域的应用 High Performance Parallelism Pearls Volume One High Performance Parallelism Pearls Volume Two 另外的学习资料大概就是Intel 编译器的指导手册: https://software.intel.com/en-us/compiler_15.0_ug_c 在Key Features有关于MIC的讲解,如果访问速度太慢, 可以访问下面的链接: http://scc.ustc.edu.cn/zlsc/tc4600/intel/2015.1.133/compiler_c/ "},"mic-helloworld.html":{"url":"mic-helloworld.html","title":"HelloWorld","keywords":"","body":"MIC HelloWorld 什么是MIC 运行模式 HelloWorld offload(分载) 非共享内存模式 共享虚拟内存模式 什么是MIC 以下摘自\"MIC高性能编程指南\" 通常提及MIC系列, 会提及以下几个名词: MIC(Many Integrated Core), Knights系列(如Knights Corner. KNC), Intel® Xeon PhiTM(官方中文译名:英特尔® 至强融核TM). MIC作为这个系列的架构名称, 类似于CPU, 是对采用这种架构的产品的总称. Knights 系列, 是Intel公司推出的MIC产品的研发代号, 类似于Ivy Bridge, 是内部研发人员对某一代产品的命名,不用于商业用途, 例如第一代正式产品锁采用的,就是Knights Corner架构. 提到具体KNx的架构, 与MIC架构相比, 可以看做是面向对象中父类与子类的关系, MIC架构是父类, 而KNx则是子类. Intel® Xeon PhiTM则是产品线的总称, 类似于Pentium、 Xeon等产品系列, Intel® Xeon PhiTM 是Intel公司推出的基于MIC架构的高性能计算协处理器卡的系列产品名称. 运行模式 MIC卡本身自带了一个简化的linux系统, 因此在安装了MIC卡的系统中, MIC既可以和CPU协同工作(使用offload), 也可以独立工作(native模式), 我们这里主要使用的是MIC和CPU协同工作的模式. HelloWorld 为了能够直观的看出我们的程序是在MIC端运行的, 首先介绍一个宏__MIC__, 这个宏只有在MIC上运行时才有效, 在CPU端运行是没有该宏的定义的. 下面是 Hello World代码: #include __attribute__ (( target (mic))) void say_hello() { //如果有__MIC__的宏定义, 证明是在MIC端运行的 #ifdef __MIC__ printf(\"Hello from MIC\\n\"); #else printf(\"Hello from CPU\\n\"); #endif } int main() { #pragma offload target(mic) say_hello(); } 使用下面的命令进行编译 icc -o helloworld helloworld.c 然后执行helloworld会打印Hello from MIC, 如果将'#pragma offload target(mic)' 注释掉, 就会打印出Hello from CPU. offload(分载) offload(分载)大概就是说程序在cpu上运行时, 会将一部分的工作交给mic去做, mic做完之后将结果再传递回来.下面是高性能编程指南中中关于分载的定义: 分载是指设计的程序运行在处理器上, 同时将部分工作负载分载到一个或多个协处理器上. 因为主处理器和协处理器之间不能共享常规的系统内存, 所以需要大量的分载控制与功能, 因此导致数据在主处理器和协处理器之间需要往复传递. 分载分为两种模式:非共享内存模式和共享虚拟内存模式. 非共享内存模式 非共享内存模式使用#pramga预编译指令, 使用方式为#pragma offload target(mic) , 上面的HelloWorld就使用了这种模式. 在这种模式下将cpu和mic的内存看作两块独立的内存(实际上也是这样), 数据在这两块内存之间根据需求相互传输. 我们可以指定将哪些数据传输到mic上, 以及将哪些数据传回cpu. 这种模式适合处理扁平的数据结构(flat structure-scalars, arrays, and structs that can be copied from one variable to another using a simple memcpy). 该模式的性能高于共享内存模式. 共享虚拟内存模式 共享虚拟内存(shared Virtual Memory) 模式默认集成到Intel Cilk Plus中, 在C/C++编程中使用_Cilk_shared和_Cilk_offload关键字. 共享虚拟内存不支持Fortran语言.在这种模式下, 变量通过_Cilk_shared 关键字在CPU和MIC之间共享, 所共享的动态内存必须通过特定的函数分配:_Offload_shared_malloc, _Offload_shared_aligned_malloc, _Offload_shared_free, _Offload_shared_aligned_free. 此模式适用于处理复杂的数据结构,比如链表, 树等. 该模式性能相对较差, 但是为编程提供了方便. "},"offload.html":{"url":"offload.html","title":"offload","keywords":"","body":"Offload 模式 这种方式对应于我们前面所说的非共享内存模型，这里记录一下它的基本用法 定义MIC使用的函数和变量 数据传输 定义MIC使用的函数和变量 如果是局部变量, 那么我们不需要做额外的工作, 如果全局变量或者函数, 要在mic上使用它们, 则需要使用下面的方式声明或者定义: __declspec( target (mic)) function-declaration __declspec( target (mic)) variable-declaration __attribute__ (( target (mic))) function-declaration __attribute__ (( target (mic))) variable-declaration 其中__declspec可以用于windows或者linux系统, 而_attribute__只能用于linux. 下面使用示例: #include #define __ONMIC__ __attribute__((target(mic))) __ONMIC__ int i; __ONMIC__ void f(int n) { printf(\"n*n is %d\\n\", n*n); } int main() { #pragma offload target(mic) { i = 100; f(i); } printf(\"i is %d\\n\", i); } 数据传输 虽然在host(主机端, 例如CPU)和targets(设备端, 例如MIC卡)端使用的指令集是相似的, 但是它们并不共享同一个系统内存, 这也就意味着在#pragma代码块中用到的变量必须同时存在 于host和target上, 为了确保这样, pragma使用特定的说明符(Specifiers)[in, out, inout]来指定在host和target之间复制的变量. in: 指定一个变量从host端复制到target端(作为target的输入), 但是不从target端复制回host端 out: 指定一个变量从target端复制回host端(作为target的输出), 但是不从host段复制到target端 inout: 指定一个变量即从host端复制到target端, 也从target段复制回host端(即是输入又是输出). 在没有显示的调用说明符, 那么默认inout. 下面是一个示例 #include int main() { int inVar = 10; int outVar = 20; int inoutVar = 30; #pragma offload target(mic) in(inVar) out(outVar) { printf(\"inVar in MIC is %d\\n\", inVar); printf(\"outVar in MIC is %d\\n\", outVar); // 这里用到了inoutVar, 但是offload时并没有指定它的说明符, 则用默认的inout printf(\"inoutVar in MIC is %d\\n\", inoutVar); inVar = 100; outVar = 200; inoutVar = 300; } printf(\"inVar in CPU is %d\\n\", inVar); printf(\"outVar in CPU is %d\\n\", outVar); printf(\"inoutVar in CPU is %d\\n\", inoutVar); } 输出结果为: inVar in CPU is 10 outVar in CPU is 200 inoutVar in CPU is 300 inVar in MIC is 10 outVar in MIC is 0 inoutVar in MIC is 30 从上面可以看出inVar的值传到了MIC上, 但是在MIC上修改后并没有传回CPU, CPU中outVar的没有传递到MIC上, 但是MIC上outVar的值却是传回到了CPU上,而inoutVar的值即传递到了MIC,也从MIC上传了回来. 同时我们还可以看到, 先打印的是in CPU, 又打印的in MIC, 这是因为在target端(比如MIC卡)输出时, 因为PCI-E设备(MIC卡是插在PCI-E插槽上的) 无法直接访问显示器, 所以必须经过CPU中转. 虽然各家厂商实现方式不尽相同, 但总免不了使用卡上的内存进行缓冲, 之后交换到host端内存中, 再进行输出, 这样就会有一定的延迟, 因此一般target上的输出要慢于host端的输出. "},"offload-详解.html":{"url":"offload-详解.html","title":"in / out / inout 详细用法","keywords":"","body":"in / out / inout 详细用法 下面的代码主要使用in作为测试, out和inout的用法应该是类似的， 下面主要以代码为主， 并且附带执行结果。 静态一维数组 静态二维数组 一个小问题 一维动态数组 使用指针实现的二维数组 包含指针的struct 注意事项 静态一维数组 #include #include void offload_one_dim_array(int n) { int arr[n]; int arr2[n]; int arr3[n]; int i; for(i = 0; i 输出结果为: arr[0] is 0 arr[1] is 1 arr[2] is 2 arr[3] is 3 arr[4] is 4 arr[5] is 5 arr[6] is 6 arr[7] is 7 arr[8] is 8 arr[9] is 9 ========================== arr2[0] is 10 arr2[1] is 11 arr2[2] is 12 arr2[3] is 13 arr2[4] is 14 arr2[5] is 0 arr2[6] is 0 arr2[7] is 0 arr2[8] is 0 arr2[9] is 0 ========================== arr3[0] is 0 arr3[1] is 0 arr3[2] is 22 arr3[3] is 23 arr3[4] is 24 arr3[5] is 25 arr3[6] is 26 arr3[7] is 0 arr3[8] is 0 arr3[9] is 0 静态二维数组 #include #include void offload_two_dim_array(int n) { int arr[n][n]; int arr2[n][n]; int arr3[n][n]; int arr4[n][n]; int i, j, index = 0; for(i = 0; i 下面是输出结果 arr[0][0] is 0 arr[0][1] is 1 arr[0][2] is 2 arr[1][0] is 3 arr[1][1] is 4 arr[1][2] is 5 arr[2][0] is 6 arr[2][1] is 7 arr[2][2] is 8 ========================== arr2[0][0] is 9 arr2[0][1] is 10 arr2[0][2] is 11 arr2[1][0] is 12 arr2[1][1] is 13 arr2[1][2] is 0 arr2[2][0] is 0 arr2[2][1] is 0 arr2[2][2] is 0 ========================== arr3[0][0] is 18 arr3[0][1] is 19 arr3[0][2] is 20 arr3[1][0] is 21 arr3[1][1] is 22 arr3[1][2] is 23 arr3[2][0] is 0 arr3[2][1] is 0 arr3[2][2] is 0 ========================== arr4[0][0] is 27 arr4[0][1] is 28 arr4[0][2] is 0 arr4[1][0] is 30 arr4[1][1] is 31 arr4[1][2] is 0 arr4[2][0] is 0 arr4[2][1] is 0 arr4[2][2] is 0 一个小问题 当数组(非指针)被offload一次之后会在mic上保存,并没有立即释放,在同一个作用域下,再次offload时, 如果值改变会更改为新值,如果没有offload某些位置的值,这些位置会使用上一次的旧值下面是局部变量测试 #include #include void offload_array_test(int n) { int arr[n]; int i; for(i = 0; i 输出结果为: arr[0] without offload is 0 arr[1] without offload is 1000 arr[2] without offload is 2 arr[3] without offload is 3 arr[4] without offload is 4 arr[5] without offload is 5 arr[6] without offload is 6 arr[7] without offload is 7 arr[8] without offload is 2000 arr[9] without offload is 9 ========================== arr[0] in first offload is 0 arr[1] in first offload is 1 arr[2] in first offload is 2 arr[3] in first offload is 3 arr[4] in first offload is 4 arr[5] in first offload is 5 arr[6] in first offload is 6 arr[7] in first offload is 7 arr[8] in first offload is 8 arr[9] in first offload is 9 ========================== arr[0] in second offload is 0 arr[1] in second offload is 1000 arr[2] in second offload is 2 arr[3] in second offload is 3 arr[4] in second offload is 4 arr[5] in second offload is 5 arr[6] in second offload is 6 arr[7] in second offload is 7 arr[8] in second offload is 8 arr[9] in second offload is 1111 下面是全局变量测试: #include #include #define __ONMIC__ __attribute__((target(mic))) __ONMIC__ int gArr[10]; void test1() { int i; for(i = 0; i 下面是测试结果: gArr[0] in test1 is 0 gArr[1] in test1 is 1 gArr[2] in test1 is 2 gArr[3] in test1 is 3 gArr[4] in test1 is 4 gArr[5] in test1 is 5 gArr[6] in test1 is 6 gArr[7] in test1 is 7 gArr[8] in test1 is 8 gArr[9] in test1 is 9 ========================== gArr[0] in test2 is 10 gArr[1] in test2 is 1 gArr[2] in test2 is 2 gArr[3] in test2 is 3 gArr[4] in test2 is 4 gArr[5] in test2 is 5 gArr[6] in test2 is 6 gArr[7] in test2 is 7 gArr[8] in test2 is 8 gArr[9] in test2 is 9 一维动态数组 #include #include void offload_point() { int n = 10; int *arr =(int*) calloc(n, sizeof(int)); int *arr2 = (int*) calloc(n, sizeof(int)); int *arr3 = (int*) calloc(n, sizeof(int)); int i; for(i = 0; i 程序输出如下: arr[0] is 0 arr[1] is 1 arr[2] is 2 arr[3] is 3 arr[4] is 4 arr[5] is 5 arr[6] is 6 arr[7] is 7 arr[8] is 8 arr[9] is 9 ========================== arr2[0] is 0 arr2[1] is 0 arr2[2] is 12 arr2[3] is 13 arr2[4] is 14 arr2[5] is 0 arr2[6] is 0 arr2[7] is 0 arr2[8] is 0 arr2[9] is 0 ========================== arr3[0] is 20 arr3[1] is 21 arr3[2] is 22 arr3[3] is 0 arr3[4] is 0 arr3[5] is 0 arr3[6] is 0 arr3[7] is 0 arr3[8] is 0 arr3[9] is 0 使用指针实现的二维数组 首先用typedef定义一个一维静态数组的类型, 然后为该类型声明一个动态数组 #include #include typedef int ARRAY[5]; //下面相当于上传了一个二维数组 void offload_point2() { int n = 3; ARRAY *arr = (ARRAY*)calloc(n, sizeof(ARRAY)); ARRAY *arr2 = (ARRAY*)calloc(n, sizeof(ARRAY)); int i, j, index = 0; for(i = 0; i 输出结果为: arr[0][0] is 0 arr[0][1] is 1 arr[0][2] is 2 arr[0][3] is 3 arr[0][4] is 4 arr[1][0] is 5 arr[1][1] is 6 arr[1][2] is 7 arr[1][3] is 8 arr[1][4] is 9 arr[2][0] is 10 arr[2][1] is 11 arr[2][2] is 12 arr[2][3] is 13 arr[2][4] is 14 ========================== arr2[0][0] is 9 arr2[0][1] is 10 arr2[0][2] is 0 arr2[0][3] is 0 arr2[0][4] is 0 arr2[1][0] is 14 arr2[1][1] is 15 arr2[1][2] is 0 arr2[1][3] is 0 arr2[1][4] is 0 arr2[2][0] is 0 arr2[2][1] is 0 arr2[2][2] is 0 arr2[2][3] is 0 arr2[2][4] is 0 包含指针的struct #include #include struct my_struct { int y; int *a; }; void offload_struct() { struct my_struct m; m.y = 10; m.a =(int*) calloc(10, sizeof(int)); int i; for(i=0; i 注意事项 使用offload不能上传指针数组, 即一个数组中的每个元素是一个指针, 或者元素中包含一个指针， 比如下面的形式 int **p struct mystruct { int *i; }; struct mystruct *m; "},"offload-other.html":{"url":"offload-other.html","title":"offload 其他函数","keywords":"","body":"Offload 其他函数 into alloc_if 和 free_if Applying the target Attribute to Multiple Declarations into 使用into可以将一个变量的值上传到另外一个变量中, 比如in (a into(b)), 表示将CPU上变量a的值赋给MIC上的变量b, 也可以out(b into(c)) 将MIC上变量b的值传回给CPU上的变量c. 需要注意的地方是into 只能用于in或者out中, 不能用于inout或者nocopy中. 下面是使用示例: #include void init_array(int* arr, int n, int start_num){ int i; for(i = 0; i length(p2)时, in的时候需要注意p的长度不可大于p2的长度 #pragma offload target(mic) in(p[0:n]:into(p1[0:n+1])) in(p[0:n-1]:into(p2[0:n-1])) out(p1) out(p2) { for(i = 0; i alloc_if 和 free_if 对于指针变量来说, 每次执行offload都会为其分配新的内存, 当offload执行完之后, 就会将该内存释放掉. 为了能够重用前面offload所开辟的空间, mic提供了alloc_if和free_if来显示指定是否为offload的指针变量(非指针变量使用alloc_if和free_if会报错)分配新的内存以及执行完offload后是否释放该内存. 下面是具体含义: alloc_if(1) - offload时为指针分配新的内存 alloc_if(0) - offload时不开辟新的内存, 而是使用前面保留的内存 free_if(1) - offload执行完成后, 释放掉为该指针分配的内存 free_if(0) - offload执行完成后, 不释放指针对应的内存 默认值是alloc_if(1) 和 free_if(1), 为了使程序更加清晰, 我们预定义几个宏 #define ALLOC alloc_if(1) #define FREE free_if(1) #define RETAIN free_if(0) #define REUSE alloc_if(0) 下面是具体的示例代码: #include #include #define ALLOC alloc_if(1) #define FREE free_if(1) #define RETAIN free_if(0) #define REUSE alloc_if(0) void init_array(int* arr, int n, int start_num){ int i; for(i = 0; i 还有一个问题就是重用内存的时候好像是不需要两个变量名相同, 看下面的代码 void retain() { int n = 10; int *p =(int*) calloc(n, sizeof(int)); int i; init_array(p, n, 0); #pragma offload target(mic) in(p:length(n) RETAIN) { for(i = 0; i 首先执行retain, 然后在执行reuse, 程序仍然可以正常运行. Applying the target Attribute to Multiple Declarations 当有多个变量或者函数需要在MIC上使用时, 我们可以采用一种较为方便的声明方式为这些变量和函数加上 target(mic) 的属性, 下面是声明方式: #pragma offload_attribute(push, target(mic)) ... #pragma offload_attribute(pop) 在两个#pragma之间声明的变量和函数都可以在mic上运行, 如果要声明共享虚拟内存模式下使用的共享变量和函数, 可以采用下面的形式 #pragma offload_attribute(push, _Cilk_shared) ... #pragma offload_attribute(pop) 下面是一个示例: #pragma offload_attribute(push, target(mic)) #include #include void test1(); void test2(); #pragma offload_attribute(pop) int main() { #pragma offload target(mic) test1(); #pragma offload target(mic) test2(); } void test1() { printf(\"this is test1\\n\"); } void test2() { printf(\"this is test2\\n\"); } "},"mic-共享虚拟内存模式.html":{"url":"mic-共享虚拟内存模式.html","title":"共享虚拟内存模式","keywords":"","body":"共享虚拟内存 前言 声明共享变量和函数 指针内存管理 二维指针示例 前言 使用 #pragma offload target(mic) 方式将程序分载到MIC上计算是比较常用的方式, 但是这种方式只支持一维指针, 如果有较为复杂的数据结构, 比如二维指针, 树, 链表等结构则需要将这些数据结构转换为一维结构(如果可以), 否则不能将数据传到MIC上去. 为了满足复杂的数据结构, mic提供了共享虚拟内存的方式, 即将mic的内存和cpu的内存看做共享同一块虚拟内存, 在共享内存中的数据被cpu和mic共享, 不需要使用offload将数据在cpu和mic之间相互传递. 声明共享变量和函数 我们可以使用_Cilk_shared来声明mic和cpu共享的变量和函数, 使用_Cilk_offload在mic端运行共享函数. _Cilk_shared int i; _Cilk_shared void a(); 共享变量的虚拟内存地址在cpu和mic上是相同的, 并且它们的值会在cpu和mic之间同步. 下面是一个示例: #include #include // 声明CPU和MIC共享的变量 _Cilk_shared int x = 1; // 声明CPU和MIC共享的函数 _Cilk_shared void run_onmic() { x = 3; printf(\"mic: the value of x is %d and the address of mic is %p\\n\", x, &x); // 确认是否在mic上执行 #ifdef __MIC__ printf(\"this is onmic\\n\"); #endif } void run_oncpu() { printf(\"cpu: the value of x is %d and the address of mic is %p\\n\", x, &x); } int main() { // 使用_Cilk_offload 代替#pragma offload target(mic) _Cilk_offload run_onmic(); run_oncpu(); } 指针内存管理 首先说下共享指针的声明方式: int *_Cilk_shared share_pointer; 上面是声明一个共享指针, 注意*号在_Cilk_shared的前面, 下面的两种方式都不是共享指针正确的声明方式 int _Cilk_shared *share_pointer; _Cilk_shared int *share_pointer; 共享内配的分配和释放应该使用下面的函数 void *_Offload_shared_malloc(size_t size); void *_Offload_shared_aligned_malloc(size_t size, size_t alignment); _Offload_shared_free(void *p); _Offload_shared_aligned_free(void *p); 其中_Offload_shared_aligned_malloc 和 _Offload_shared_aligned_free 用于处理需要内存对齐时的情况. 不过好像在共享函数中可以使用malloc为共享变量分配内存, 但是不清楚是否会有什么副作用. 还要注意的一点是_Offload_shared_malloc和free , malloc和_Offload_shared_free 不能混用, 否则可能出现意想不到的结果, 下面是一个示例代码: #include #include // int _Cilk_shared *p 是本地指针, 可以指向共享数据, 如果直接p = _Offload_share_malloc() 会报warning // 而使用下面的方式定义则没有问题 // typedef int *fp; // _Cilk_shared fp p; // p = (fp)_Offload_shared_malloc(sizeof(int) * n); // _Offload_shared_free(p); int *_Cilk_shared share_pointer; _Cilk_shared int n = 8; // 在共享函数内, 使用malloc和free为共享变量分配和释放内存 _Cilk_shared void cilk_malloc() { int i; share_pointer = (int *)malloc(sizeof(int) * n); for(i = 0; i 当在mic上执行cilk_share_free时会报错误, 原因是只能在cpu端调用_Offload_shared_free函数释放内存. 二维指针示例 下面是一个使用共享二维指针的一个示例 #include int **_Cilk_shared p; _Cilk_shared int n = 3, m = 3; void init_p() { int index = 0, i, j; for(i = 0; i "},"mic-异步计算和传输.html":{"url":"mic-异步计算和传输.html","title":"异步计算和传输","keywords":"","body":"异步计算和传输 异步计算 异步传输 异步计算 当使用#pragma offload target(mic) 方式分载时, cpu会等待offload的代码块执行完再继续往下执行, 如果不希望等待offload, 我们可以使用cpu和mic异步计算的方式. 具体方法为在offload的时候添加一个信号量, 如下面的形式: char signal_var; #pragma offload target(mic:0)signal(&signal_var) { ... } 此时offload 的代码就会异步执行, 需要注意的一点是要制定mic的编号(如上面的target(mic:0)), 如果需要等待offload执行完后在往下执行, 可以使用offload_wait, 如下面的形式 #pragma offload_wait target(mic:0) wait(&signal_var) 当代码执行到这一句时如果offload没有执行完就会处于等待状态, 直到offload执行完再往下执行. 下面是一个完整的示例, test1是异步执行, test2是同步执行. #include #include void test1() { char signal_var; //需要指定mic卡的编号 #pragma offload target(mic:0)signal(&signal_var) { long long i; long long t; for(i = 0; i 异步传输 如果数据量很大, 那么cpu和mic之间的数据传输也要花费一些时间, 如果不希望等待数据传输, 那么可以使用offload_transfer进行异步数据传输, 如下面的方式 #pragma offload_transfer target(mic:0) signal(f1) \\ in (f1:length(n) alloc_if(1) free_if(0)) 如果后面的offload需要使用本次offload上传的数据, 那么可以使用wait来等待数据传输完毕再执行 #pragma offload target(mic:0) wait(f1) 下面是一个完整的示例: #include #include #define __ONMIC__ __attribute__((target(mic))) __ONMIC__ void add_inputs(int n, float *f1, float *f2){ int i; for( i =0; i "},"mic-向量化.html":{"url":"mic-向量化.html","title":"向量化","keywords":"","body":"向量化 前言 数据类型 向量化函数(Intrinsics) 算术运算 With Mask Bitwise运算 移位操作 _mm512_alignr_epi32 前言 向量化简单的说就是使用SIMD指令, 来实现使用一条指令同时处理多个数据, MIC中具有32个长度为512位的向量处理单元, 每个向量处理单元可以处理16个32位或者8个64位的数据. 这里主要记录一下MIC向量化的使用方式以及一些向量指令的作用. 数据类型 MIC中使用下面的数据类型作为执行向量函数的操作数 __m512, __m512i __m512d 下面是它们的各自的作用: __m512 - 处理单精度向量(float32 vector) __m512d - 处理双精度向量(float64 vector) __m512i - 处理整形向量, 包括32位和64位整形(int32/int64) 上面的数据类型直接映射到向量寄存器上(vector registers), 除此之外还有一种数据类型__mmask16 - is an unsigned short type associated with the mask register values. 我们可以使用 Load Intrinsics(为向量赋值) 和 Store Intrinsics (保存向量的值) 实现向量的存取. 下面是一个示例 void test_load_store() { // 使用int32_t和int64_t 需要引入stdint.h int32_t *arr_int32; int64_t *arr_int64; int i, n = 32; // 需要使用_mm_malloc分配内存, 并且以64位对齐, 否则可能出现错误 arr_int32 = _mm_malloc(sizeof(int32_t) * n, 64); arr_int64 = _mm_malloc(sizeof(int64_t) * n, 64); for(i = 0; i 向量化函数(Intrinsics) 这里主要记录一些编译器提供的向量化函数, 完整的函数集可以在这里或者这里查询 算术运算 MIC中提供了加,减, 乘 三种算术运算函数, 这里以32位整型的加法为例: #include #include #include #include void mic_add() { uint32_t *arr_a, *arr_b, *arr_c; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_b = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 输出结果为: arr_a[ 0] is: 0 arr_b[ 0] is: 16 arr_c[ 0] is : 16 arr_a[ 1] is: 1 arr_b[ 1] is: 17 arr_c[ 1] is : 18 arr_a[ 2] is: 2 arr_b[ 2] is: 18 arr_c[ 2] is : 20 arr_a[ 3] is: 3 arr_b[ 3] is: 19 arr_c[ 3] is : 22 arr_a[ 4] is: 4 arr_b[ 4] is: 20 arr_c[ 4] is : 24 arr_a[ 5] is: 5 arr_b[ 5] is: 21 arr_c[ 5] is : 26 arr_a[ 6] is: 6 arr_b[ 6] is: 22 arr_c[ 6] is : 28 arr_a[ 7] is: 7 arr_b[ 7] is: 23 arr_c[ 7] is : 30 arr_a[ 8] is: 8 arr_b[ 8] is: 24 arr_c[ 8] is : 32 arr_a[ 9] is: 9 arr_b[ 9] is: 25 arr_c[ 9] is : 34 arr_a[10] is: 10 arr_b[10] is: 26 arr_c[10] is : 36 arr_a[11] is: 11 arr_b[11] is: 27 arr_c[11] is : 38 arr_a[12] is: 12 arr_b[12] is: 28 arr_c[12] is : 40 arr_a[13] is: 13 arr_b[13] is: 29 arr_c[13] is : 42 arr_a[14] is: 14 arr_b[14] is: 30 arr_c[14] is : 44 arr_a[15] is: 15 arr_b[15] is: 31 arr_c[15] is : 46 With Mask MIC提供的向量函数一般有两种形式 // Without Mask extern _m512i __cdecl _mm512_add_epi32(_m512i v2, _m512i v3); // With Mask extern _m512i __cdecl _mm512_mask_add_epi32(_m512i v1_old, __mmask16 k1, _m512i v2, _m512i v3); 一种是带Mask的, 一种是不带Mask的. 带Mask的多了两个参数: v1_old和k1, 其中k1是__mmask16类型的数据, 在上面我们知道__mmask类型就是unsigned short类型, 长度为16位. 关于带mask函数的解释: 将v1的16位分别对应到_m512i的16个整型上, 如果k1某个位是1, 则将v2和v3中与该位对应的整型相加, 作为结果值, 如果k1某个位为0, 就使用v1_old向量中对应位的整型作为结果值. 例如如果k1的第一位为1, 那么就将v2的第一个整数和v3的第一个整数相加, 作为结果向量的第一个整型的值. 如果k1的第一位是0, 就将v1_old向量中的第一个整型的值作为结果向量中第一个整型的值. 好吧, 还是看个例子吧. void mic_mask_add() { uint32_t *arr_a, *arr_b, *arr_c, *arr_old; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_b = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); arr_old = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 运行结果为: arr_a[ 0] is: 0 arr_b[ 0] is: 16 arr_old[ 0] is: 10000 arr_c[ 0] is : 16 arr_a[ 1] is: 1 arr_b[ 1] is: 17 arr_old[ 1] is: 10000 arr_c[ 1] is : 18 arr_a[ 2] is: 2 arr_b[ 2] is: 18 arr_old[ 2] is: 10000 arr_c[ 2] is : 10000 arr_a[ 3] is: 3 arr_b[ 3] is: 19 arr_old[ 3] is: 10000 arr_c[ 3] is : 22 arr_a[ 4] is: 4 arr_b[ 4] is: 20 arr_old[ 4] is: 10000 arr_c[ 4] is : 10000 arr_a[ 5] is: 5 arr_b[ 5] is: 21 arr_old[ 5] is: 10000 arr_c[ 5] is : 10000 arr_a[ 6] is: 6 arr_b[ 6] is: 22 arr_old[ 6] is: 10000 arr_c[ 6] is : 10000 arr_a[ 7] is: 7 arr_b[ 7] is: 23 arr_old[ 7] is: 10000 arr_c[ 7] is : 10000 arr_a[ 8] is: 8 arr_b[ 8] is: 24 arr_old[ 8] is: 10000 arr_c[ 8] is : 10000 arr_a[ 9] is: 9 arr_b[ 9] is: 25 arr_old[ 9] is: 10000 arr_c[ 9] is : 10000 arr_a[10] is: 10 arr_b[10] is: 26 arr_old[10] is: 10000 arr_c[10] is : 10000 arr_a[11] is: 11 arr_b[11] is: 27 arr_old[11] is: 10000 arr_c[11] is : 10000 arr_a[12] is: 12 arr_b[12] is: 28 arr_old[12] is: 10000 arr_c[12] is : 10000 arr_a[13] is: 13 arr_b[13] is: 29 arr_old[13] is: 10000 arr_c[13] is : 10000 arr_a[14] is: 14 arr_b[14] is: 30 arr_old[14] is: 10000 arr_c[14] is : 10000 arr_a[15] is: 15 arr_b[15] is: 31 arr_old[15] is: 10000 arr_c[15] is : 10000 Bitwise运算 MIC中提供了3中Bitwise运算函数- and or xor, 其中取反元素可以通过与1异或来实现, 下面是and操作的一个例子 void mic_and() { uint32_t *arr_a, *arr_b, *arr_c; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_b = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 其中print_binary是一个打印二进制的函数, 这里只打印了后8位 // 打印二进制 void print_binary(uint64_t t, int bit_len) { short buffer[bit_len]; int i; for(i = 0; i = 0; i--) { printf(\"%hd\", buffer[i]); } } 下面是一个取反的示例 void mic_not() { uint32_t *arr_a, *arr_c; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 下面是运行结果 ~ 00000000 = 11111111 ~ 00000001 = 11111110 ~ 00000010 = 11111101 ~ 00000011 = 11111100 ~ 00000100 = 11111011 ~ 00000101 = 11111010 ~ 00000110 = 11111001 ~ 00000111 = 11111000 ~ 00001000 = 11110111 ~ 00001001 = 11110110 ~ 00001010 = 11110101 ~ 00001011 = 11110100 ~ 00001100 = 11110011 ~ 00001101 = 11110010 ~ 00001110 = 11110001 ~ 00001111 = 11110000 移位操作 移位操作分为算术移位和逻辑移位, 逻辑左移和算术左移的规则是一样的, 所以两者共用同一个左移函数, 而逻辑右移和算术右移不同, 逻辑右移是一直补0, 而算术右移要看符号位, 符号位为0则补0, 符号位为1, 则补1. 同时移位操作有两种形式, 一种给定一个常数, 向量中的每个元素都移该常数位, 一种是给定一个向量, 向量中的每个元素移给定向量中对应数值的位. 好吧下面还是看例子吧. 左移: 给定一个常数 void mic_lshift() { uint32_t *arr_a, *arr_c; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 左移:给定一个向量 void mic_lshift_v() { uint32_t *arr_a, *arr_c; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 执行结果为: 00000001 00010000 00000010 00010000 00000011 00001100 00000100 00001000 00000101 01010000 00000110 00110000 00000111 00011100 00001000 00010000 00001001 10010000 00001010 01010000 00001011 00101100 00001100 00011000 00001101 11010000 00001110 01110000 00001111 00111100 00010000 00100000 算术右移 void mic_arshift() { uint32_t *arr_a, *arr_b, *arr_c, *arr_d; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_b = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); arr_d = _mm_malloc(sizeof(uint32_t) * n, 64); uint32_t high_one = 1 执行结果为: 符号位为0: 00000000000000000000000000000001 00000000000000000000000000000000 00000000000000000000000000000010 00000000000000000000000000000000 00000000000000000000000000000011 00000000000000000000000000000000 00000000000000000000000000000100 00000000000000000000000000000001 00000000000000000000000000000101 00000000000000000000000000000001 00000000000000000000000000000110 00000000000000000000000000000001 00000000000000000000000000000111 00000000000000000000000000000001 00000000000000000000000000001000 00000000000000000000000000000010 00000000000000000000000000001001 00000000000000000000000000000010 00000000000000000000000000001010 00000000000000000000000000000010 00000000000000000000000000001011 00000000000000000000000000000010 00000000000000000000000000001100 00000000000000000000000000000011 00000000000000000000000000001101 00000000000000000000000000000011 00000000000000000000000000001110 00000000000000000000000000000011 00000000000000000000000000001111 00000000000000000000000000000011 00000000000000000000000000010000 00000000000000000000000000000100 符号位为1: 10000000000000000000000000000001 11100000000000000000000000000000 10000000000000000000000000000010 11100000000000000000000000000000 10000000000000000000000000000011 11100000000000000000000000000000 10000000000000000000000000000100 11100000000000000000000000000001 10000000000000000000000000000101 11100000000000000000000000000001 10000000000000000000000000000110 11100000000000000000000000000001 10000000000000000000000000000111 11100000000000000000000000000001 10000000000000000000000000001000 11100000000000000000000000000010 10000000000000000000000000001001 11100000000000000000000000000010 10000000000000000000000000001010 11100000000000000000000000000010 10000000000000000000000000001011 11100000000000000000000000000010 10000000000000000000000000001100 11100000000000000000000000000011 10000000000000000000000000001101 11100000000000000000000000000011 10000000000000000000000000001110 11100000000000000000000000000011 10000000000000000000000000001111 11100000000000000000000000000011 10000000000000000000000000010000 11100000000000000000000000000100 _mm512_alignr_epi32 函数原型为: extern __m512i __cdecl _mm512_alignr_epi32(__m512i v2, __m512i v3, const int count); 该函数的作用就是将v2和v3拼接起来, v2在前, v3在后, 然后循环左移count个元素, 然后取最右侧的16个元素, 下面看个例子 void mic_alignr() { uint32_t *arr_a, *arr_b, *arr_c, *arr_d; int i = 0, n = 16; arr_a = _mm_malloc(sizeof(uint32_t) * n, 64); arr_b = _mm_malloc(sizeof(uint32_t) * n, 64); arr_c = _mm_malloc(sizeof(uint32_t) * n, 64); arr_d = _mm_malloc(sizeof(uint32_t) * n, 64); for(i = 0; i 执行结果为: arr_a: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 arr_b: 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 count = 3 arr_c: 20 21 22 23 24 25 26 27 28 29 30 31 32 1 2 3 count = 8 arr_c: 25 26 27 28 29 30 31 32 1 2 3 4 5 6 7 8 "}}