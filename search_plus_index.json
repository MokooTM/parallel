{"./":{"url":"./","title":"前言","keywords":"","body":"并行计算 记录并行计算的相关知识，目前主要包括下面的内容 MPI "},"mpi.html":{"url":"mpi.html","title":"MPI","keywords":"","body":"MPI 学习记录 高性能计算之并行编程技术 —— MPI并行程序设计 MPI Tutorial Message Passing Interface DeinoMPI 大部分内容摘抄自 MPI并行程序设计，同时增加了部分内容以及自己的一些理解。另外代码全部使用 c 语言，大部分在原书代码的基础上进行了修改，使代码更易于理解。 "},"mpi-helloworld.html":{"url":"mpi-helloworld.html","title":"MPI-HelloWorld","keywords":"","body":"HelloWord 安装 MPI 运行环境 HelloWorld 编译 运行 安装 MPI 运行环境 Installing MPICH2 on a Single Machine 去 MPICH2 官网下载源码包，然后安装 tar -xzf mpich-3.2.tar.gz cd mpich-3.2 ./configure --disable-fortran CC=gcc CXX=g++ make sudo mark install 安装了 Intel 编译器的可以使用 mpiicc 和 mpiicpc HelloWorld #include #include int main(int argc, char** argv) { // Initialize the MPI environment MPI_Init(NULL, NULL); // Get the number of processes int world_size; MPI_Comm_size(MPI_COMM_WORLD, &world_size); // Get the rank of the process int world_rank; MPI_Comm_rank(MPI_COMM_WORLD, &world_rank); // Get the name of the processor char processor_name[MPI_MAX_PROCESSOR_NAME]; int name_len; MPI_Get_processor_name(processor_name, &name_len); // Print off a hello world message printf(\"Hello world from processor %s, rank %d\" \" out of %d processors\\n\", processor_name, world_rank, world_size); // Finalize the MPI environment. MPI_Finalize(); } 编译 mpicc -o helloworld helloworld.c 运行 mpiexec ./helloworld // 或者 mpirun ./helloworld mpirun 等同于 mpiexec mpirun 命令 mpirun -n -ppn -f ./myprog -n - sets the number of MPI processes to launch; if the option is not specified, the process manager pulls the host list from a job scheduler, or uses the number of cores on the machine. -ppn - sets the number of processes to launch on each node; if the option is not specified, processes are assigned to the physical cores on the first node; if the number of cores is exceeded, the next node is used. -f - specifies the path to the host file listing the cluster nodes; alternatively, you can use the -hosts option to specify a comma-separated list of nodes; if hosts are not specified, the local node is used. "},"基础函数.html":{"url":"基础函数.html","title":"基础函数","keywords":"","body":"基本函数 6个基本函数 信息传递 预定义数据类型 任意源和任意标识 示例 线程依次传参 使用 MPI_BYTE 获得接受信息的长度 使用任意源和任意标识 6个基本函数 MPI 有6个基本函数，从理论上讲 MPI 的所有通信功能都可以使用它的6个基本函数来实现。这 6 个函数为 // 初始化 MPI_Init(int *argc, char ***argv)` // MPI 结束调用 MPI_Finalize(void) /** * 获得进程的标识号 * 进程标保存到 rank 里 */ MPI_Comm_rank(MPI_Comm comm, int *rank) /** * 获得当前通信域中进程的个数 * 进程数量保存到 size 里 */ MPI_Comm_size(MPI_Comm comm, int *size)` // 发送消息 MPI_Send(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm) // 接受消息 MPI_Recv(void * buf, int count, MPI_Datatype data, int source, int tag, MPI_Comm comm, MPI_Status *status) 信息传递 MPI 通过 MPI_Send 和 MPI_Receive 发送和接受消息。MPI_Send 各参数的含义： buf - 发送缓冲区的起始位置 count - 发送的数据个数 datatype - 发送数据的数据类型 dest - 目标进程标号 tag - 消息标志 comm - 通信域 MPI_Recv 各参数含义： buf - 接受缓冲区地址 count - 最多可接受的数据个数 datatype - 接受的数据类型 source - 发送数据的进程号 tag - 消息标志 comm - 通信域 status - 返回状态 发送和接受的时候是以指定的 datatype 为基本单位的，count 是 datatype 类型数据的数目。在接受数据时，接受缓冲区的长度可以大于发送数据的长度。但是 MPI 中没有数据截断，如果发送数据长度大于接受缓冲区的长度就会报错。 在 C 实现中，状态变量 MPI_Status 必须要包含 3 个信息：MPI_SOURCE, MPI_TAG 和 MPI_ERROR，除此之外还可以包含其它的附加域。 通过 MPI_Status，我们可以获得下面的三种主要信息 发送进程的标号，存放在 MPI_SOURCE 属性中 消息的标记号，存放在 MPI_TAG 属性中 消息的长度，通过借助于 MPI_Get_count 函数，将 status 变量和数据类型传入，消息长度存放在 count 中 MPI_Get_count( MPI_Status* status, MPI_Datatype datatype, int* count) 预定义数据类型 MPI 预定义了下面几种数据类型 MPI预定义数据类型 对应的C数据类型 MPI_CHAR signed char MPI_SHORT signed short int MPI_INT signed int MPI_LONG signed long int MPI_LONG_LONG_INT long long int (optional) MPI_UNSIGNED_CHAR unsigned char MPI_UNSIGNED_SHORT unsigned short int MPI_UNSIGNED unsigned int MPI_UNSIGNED_LONG unsigned long int MPI_FLOAT float MPI_DOUBLE double MPI_LONG_DOUBLR long double MPI_BYTE 无 MPI_PACKED 无 在传递信息的时候，要保证两个方面的类型匹配（除了 MPI_BYTE 和 MPI_PACKED ）： 传输的数据类型和通信中声明的 MPI 类型要对应，即数据类型为 int， 那么通信时声明的数据类型就要为 MPI_INT。 发送方和接受方的类型要匹配 MPI_BYTE 和 MPI_PACKED 可以和任意以字节为单位的存储相匹配。 MPI_BYTE 可以用于不加修改的传送内存中的二进制值。 任意源和任意标识 在消息传递时，发送操作必须明确指定发送对象的进程标号和消息标识，但是接收消息时，可以通过使用 MPI_ANY_SOURCE 和 MPI_ANY_TAG 来接受任意进程发送给本进程的消息，类似于通配符。MPI_ANY_SOURCE 和 MPI_ANY_TAG 可以同时使用或者分别单独使用。 示例 线程依次传参 #include #include #include \"mpi.h\" /** * 数据传递 * * 线程 i 向线程 i+1 传数据 */ int main() { int param; int process_num; int process_id; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &process_id); MPI_Comm_size(MPI_COMM_WORLD, &process_num); if(process_id == 0) { param = 3; printf(\"rank 0 send rank 1: %d\\n\", param); param++; MPI_Send(&param, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); } else { MPI_Recv(&param, 1, MPI_INT, process_id - 1, 99, MPI_COMM_WORLD, &status); printf(\"rank %d receive from %d: %d\\n\",process_id, process_id-1, param); if(process_id 运行结果 rank 0 send rank 1: 3 rank 1 receive from 0: 4 rank 1 send rank 2: 5 rank 2 receive from 1: 5 rank 2 send rank 3: 6 rank 3 receive from 2: 6 使用 MPI_BYTE 下面是一个使用 MPI_BYTE 传递自定义结构体的例子。 #include #include #include \"mpi.h\" typedef struct _custom_t { int a; double b; } custom_t; int main() { int rank; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); custom_t msg; if(rank == 0) { msg.a = 2; msg.b = 10.0; MPI_Send(&msg, sizeof(custom_t), MPI_BYTE, 1, 99, MPI_COMM_WORLD); } if(rank == 1) { MPI_Recv(&msg, sizeof(custom_t), MPI_BYTE, 0, 99, MPI_COMM_WORLD, &status); printf(\"msg: a is %d and b is %.2f\\n\", msg.a, msg.b); } MPI_Finalize(); } 获得接受信息的长度 在接收完消息后，获得消息的长度。要求接受缓冲区的长度要大于消息的长度 #include #include #include \"mpi.h\" int main() { int n = 10; int half_n = 5; int buffer[n]; int i; int rank; MPI_Status status; for(i = 0; i 上面的方法不太灵活，因为在接受消息的时候我们仍然不知道消息的长度，在 Recv函数中的 count 不太好指定。通过使用 MPI_Probe 方法，我们可以事先获取要接受的消息的相关信息，可以灵活的接受消息。 下面是 MPI_Probe 函数的原型： MPI_Probe( int source, int tag, MPI_Comm comm, MPI_Status* status) 通过传入发送进程的进程号，消息标记以及通信域，就可以提前获得消息的 status，下面是一个示例： #include #include #include \"mpi.h\" int main() { int n = 10; int half_n = 5; int buffer[n]; int i; int rank; MPI_Status status; for(i = 0; i 使用任意源和任意标识 线程 1 到 n-1 向线程 0 发送消息 #include #include #include \"mpi.h\" int main() { int rank, size; int value = 0; int i; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { for(i = 0; i 示例输出 receive from process 2 and values is 4 receive from process 3 and values is 6 receive from process 1 and values is 2 "},"常用函数.html":{"url":"常用函数.html","title":"常用函数","keywords":"","body":"常用函数 获得当前时间 double MPI_Wtime(void) 获得机器名字 int MPI_Get_processor_name(char *name, int *result_len) name - 当前进程所在机器的名字 result_len - 返回名字的长度 获得 MPI 版本 int MPI_Get_version(int *version, int *subversion) version - MPI 主版本号 subversion - MPI 次版本号 判断 MPI_Init 是否执行，唯一一个可以在 MPI_Init 之前调用的函数。 int MPI_Initialized(int *flag) - flag - 是否已调用 使通信域 Comm 中的所有进程退出 int MPI_Abort(MPI_Comm comm, int errorcode) comm - 退出进程所在的通信域 errorcode - 错误码 "},"编程模式.html":{"url":"编程模式.html","title":"编程模式","keywords":"","body":"编程模式 模式类别 对等模式 单独发送接收 同时发送接受 使用虚拟进程 主从模式 矩阵向量乘 模式类别 使用 MPI 时有两种最基本的并行程序设计模式，即对等模式和主从模式（master-salve）。在对等模式中，每个进程的功能和代码基本一致，只是处理的数据和对象有所不同。而主从模式中，会有一个 master 线程，用来管理其他的线程（称为 workers 或者 slaves）。 对等模式 单独发送接收 这里我们使用 MPI 并行程序设计 书中的例子 -- Jacobi 迭代来展示对等模式。下面是根据书上的代码修改后的原始代码。从下面的代码可以看出，Jacobi 迭代得到的新值其实就是原来旧值点相邻数值点的平均数。这里为了简单，我们忽略了第一行、第一列、最后一行和最后一列值的计算，而只是使用它们的初始值。还有一点，在每次 k 迭代时，a 更新后的值会首先保存到数组 b 中，等到 a 的值全部计算出来之后，再将 a 的值更新，这样在进行 i 迭代和 j 迭代时，迭代之间就没有数据依赖关系，可以并行。 如果将 b[i][j] = 0.25 * ... 修改为 a[i][j] = 0.25 * ...，这样在进行 i 迭代和 j 迭代时，每次迭代都会依赖前一次的计算结果，是很难并行的。 // 迭代10次，计算时忽略了 0，n-1 行 和 0，n-1 列 for (k = 0; k 对于上面的代码，并行策略十分简单，假设有 n 个进程，m 行数据，每个进程计算 m / n 行数据即可。 假设有 4 个进程，8行数据（去掉第一行和最后一行），那么进程 0 计算 1 和 2 行数据，进程 1 计算 3 和 4 行数据，以此类推。进程在每次 k 迭代时，除了之后自己计算行的数据，还需要知道相邻行的数据。以进程 1 为例，在 i 迭代开始之前，除了需要知道第 3 行和第 4 行的数据，还需要知道第 2 行和第 5 行的数据，这两行数据分别由进程 0 和进程 2 计算，需要在 i 迭代开始之前，由进程 0 和 进程 2 传递过来，同时进程 1 的第 3 行数据需要传给进程 0， 第 4 行数据数据需要传给进程 2 。下面是一张示意图，原文中是按列计算，和按行计算原理一样。 下面是代码实现 void mpi_jacobi() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i 0) { MPI_Send(&a[start][0], n, MPI_INT, rank - 1, 100, MPI_COMM_WORLD); } // 向右侧邻居发送数据 if(rank 0 ) { MPI_Recv(&a[start - 1][0], n, MPI_INT, rank - 1, 99, MPI_COMM_WORLD, &status); } for(i = start; i 同时发送接受 通过使用 MPI_Sendrecv 函数，我们可以实现同时向其他进程发送数据以及从其他进程接受数据，下面是函数原型： MPI_Sendrecv( void *sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据的数据类型 int dest, // 目标进程 int sendtag, // 发送消息标识 void *recvbuf， // 接收缓冲区的初始地址 int recvcount， // 最大接受数据个数 MPI_Datatype recvtype, // 接受数据类型 int source, // 源进程标识 int recvtag, // 接受消息标识 MPI_Comm comm, // 通信域 MPI_Status *status // 返回状态 ) MPI_Sendrecv 可以接收 MPI_Send 的消息， MPI_Recv 也可以接收 MPI_Sendrecv 的消息。 除了 MPI_Sendrecv，我们还可以使用 MPI_Sendrecv_replace 来同时发送和接受。下面是函数原型： MPI_Sendrecv_replace( void * buf, // 发送和接收缓冲区的起始地址 int count, // 发送和接收缓冲区中的数据个数 MPI_Datatype datatype, // 缓冲区中的数据类型 int dest, // 目标进程标识 int sendtag, // 发送信息标识 int source, // 源进程标识 int recvtag, // 接收消息标识 MPI_Comm comm, // 通信域 MPI_Status status // 返回状态 ) MPI_Sendrecv_replace 函数会首先将缓冲区的数据发送出去，然后再将接受的数据放到缓冲区里。 下面是代码示例： void mpi_jacobi2() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i 0) { MPI_Sendrecv(&a[start][0], n, MPI_INT, rank - 1, 100, &a[start - 1][0], n, MPI_INT, rank - 1, 100, MPI_COMM_WORLD, &status); } for(i = start; i 使用虚拟进程 虚拟进程（MPI_PROC_NULL）是不存在的假想进程。在 MPI 中的主要作用是充当真是进程通信的目标或者源。使用虚拟进程可以大大简化处理边界的代码，使程序更加清晰。一个真实进程向虚拟进程 MPI_PROC_NULL 发送消息会立即成功返回，一个真实进程从虚拟进程 MPI_PROC_NULL 接收消息也会立即返回。下面是示例 void mpi_jacobi3() { int m = 10; int n = 10; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { dist_pro = MPI_PROC_NULL; } // 向右侧邻居发送数据并且从右侧邻居获得数据 MPI_Sendrecv(&a[end][0], n, MPI_INT, dist_pro, 100, &a[end+1][0], n, MPI_INT, dist_pro, 100, MPI_COMM_WORLD, &status); dist_pro = rank - 1; if(dist_pro 主从模式 矩阵向量乘 在矩阵向量乘中，首先主进程将向量 B 广播给所有的从进程，然后将矩阵 A 的各行依次发送给从进程，从进程计算一行和 B 相乘的结果，然后将结果返回给主线程。为了简单我们使用下面的矩阵，程序中一共 4 个进程，除去一个主进程，其余进程正好处理一行 A 矩阵。 [012123234]×[222]=[61218] \\begin{bmatrix} 0 & 1 & 2 \\\\ 1 & 2 & 3 \\\\ 2 & 3 & 4 \\end{bmatrix} \\times \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 12 \\\\ 18 \\end{bmatrix} ​⎣​⎡​​​0​1​2​​​1​2​3​​​2​3​4​​​⎦​⎤​​×​⎣​⎡​​​2​2​2​​​⎦​⎤​​=​⎣​⎡​​​6​12​18​​​⎦​⎤​​ 下面是矩阵乘的原始代码 void matrix() { int n = 3; int a[n][n]; int b[n][1]; int c[n][1]; int i, j; for(i = 0; i 在修改之前，我们首先介绍一个函数 MPI_Bcast，MPI_Bcast 可以将数据广播给其他进程，下面是函数原型 MPI_Bcast( void* data, // 缓冲区的起始地址 int count, // 数据的个数 MPI_Datatype datatype, // 数据类型 int root, // 广播数据的根进程标识 MPI_Comm communicator // 通信域 ) MPI_Bcast 的实现类似于下面的代码，不过 MPI 的实现进行了优化，使广播更加高效。 void my_bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) { int world_rank; MPI_Comm_rank(communicator, &world_rank); int world_size; MPI_Comm_size(communicator, &world_size); if (world_rank == root) { // If we are the root process, send our data to everyone int i; for (i = 0; i 在使用 MPI_Bcast 的时候，我们要注意不需要使用 MPI_Recv 接受，而是在每个进程里都调用 MPI_Bcast，下面是使用 MPI 实现向量乘的一个简单示例： void mpi_matrix() { int n = 3; int a[n][n]; int b[n][1]; int c[n][1]; int i, j; int rank, size; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { // 在主进程进行初始化 for(i = 0; i "},"通信模式.html":{"url":"通信模式.html","title":"通信模式","keywords":"","body":"通信模式 模式类别 标准通信模式 缓存通信模式 同步通信模式 就绪通信模式 模式类别 在 MPI 中共有四种通信模式，如下表所示： 通信模式 发送 接受 标准通信模式（standard mode） MPI_Send MPI_Recv 缓存通信模式（buffered mode） MPI_Bsend   同步通信模式（synchronous mode） MPI_Ssend   就绪通信模式（ready mode） MPI_Rsend   对于非标准的通信模式来说，只有发送操作，没有相应的接收操作。这四种模式的不同点主要表现在两个方面： 数据缓冲区（ buffering ）- 在消息被目标进程接收之前，数据存储的地方 同步（ synchronization ） - 怎样才算完成了发送操作 标准通信模式 使用 MPI_Send 进行消息发送的被成为标准通信模式，在这种模式下，是否使用数据缓冲区以及对数据缓冲区的管理都是由 MPI 自身决定的，用户无法控制。根据 MPI 是否选择缓存发送数据，可以将发送操作完成的标准可以分为下面两种情况： MPI 缓存数据 - 在这种情况下，发送操作不管接受操作是否执行，都可以进行，并且发送操作不需要接收操作收到数据就可以成功返回。 MPI 不缓存数据 - 缓存数据是需要付出代价的，它会延长通信的时间，并且缓冲区并不是总能得到的，所以 MPI 可以选择不缓存数据。在这种情况下，只有当接收操作被调用，并且发送的数据完全到达接收缓冲区后，发送操作才算完成。需要注意的一点，对于非阻塞通信，发送操作虽然没有完成，但是发送调用可以正确返回，程序可以执行其他操作。 下面是标准通信模式的示意图 缓存通信模式 如果希望可以直接对通信缓冲区进行控制，我们可以使用缓存通信模式，下面是缓存发送的函数原型： int MPI_Bsend( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程 int tag, // 消息标识 MPI_Comm comm // 通信域 ) MPI_Bsend 和 MPI_Send 的各参数含义相同，只是在使用 MPI_Bsend 之前需要用户手动指定缓冲区，假设我们不指定缓冲区就直接调用 MPI_Bsend，程序就会报下面的错误： Fatal error in MPI_Bsend: Invalid buffer pointer, error stack: MPI_Bsend(214).......: MPI_Bsend(buf=0x7ffdff7c2d84, count=1, MPI_INT, dest=1, tag=99, MPI_COMM_WORLD) failed MPIR_Bsend_isend(311): Insufficient space in Bsend buffer; requested 4; total buffer size is 0 下面是缓存通信模式的示意图 在手动指定缓冲区时，有3件事需要我们考虑： 如何指定缓冲区 应该指定多大的缓冲区 怎么释放缓冲区 通过 MPI_Buffer_attach 我们可以指定缓冲区，下面是函数原型 int MPI_Buffer_attach( void * buffer, // 缓冲区地址 int size // 缓冲区大小（以字节为单位） ) 通过 MPI_Buffer_detach 我们可以回收缓冲区，下面是函数原型。 int MPI_Buffer_detach( void ** buffer, // 缓冲区地址 int * size // 缓冲区大小 ) 回收操作是阻塞调用，它会一直等到使用该缓存的消息发送完成之后才返回。只有调用返回之后，用户才可以重新使用该缓冲区或者将缓冲区释放。 缓冲区的大小的计算稍微繁琐一些。首先的一点，申请的总缓冲区的大小应该是所有未完成的 MPI_Bsend 所需缓冲区大小的总和。每个 MPI_Bsend 所需的缓冲区大小除了它所传输的数据大小还需要加上一个 MPI_BSEND_OVERHEAD。MPI_BSEND_OVERHEAD 指在 MPI_Bsend 调用时，该调用自身可能占用的最大空间。另外，我们需要使用 MPI_Pack_size 函数获取所传输数据大小，下面是函数原型： int MPI_Pack_size( int incount, // 数据的个数 MPI_Datatype datatype, // 数据的类型 MPI_Comm comm, // 通信域 int *size // 数据所需要的空间，以字节为单位 ) 假设只有一个 MPI_Bsend 调用， 缓冲区大小计算如下所示： MPI_Bsend( ..., count=c, datatype=type, ... ); MPI_Pack_size(c, type, comm, &s1); size = s1 + MPI_BSEND_OVERHEAD; 假设有两个 MPI_Bsend 调用，缓冲区的大小计算如下所示： MPI_Bsend( ..., count=c1, datatype=type1, ... ); MPI_Bsend( ..., count=c2, datatype=type2, ... ); MPI_Pack_size(c1, type1, comm, &s1); MPI_Pack_size(c2, type2, comm, &s2); size = s1 + s2 + 2 * MPI_BSEND_OVERHEAD; 下面是使用 MPI_Bsend 的一个例子 void bsend() { int rank; int a; int * tmp_buffer; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { a = 3; int size; // 计算缓冲区大小 MPI_Pack_size(1, MPI_INT, MPI_COMM_WORLD, &size); size += MPI_BSEND_OVERHEAD; tmp_buffer = (int *)malloc(size); // 指定缓冲区 MPI_Buffer_attach(tmp_buffer, size); MPI_Bsend(&a, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); // 回收缓冲区 MPI_Buffer_detach(&tmp_buffer, &size); free(tmp_buffer); } if(rank == 1) { MPI_Recv(&a, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &status); printf(\"a is %d\\n\", a); } MPI_Finalize(); } 同步通信模式 在同步通信模式中，发送进程必须要等到相应的接受进程开始后才可以正确返回。也就是说如果发送的数据一直没有被接受，发送进程就会一直处于等待状态。当同步发送操作返回之后，说明发送缓冲区中的数据已经全被系统缓冲区缓存，并且已经开始发送，这样发送缓冲区就可以被释放或者重新使用。下面是同步发送的函数原型： MPI_Ssend( void *buf, // 发送缓冲区起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程号 int tag, // 消息标识 MPI_Comm comm // 通信域 ) 下面是同步通信模式的示意图： 下面是同步通信的示例： void ssend() { int rank; int a; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { a = 3; MPI_Ssend(&a, 1, MPI_INT, 1, 99, MPI_COMM_WORLD); printf(\"rank 0 has finished\\n\"); } if(rank == 1) { MPI_Recv(&a, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &status); printf(\"a is %d\\n\", a); } MPI_Finalize(); } 就绪通信模式 在就绪通信模式中，只有当接收进程的接收操作已经启动时，才可以在发送进程启动发送操作。就绪通信模式的特殊之处在于它要求接受操作先于发送操作而被启动，因此在一个正确的程序中，一个就绪发送可以被一个标准发送替代，它对程序的语义没有影响，而对程序的性能有影响。下面是就绪发送的函数原型： MPI_Rsend( void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm ) 下面是就绪通信的示意图： "},"非阻塞通信.html":{"url":"非阻塞通信.html","title":"非阻塞通信","keywords":"","body":"非阻塞通信 简介 非阻塞通信模式 非阻塞通信函数 非阻塞通信完成 基本示例 非阻塞通信取消 释放非阻塞通信对象 消息到达后检查 使用非阻塞通信实现 Jacobi 迭代 简介 前面所讲的 MPI_Send 的通信模式为阻塞通信模式，在这种模式下，当一个阻塞通信正确返回后，可以得到下面的信息： 通信操作已正确完成，即消息已成功发出或者接收 通信占用的缓冲区可以使用，若是发送操作，则该缓冲区可以被其他操作更新，若是接收操作，那么该缓冲区中的数据已经完整，可以被正确使用。 下面是阻塞消息发送和接收的示意图： 在阻塞通信中，对于接收进程，在接受消息时，要保证按照消息发送的顺序接受消息.例如进程 0 向进程 1 连续发送了 2 条消息，记为消息0 和消息1，消息0先发送，这时即便消息1 先到达了进程1，进程1 也无法接受消息1，必须要等到消息0 被接收之后，消息1 才可以被接收。 与阻塞通信不同，非阻塞通信不必等到通信操作完成完成便可以返回，相对应的通信操作会交给特定的通信硬件去完成，在该通信硬件进行通信操作的同时，处理器可以同时进行计算。通过通信与计算的重叠，可以大大提高程序执行的效率。下面是非阻塞消息发送和接收的示意图： 非阻塞通信模式 在前面阻塞通信中，我们知道有 4 种基本的通信模式： 标准通信模式 缓存通信模式 同步通信模式 就绪通信模式 非阻塞通信和这四种通信模式相结合，也有四种不同的模式。同时针对某些通信是在一个循环中重复执行的情况， MPI 又引入了重复非阻塞通信方式，以进一步提高效率。对于重复非阻塞通信，和四种通信模式相结合，又有四种不同的具体形式。下面是具体的通信模式： 通信模式 发送 接受 标准通信模式 MPI_Isend MPI_IRecv 缓存通信模式 MPI_Ibsend 同步通信模式 MPI_Issend 就绪通信模式 MPI_Irsend 重复非阻塞通信 标准通信模式 MPI_Send_init MPI_Recv_init 缓存通信模式 MPI_Bsend_init 同步通信模式 MPI_Ssend_init 就绪通信模式 MPI_Rsend_init 同时需要注意的是只要消息信封相吻合，并且符合有序接受的语义，任何形式的发送和任何形式的接受都可以匹配。 非阻塞通信函数 标准非阻塞发送操作 MPI_Isend( void * buf, // 发送缓冲区起始地址 int count, // 发送数据个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程号 int tag, // 消息标志 MPI_Comm comm, // 通信域 MPI_Request * request // 返回的非阻塞通信对象 ) 标准非阻塞接收 MPI_Irecv( void * buf, // 接受缓冲区的起始地址 int count, // 接受数据的最大个数 MPI_Datatype datatype, // 数据类型 int source, // 源进程标识 int tag, // 消息标志 MPI_Comm comm, // 通信域 MPI_Request * request // 非阻塞通信对象 ) 其余的三种通信模式和阻塞通信的函数形式类似，只是函数名称修改了一下，这里不做详细介绍。 // 同步通信模式 MPI_Issend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) // 缓存通信模式 MPI_Ibsend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) // 就绪通信模式 MPI_Irsend(void * buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm, MPI_Request * request) 非阻塞通信完成 由于非阻塞通信返回并不意味着该通信已经完成，因此 MPI 提供了一个非阻塞通信对象 -- MPI_Request 来查询通信的状态。通过结合 MPI_Request 和下面的一些函数，我们等待或者检测阻塞通信。 对于单个非阻塞通信来说，可以使用下面两个函数来等待或者检测非阻塞通信。其中 MPI_Wait 会阻塞当前进程，一直等到相应的非阻塞通信完成之后再返回。MPI_Test 只是用来检测通信是否完成，它会立即返回，不会阻塞当前进程。如果通信完成，将 flag 置为 true，如果通信还没完成，则将 flag 置为 false。 MPI_Wait( MPI_Request * request, // 非阻塞通信对象 MPI_Status * status // 返回的状态 ); MPI_Test( MPI_Request * request, // 非阻塞通信对象 int * flag, // 操作是否完成，完成 - true，未完成 - false MPI_Status * status // 返回的状态 ); 对于多个非阻塞通信，MPI 也提供了相应的等待或者检测函数。MPI_Waitany 用来等待多个非阻塞对象中的任何一个非阻塞对象，MPI_Waitall 会等待所有的非阻塞通信完成，MPI_Waitsome 介于 MPI_Waitany 和 MPI_Waitall之间，只要有一个或者多个非阻塞通信完成，该调用就返回。 MPI_Waitany( int count, // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 非阻塞通信对象数组 int * index, // 通信完成的对象在数组中的索引 MPI_Status * status // 返回的状态 ); MPI_Waitall( int count， // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 费组摄通信对象数组 MPI_Status * array_of_status // 状态数组 ); MPI_Waitsome( int incount, // 非阻塞通信对象的个数 MPI_Request * array_of_requests // 非阻塞通信对象数组 int * outcount, // 已完成对象的数目 int * array_of_indices // 已完成对象的索引数组 MPI_Status * array_of_statuses // 已完成对象的状态数组 ); MPI_Testany 用来检测非阻塞通信中是否有任何一个对象已经完成（若有多个非阻塞通信对象完成则从中任取一个），这里只是检测，不会阻塞进程。MPI_Testall 用来检测是否所有的非阻塞通信都已经完成，MPI_Testsome用来检测有非阻塞通信已经完成。 MPI_Testany( int count, // 非阻塞通信对象的个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * index, // 非阻塞通信对象的索引 int * flag, // 是否有对象完成 MPI_Status * status // 返回的状态 ); MPI_Testall( int count, // 非阻塞通信对象个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * flag, // 所有非阻塞通信对象是否都完成 MPI_Status * array_of_statuses // 状态数组 ); MPI_Testsome( int incount, // 非阻塞通信对象的个数 MPI_Request * array_of_requests, // 非阻塞通信对象数组 int * outcount, // 已完成对象的数目 int * array_of_indices, // 已完成对象的索引数组 MPI_Status * array_of_statuses // 已完成对象的状态数组 ) 基本示例 下面是非阻塞通通信的一个基本示例 #include #include #include \"mpi.h\" void isend() { int rank; int n = 1000; int a[n]; int i; int has_finished; MPI_Request request; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { for(i = 0; i 非阻塞通信取消 可以使用 MPI_Cancel 来取消已经调用的非阻塞通信，该调用立即返回。取消调用并不意味着相应的通信一定会被取消，如果非阻塞通信已经开始，那么它会正常完成。如果取消操作时非阻塞通信还没有开始，那么可以取消该阻塞通信，释放通信占用的资源。对于非阻塞通信，即使执行了取消操作，也必须调用 MPI_Wait 或者 MPI_Test 来释放对象。下面是 MPI_Cancel 的函数原型： int MPI_Cancel(MPI_Request *request); 如果一个非阻塞通信已经被执行了取消操作，那么该通信中的 MPI_Wait 和 MPI_Test 将释放非阻塞通信对象，并且在返回结果 status 中指明该通信已经被取消。 一个通信是否被取消，可以通过 MPI_Test_cancelled 来检查，如果返回结果 flag=1 则表明通信已被成功取消，负责说明通信还没有被取消。 int MPI_Test_cancelled( MPI_Status * status, // 状态 int * flag // 是否取消标志 ); 下面是使用 MPI_Cancel 的一个示例 #include #include #include \"mpi.h\" int main() { int rank; MPI_Request request; MPI_Status status; int value; int has_canceled; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; MPI_Send(&value,1, MPI_INT, 1, 99, MPI_COMM_WORLD); } else if(rank == 1) { MPI_Irecv(&value, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &request); MPI_Cancel(&request); MPI_Wait(&request, &status); MPI_Test_cancelled(&status, &has_canceled); printf(\"has_canceled is %d\\n\", has_canceled); printf(\"value is %d\\n\", value); if(has_canceled) { MPI_Irecv(&value, 1, MPI_INT, 0, 99, MPI_COMM_WORLD, &request); } MPI_Wait(&request, &status); printf(\"value is %d\\n\", value); } MPI_Finalize(); } 释放非阻塞通信对象 当能够确认一个非阻塞通信操作已经完成时，我们可以直接调用 MPI_Request_free 直接释放掉该对象所占的资源（非阻塞通信对象一定要被释放，通过MPI_Wait 和 MPI_Test 可以释放非阻塞通信对象，但是对于 MPI_Test 来说，只要通信完成之后调用才可以释放对象）。当调用 MPI_Request_free 操作之后，通信对象不会立即释放，而是要等到阻塞通信结束之后才会释放。下面是函数原型 int MPI_Request_free(MPI_Request * request); 消息到达后检查 通过调用 MPI_Probe 和 MPI_Iprobe，我们可以在不实际接收消息的情况下检查消息是否到达，其中 MPI_Probe 用于阻塞通信，MPI_Iprobe 用于非阻塞通信。如果消息已经到达，那么通过调用 MPI_Probe 和 MPI_Iprobe 返回的 status 和通过 MPI_Recv 返回的 status 是一样的。通过返回的 status，我们可以获得消息的相关信息，例如消息的长度等。下面是函数的原型： int MPI_Probe( int source, // 源进程标识 或者 `MPI_ANY_SOURCE` int tag, // 特定的消息标识值 或者 `MPI_ANY_TAG` MPI_Comm comm, // 通信域 MPI_Status *status // 返回的状态 ) int MPI_Iprobe( int source, // 源进程标识 或者 `MPI_ANY_SOURCE` int tag, // 特定的消息标识值 或者 `MPI_ANY_TAG` MPI_Comm comm, // 通信域 int * falg, // 消息是否到达 MPI_Status * status // 返回的状态 ); 下面是使用 MPI_Iprobe 的一个示例，在接收时调用了5次 MPI_Iprobe 函数，主要是用来等待消息到达。 #include #include #include \"mpi.h\" int main() { int rank; MPI_Request request; MPI_Status status; int value; int hava_message = 0; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; MPI_Isend(&value,1, MPI_INT, 1, 99, MPI_COMM_WORLD, &request); MPI_Request_free(&request); } else if(rank == 1) { for(int i = 0; i 使用非阻塞通信实现 Jacobi 迭代 通过使用非阻塞通信实现 Jacobi 迭代，可以实现通信和计算的重叠。为了实现计算与通信的最大重叠，一个通用的原则就是“尽早开始通信，进晚结束通信”，在开始通信和完成通信之间进行计算，这样就有更多的计算任务和通信重叠。这里，我们首先计算边界处的数据，计算完成之后执行非阻塞通信，在通信的同时计算剩余的数据。下面是具体实现方式： void mpi_jacobi_new() { int m = 18; int n = 18; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { right = MPI_PROC_NULL; } MPI_Isend(&b[start][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[0]); MPI_Isend(&b[end][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[1]); MPI_Irecv(&a[start - 1][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[2]); MPI_Irecv(&a[end+1][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[3]); // 计算剩余的部分 for(i = start+1; i "},"重复非阻塞通信.html":{"url":"重复非阻塞通信.html","title":"重复非阻塞通信","keywords":"","body":"重复非阻塞通信 介绍 通信模式 示例 实现 Jacobi 迭代 介绍 如果一个通信会被重复执行，比如循环结构内的通信调用，MPI 提供了重复非阻塞通信进行优化，以降低不必要的通信开销，下面是非阻塞通信的流程： 在重复通信时，通信的初始化操作并没有启动消息通信，消息的真正通信是由 MPI_START 触发的，消息的完成操作并不释放相应的非阻塞通信对象，只是将其状态置为非活动状态，若下面进行重复通信，再由 MPI_START 将对象置为活动状态，并启动通信。当不需要再进行通信时，必须通过显式的语句MPI_Request_free将非阻塞通信对象释放掉。 通信模式 根据通信模式的不同，重复非阻塞通信也有四种不同的形式，即标准模式、缓存模式、同步模式和就绪模式，分别对应的函数为 MPI_Send_init，MPI_Bsend_init，MPI_Ssend_init和MPI_Rsend_init，下面是函数原型： // 标准模式 int MPI_Send_init( void * buf, // 发送缓冲区起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request * request // 非阻塞通信对象 ); // 缓存模式 int MPI_Bsend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); // 同步模式 int MPI_Ssend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); // 就绪模式 int MPI_Ssend_init( void * buf, // 发送缓冲区的起始地址 int count, // 发送数据的个数 MPI_Datatype datatype, // 发送数据的数据类型 int dest, // 目标进程标识 int tag, // 消息标识 MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); 通过 MPI_Recv_init 函数来完成接收操作，下面是函数原型： int MPI_Recv_init( void * buf, // 接受缓冲区的起始地址 int count, // 接受数据的个数 MPI_Datatype datatype, // 接受数据的数据类型 int dest, // 源进程标识 或者 MPI_ANY_SOURCE int tag, // 消息标识 或者 MPI_ANY_TAG MPI_Comm comm, // 通信域 MPI_Request *request // 非阻塞通信对象 ); 在前面提到，一个非阻塞通信在创建后会处于非活动状态，需要使用 MPI_Start 函数来激活通信，下面是函数原型 int MPI_Start( MPI_Request * request // 费祖通信对象 ); 对于多个非阻塞通信，我们还可以使用 MPI_Startall 来同时激活多个非阻塞通信，下面是函数原型 int MPI_Startall( int count, // 开始非阻塞通信对象的个数 MPI_Request * requests // 非阻塞通信对象数组 ); 示例 下面是使用重复非阻塞通信的一个示例： #include #include #include \"mpi.h\" int main() { int rank; int value; MPI_Request request; MPI_Status status; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { MPI_Send_init(&value, 1, MPI_INT, 1, 99, MPI_COMM_WORLD, &request); for(int i = 0; i 实现 Jacobi 迭代 下面是用重复非阻塞通信实现的Jacobi 迭代 void mpi_jacobi_new2() { int m = 18; int n = 18; int a[m][n]; int b[m][n]; int i, j, k; for(i = 0; i size - 1) { right = MPI_PROC_NULL; } MPI_Send_init(&b[start][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[0]); MPI_Send_init(&b[end][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[1]); MPI_Recv_init(&a[start - 1][0], n, MPI_INT, left, 99, MPI_COMM_WORLD, &request[2]); MPI_Recv_init(&a[end+1][0], n, MPI_INT, right, 99, MPI_COMM_WORLD, &request[3]); // 迭代10次，计算时忽略了 0，n-1 行 和 0，n-1 列 for(k = 0; k "},"组通信.html":{"url":"组通信.html","title":"组通信","keywords":"","body":"组通信 功能 消息通信 同步 计算 广播 收集 散发 前面提到的通信都是点到点通信，这里介绍组通信。MPI 组通信和点到点通信的一个重要区别就在于它需要一个特定组内的所有进程同时参加通信，而不是像点对点通信那样只涉及到发送方和接收方两个进程。组通信在各个进程中的调用方式完全相同，而不是像点对点通信那样在形式上有发送和接收的区别。 功能 组通信一般实现三个功能： 通信：主要完成组内数据的传输 同步：实现组内所有进程在特定点的执行速度保持一致 计算：对给定的数据完成一定的操作 消息通信 对于组通信来说，按照通信方向的不同，可以分为以下三种：一对多通信，多对一通信和多对多通信，下面是这三类通信的示意图： 同步 组通信提供了专门的调用以完成各个进程之间的同步，从而协调各个进程的进度和步伐。下面是 MPI 同步调用的示意图 计算 MPI 组通信提供了计算功能的调用，通过这些调用可以对接收到的数据进行处理。当消息传递完毕后，组通信会用给定的计算操作对接收到的数据进行处理，处理完毕后将结果放入指定的接收缓冲区。 广播 MPI_Bcast 是一对多通信的典型例子，它可以将 root 进程中的一条信息广播到组内的其它进程，同时包括它自身。在执行调用时，组内所有进程（不管是 root 进程还是其它的进程）都使用同一个通信域 comm 和根标识 root，其执行结果是将根进程消息缓冲区的消息拷贝到其他的进程中去。下面是 MPI_Bcast 的函数原型： int MPI_Bcast( void * buffer, // 通信消息缓冲区的起始位置 int count, // 广播 / 接收数据的个数 MPI_Datatype datatype, // 广播 / 接收数据的数据类型 int root, // 广播数据的根进程号 MPI_Comm comm // 通信域 ); 对于广播调用，不论是广播消息的根进程，还是从根接收消息的其他进程，在调用形式上完全一致，即指明相同的根，相同的元素个数以及相同的数据类型。下面是广播前后各进程缓冲区中数据的变化 MPI_Bcast 的实现类似于下面的代码，不过 MPI 的实现进行了优化，使广播更加高效。 void my_bcast(void* data, int count, MPI_Datatype datatype, int root, MPI_Comm communicator) { int world_rank; MPI_Comm_rank(communicator, &world_rank); int world_size; MPI_Comm_size(communicator, &world_size); if (world_rank == root) { // If we are the root process, send our data to everyone int i; for (i = 0; i 下面是使用 MPI 广播的一个例子，进程 0 初始化数据，同时广播到其他进程 #include #include #include \"mpi.h\" int main() { int rank; int value; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); if(rank == 0) { value = 10; } // 将进程 0 的数据广播到其他进程中 MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD); printf(\"Process %d value is %d\\n\", rank, value); MPI_Finalize(); } 收集 通过 MPI_Gather 可以将其他进程中的数据收集到根进程。根进程接收这些消息，并把它们按照进程号 rank 的顺序进行存储。对于所有非根进程，接收缓冲区会被忽略，但是各个进程仍需提供这一参数。在 gather 调用中，发送数据的个数 sendcount 和发送数据的类型 sendtype 接收数据的个数 recvcount 和接受数据的类型 recvtype 要完全相同。下面是 MPI_Gather 的函数原型 int MPI_Gather( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是 gather 的示意图： MPI_Gatherv 和 MPI_Gather 类似，也可以完成数据收集的功能，但是它可以从不同的进程接受不同数量的数据。进程接收元素的个数 recvcounts 是一个数组，用来指定从不同进程接受的数据元素的个数 。跟从每一个进程接收的数据元素个数可以不同，但是需要注意的是发送和接受的个数需要保持一致。另外 MPI_Gatherv 还提供一个位置偏移数组 displs，用户指定接收的数据在消息缓冲区中的索引，下面是 MPI_Gatherv 的函数原型： int MPI_Gatherv( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int * recvcounts, // 从每个进程接收的数据个数 int * displs, // 接收数据在消息缓冲区中的索引 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是使用 MPI_Gather 的一个示例： void gather() { int size; int rank; int n = 10; int send_array[n]; int * recv_array; int i; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); // 初始化其它进程的数据 for(i = 0; i 下面是使用 MPI_Gatherv 的一个示例 void gatherv() { int size; int rank; int n = 10; int send_array[n]; int * recv_array; int i; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); int recv_count[size]; int displs[size]; for(i = 0; i 散发 MPI_Scatter 是一对多的组通信调用，和广播不同的是，root 进程向各个进程发送的数据可以是不同的。MPI_Scatter 和 MPI_Gather 的效果正好相反，两者互为逆操作。下面是 MPI_Scatter 的函数原型 int MPI_scatter( void * sendbuf, // 发送缓冲区的起始地址 int sendcount, // 发送数据的个数 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是 scatter 的示意图： MPI_Scatterv 和 MPI_Gatherv 也是一对互逆操作，下面是 MPI_Scatterv 的函数原型 int MPI_scatter( void * sendbuf, // 发送缓冲区的起始地址 int* sendcounts, // 向每个进程发送的数据个数 int* displs, // 发送数据的偏移 MPI_Datatype sendtype, // 发送数据类型 void * recvbuf, // 接收缓冲区的起始地址 int recvcount, // 接收数据的个数 MPI_Datatype recvtype, // 接收数据的类型 int root, // 根进程的编号 MPI_Comm comm // 通信域 ); 下面是使用 MPI_Scatter 的一个示例： void scatter() { int size; int rank; int n = 10; int * send_array; int recv_array[n]; int i, j; MPI_Init(NULL, NULL); MPI_Comm_rank(MPI_COMM_WORLD, &rank); MPI_Comm_size(MPI_COMM_WORLD, &size); if(rank == 0) { send_array = (int *)malloc(sizeof(int) * n * size); for(i = 0; i "}}